import logging
import os

import mlflow.pyfunc
import mlflow.spark
from pyspark import SparkConf
from pyspark.sql import SparkSession

from src.logging_config import configure_logger
from src.utils import log_env, setup_env, validate_env

logger = configure_logger("predict", logging.DEBUG)

setup_env()
validate_env()
log_env()

MONGODB_HOST = os.environ.get("MONGODB_HOST")
MONGODB_PORT = os.environ.get("MONGODB_PORT")
MONGODB_URI = f"mongodb://{MONGODB_HOST}:{MONGODB_PORT}/"
MONGODB_DB = os.environ.get("MONGODB_DB")

MLFLOW_HOST = os.environ.get("MLFLOW_HOST")
MLFLOW_PORT = os.environ.get("MLFLOW_PORT")
MLFLOW_URI = f"http://{MLFLOW_HOST}:{MLFLOW_PORT}"
MLFLOW_EXPERIMENT_NAME = os.environ.get("MLFLOW_EXPERIMENT_NAME")
MLFLOW_PIPELINE_NAME = os.environ.get("MLFLOW_PIPELINE_NAME")
MLFLOW_MODEL_NAME = os.environ.get("MLFLOW_MODEL_NAME")
MLFLOW_MODEL_VERSION = os.environ.get("MLFLOW_MODEL_VERSION")

MONGODB_PREDICTION_COLLECTION = os.environ.get("MONGODB_PREDICTION_COLLECTION")

SPARK_AVRO_JAR = os.environ.get("SPARK_AVRO_JAR")
SPARK_MONGO_CONNECTOR_JAR = os.environ.get("SPARK_MONGO_CONNECTOR_JAR")
CONF = (
    SparkConf()
    .set("spark.master", "local")
    .set("spark.app.name", "BDM Spark formatted pipeline")
    .set(
        "spark.jars.packages",
        f"{SPARK_AVRO_JAR},{SPARK_MONGO_CONNECTOR_JAR}",
    )
    .set("spark.driver.extraJavaOptions", "-Dlog4j.configuration=file:log4j.properties")
    .set(
        "spark.executor.extraJavaOptions", "-Dlog4j.configuration=file:log4j.properties"
    )
    .set("spark.mongodb.read.connection.uri", MONGODB_URI)
)

spark = SparkSession.builder.config(conf=CONF).getOrCreate()
logger.info(f"Python version = {spark.sparkContext.pythonVer}")
logger.info(f"Spark version = {spark.version}")

logger.info("Loading data to predict on from MongoDB")
idealista_df = (
    spark.read.format("mongodb")
    .option("database", MONGODB_DB)
    .option("collection", MONGODB_PREDICTION_COLLECTION)
    .load()
)

mlflow.set_tracking_uri(MLFLOW_URI)
mlflow.set_experiment(f"/{MLFLOW_EXPERIMENT_NAME}")
mlflow.start_run()

logger.info("Loading the transformation pipeline")
transformation_pipeline = mlflow.spark.load_model(
    model_uri=f"models:/{MLFLOW_PIPELINE_NAME}/{MLFLOW_MODEL_VERSION}"
)
logger.info("Transforming the data")
transformed_df = transformation_pipeline.transform(idealista_df)

logger.info("Loading the model")
model = mlflow.spark.load_model(
    model_uri=f"models:/{MLFLOW_MODEL_NAME}/{MLFLOW_MODEL_VERSION}"
)
logger.info("Predicting...")
model.transform(transformed_df).select("prediction").show()

mlflow.end_run()
