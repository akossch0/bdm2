import logging
import os

from pyspark import SparkConf
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, expr, split, when
from sqlalchemy import Column, ForeignKey, Integer, PrimaryKeyConstraint, String, create_engine, text
from sqlalchemy.orm import declarative_base

from src.logging_config import configure_logger
from src.utils import log_env, setup_env, validate_env, write_with_jdbc

logger = configure_logger("multidimensional", logging.DEBUG)

setup_env()
validate_env()
log_env()

MONGODB_HOST = os.environ.get("MONGODB_HOST")
MONGODB_PORT = os.environ.get("MONGODB_PORT")
MONGODB_URI = f"mongodb://{MONGODB_HOST}:{MONGODB_PORT}/"
MONGODB_DB = os.environ.get("MONGODB_DB")

POSTGRES_HOST = os.environ.get("POSTGRES_HOST")
POSTGRES_PORT = os.environ.get("POSTGRES_PORT")
POSTGRES_DB = os.environ.get("POSTGRES_DB")
POSTGRES_USER = os.environ.get("POSTGRES_USER")
POSTGRES_PASSWORD = os.environ.get("POSTGRES_PASSWORD")
CONNECTION_STR = f"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}"
JDBC_URL = f"jdbc:postgresql://{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}"
POSTGRES_CONNECTION_PROPERTIES = {
    "user": POSTGRES_USER,
    "password": POSTGRES_PASSWORD,
    "driver": "org.postgresql.Driver",
}

SPARK_AVRO_JAR = os.environ.get("SPARK_AVRO_JAR")
SPARK_MONGO_CONNECTOR_JAR = os.environ.get("SPARK_MONGO_CONNECTOR_JAR")
SPARK_POSTGRESQL_JDBC_JAR = os.environ.get("SPARK_POSTGRESQL_JDBC_JAR")

MONGODB_LOCATION_LOOKUP_COLLECTION = os.environ.get("MONGODB_LOCATION_LOOKUP_COLLECTION")
MONGODB_INCOME_COLLECTION = os.environ.get("MONGODB_INCOME_COLLECTION")
MONGODB_IDEALISTA_COLLECTION = os.environ.get("MONGODB_IDEALISTA_COLLECTION")
MONGODB_AIRQUALITY_COLLECTION = os.environ.get("MONGODB_AIRQUALITY_COLLECTION")

CONF = (
    SparkConf()
    .set("spark.master", "local")
    .set("spark.app.name", "BDM Spark formatted pipeline")
    .set(
        "spark.jars.packages",
        f"{SPARK_AVRO_JAR},{SPARK_MONGO_CONNECTOR_JAR},{SPARK_POSTGRESQL_JDBC_JAR}",
    )
    .set("spark.driver.extraJavaOptions", "-Dlog4j.configuration=file:log4j.properties")
    .set("spark.executor.extraJavaOptions", "-Dlog4j.configuration=file:log4j.properties")
    .set("spark.mongodb.read.connection.uri", MONGODB_URI)
)

spark = SparkSession.builder.config(conf=CONF).getOrCreate()
logger.info(f"Python version = {spark.sparkContext.pythonVer}")
logger.info(f"Spark version = {spark.version}")


def create_tables_if_not_exist():
    Base = declarative_base()

    logger.info("Removing all rows from tables if they exist")
    remove_all_rows_sql = """
    DO $$ 
    DECLARE 
        tbl text;
    BEGIN 
        -- List of tables to delete rows from, in the correct order
        FOR tbl IN 
            SELECT table_name 
            FROM unnest(array['fact_income', 'fact_idealista', 'dim_day', 'dim_month', 'dim_year', 'dim_location']) AS table_name
        LOOP
            -- Check if the table exists
            IF EXISTS (SELECT FROM information_schema.tables WHERE table_schema = 'public' AND table_name = tbl) THEN
                EXECUTE 'DELETE FROM ' || quote_ident(tbl);
            END IF;
        END LOOP;
    END $$;
    """
    engine = create_engine(CONNECTION_STR)
    with engine.connect() as connection:
        connection.execute(text(remove_all_rows_sql))
        connection.commit()

    class DimLocation(Base):
        __tablename__ = "dim_location"
        neighborhood_id = Column(String(), primary_key=True)
        district = Column(String(), nullable=False)
        district_id = Column(String(), nullable=False)
        district_n = Column(String(), nullable=False)
        district_n_reconciled = Column(String(), nullable=False)
        neighborhood = Column(String(), nullable=False)
        neighborhood_n = Column(String(), nullable=False)
        neighborhood_n_reconciled = Column(String(), nullable=False)

    class DimYear(Base):
        __tablename__ = "dim_year"
        year = Column(String(), primary_key=True)

    class DimMonth(Base):
        __tablename__ = "dim_month"
        month = Column(String(), primary_key=True)
        year = Column(String(), ForeignKey("dim_year.year"), nullable=False)

    class DimDay(Base):
        __tablename__ = "dim_day"
        day = Column(String(), primary_key=True)
        month = Column(String(), ForeignKey("dim_month.month"), nullable=False)

    class FactIncome(Base):
        __tablename__ = "fact_income"
        year = Column(String(), ForeignKey("dim_year.year"), nullable=False)
        neighborhood_id = Column(String(), ForeignKey("dim_location.neighborhood_id"), nullable=False)
        population = Column(Integer(), nullable=False)
        rfd_index = Column(Integer(), nullable=False)
        __table_args__ = (PrimaryKeyConstraint("year", "neighborhood_id"),)

    class FactIdealista(Base):
        __tablename__ = "fact_idealista"
        propertycode = Column(String(), nullable=False)
        day = Column(String(), ForeignKey("dim_day.day"), nullable=False)
        neighborhood_id = Column(String(), ForeignKey("dim_location.neighborhood_id"), nullable=False)
        price = Column(Integer(), nullable=False)
        size = Column(Integer(), nullable=False)
        rooms = Column(Integer(), nullable=False)
        bathrooms = Column(Integer(), nullable=False)
        __table_args__ = (PrimaryKeyConstraint("propertycode", "day", "neighborhood_id"),)

    logger.info("Creating tables if they do not exist")
    Base.metadata.create_all(engine)


def read_mongo_tables():
    logger.info("Reading data from MongoDB")
    idealista_df = spark.read.format("mongodb").option("database", MONGODB_DB).option("collection", MONGODB_IDEALISTA_COLLECTION).load()
    location_lookup_df = (
        spark.read.format("mongodb").option("database", MONGODB_DB).option("collection", MONGODB_LOCATION_LOOKUP_COLLECTION).load()
    )
    income_df = spark.read.format("mongodb").option("database", MONGODB_DB).option("collection", MONGODB_INCOME_COLLECTION).load()
    airquality_new_df = (
        spark.read.format("mongodb").option("database", MONGODB_DB).option("collection", MONGODB_AIRQUALITY_COLLECTION + "_new").load()
    )
    idealista_resolved_location_lookup = (
        spark.read.format("mongodb").option("database", MONGODB_DB).option("collection", "idealista_resolved_location_lookup").load()
    )
    return (
        idealista_df,
        location_lookup_df,
        income_df,
        airquality_new_df,
        idealista_resolved_location_lookup,
    )


def create_dimension_tables(idealista_df, location_lookup_df, income_df, airquality_new_df):
    location_lookup_df = location_lookup_df.drop("_id")
    logger.info(f"Loading location dimension table with {location_lookup_df.count()} rows")
    write_with_jdbc(location_lookup_df, "dim_location", JDBC_URL, POSTGRES_CONNECTION_PROPERTIES)

    # Extract distinct and non-null values for income, idealista, and airquality data
    income_years_df = income_df.select("any").distinct().filter(col("any").isNotNull())
    idealista_days_df = idealista_df.select("day").distinct().filter(col("day").isNotNull())
    airquality_month_df = airquality_new_df.select("month").distinct().filter(col("month").isNotNull())

    # Collect all years from different dataframes
    income_years = income_years_df.select("any").rdd.flatMap(lambda x: x).collect()
    idealista_years = (
        idealista_days_df.select(split(col("day"), "_").getItem(0).alias("year")).distinct().rdd.flatMap(lambda x: x).collect()
    )
    airquality_years = (
        airquality_month_df.select(split(col("month"), "_").getItem(0).alias("year")).distinct().rdd.flatMap(lambda x: x).collect()
    )
    years = set(income_years) | set(idealista_years) | set(airquality_years)
    years_df = spark.createDataFrame([(year,) for year in years], ["year"]).distinct()

    logger.info(f"Loading year dimension table with {years_df.count()} rows")
    write_with_jdbc(years_df, "dim_year", JDBC_URL, POSTGRES_CONNECTION_PROPERTIES)

    # Collect all months from idealista and airquality data
    idealista_days_df = idealista_days_df.withColumn("year", split(col("day"), "_").getItem(0)).withColumn(
        "month", expr("concat(split(day, '_')[0], '_', split(day, '_')[1])")
    )
    months = idealista_days_df.select("month").distinct().union(airquality_month_df.select("month").distinct())
    months_df = months.withColumn("year", split(col("month"), "_").getItem(0)).distinct()

    logger.info(f"Loading month dimension table with {months_df.count()} rows")
    write_with_jdbc(months_df, "dim_month", JDBC_URL, POSTGRES_CONNECTION_PROPERTIES)

    # Extract day and month columns for days_df
    days_df = idealista_days_df.select("day", "month").distinct()

    logger.info(f"Loading day dimension table with {days_df.count()} rows")
    write_with_jdbc(days_df, "dim_day", JDBC_URL, POSTGRES_CONNECTION_PROPERTIES)


def create_fact_tables(income_df, location_lookup_df, idealista_df, idealista_resolved_location_lookup):
    # FACT TABLES
    # income
    income_df_looked_up = income_df.join(
        location_lookup_df,
        income_df.nom_barri == location_lookup_df.neighborhood,
        "left",
    )
    income_df_looked_up.count()
    fact_income_df = (
        income_df_looked_up.select("any", "neighborhood_id", "població", "índex rfd barcelona = 100")
        .distinct()
        .withColumnsRenamed(
            {
                "any": "year",
                "neighborhood_id": "neighborhood_id",
                "població": "population",
                "índex rfd barcelona = 100": "rfd_index",
            }
        )
    )
    logger.info(f"Loading income fact table with {fact_income_df.count()} rows")
    write_with_jdbc(fact_income_df, "fact_income", JDBC_URL, POSTGRES_CONNECTION_PROPERTIES)

    # idealista
    logger.info(f"First lookup: looking up neighborhood_id for idealista with location_lookup")
    idealista_df_looked_up = idealista_df.filter("municipality = 'Barcelona'").join(
        location_lookup_df.withColumnsRenamed({"neighborhood": "neighborhood_", "district": "district_"}),
        idealista_df.neighborhood == location_lookup_df.neighborhood_n_reconciled,
        "left",
    )
    logger.info(
        f"Out of {idealista_df_looked_up.count()} listings in Barcelona, {idealista_df_looked_up.filter('neighborhood_id is null').count()} listings have non-reconciled neighborhoods"
    )

    idealista_columns = idealista_df.columns
    idealista_columns.remove("_id")
    idealista_df_looked_up = idealista_df_looked_up.select(idealista_columns + [col("neighborhood_id")])

    logger.info(f"Second lookup: looking up neighborhood_id for idealista with idealista_resolved_location_lookup")
    idealista_df_looked_up = idealista_df_looked_up.join(
        idealista_resolved_location_lookup,
        idealista_df_looked_up.neighborhood == idealista_resolved_location_lookup.neighborhood_,
        "left",
    )

    idealista_df_looked_up = idealista_df_looked_up.withColumn(
        "neighborhood_id",
        when(col("neighborhood_id").isNull(), col("neighborhood_id_")).otherwise(col("neighborhood_id")),
    )
    fact_idealista = idealista_df_looked_up.select(
        "propertycode", "day", "neighborhood_id", "price", "size", "rooms", "bathrooms"
    ).distinct()

    logger.info(
        f"There are {fact_idealista.count()} listings in Barcelona, out of which {fact_idealista.filter('neighborhood_id is null').count()} listings have non-reconciled neighborhoods after the second lookup"
    )

    if fact_idealista.filter("neighborhood_id is null").count() > 0:
        logger.info("Filtering out listings with non-reconciled neighborhoods")
        fact_idealista = fact_idealista.filter(col("neighborhood_id").isNotNull())

    logger.info(f"Loading idealista fact table with {fact_idealista.count()} rows")
    write_with_jdbc(fact_idealista, "fact_idealista", JDBC_URL, POSTGRES_CONNECTION_PROPERTIES)


if __name__ == "__main__":
    create_tables_if_not_exist()

    (
        idealista_df,
        location_lookup_df,
        income_df,
        airquality_new_df,
        idealista_resolved_location_lookup,
    ) = read_mongo_tables()

    create_dimension_tables(idealista_df, location_lookup_df, income_df, airquality_new_df)

    create_fact_tables(income_df, location_lookup_df, idealista_df, idealista_resolved_location_lookup)
