import logging
import os

import mlflow.spark
from pyspark import SparkConf
from pyspark.ml import Pipeline
from pyspark.ml.feature import Imputer, OneHotEncoder, SQLTransformer, StringIndexer, VectorAssembler
from pyspark.ml.regression import LinearRegression
from pyspark.sql import SparkSession
from pyspark.sql.types import BooleanType, DoubleType, IntegerType, StringType

from src.logging_config import configure_logger
from src.utils import log_env, setup_env, validate_env

logger = configure_logger("train", logging.DEBUG)

setup_env()
validate_env()
log_env()

MAX_ITER = 10
REG_PARAM = 0.3
ELASTIC_NET_PARAM = 0.8

MONGODB_HOST = os.environ.get("MONGODB_HOST")
MONGODB_PORT = os.environ.get("MONGODB_PORT")
MONGODB_URI = f"mongodb://{MONGODB_HOST}:{MONGODB_PORT}/"
MONGODB_DB = os.environ.get("MONGODB_DB")

MLFLOW_HOST = os.environ.get("MLFLOW_HOST")
MLFLOW_PORT = os.environ.get("MLFLOW_PORT")
MLFLOW_URI = f"http://{MLFLOW_HOST}:{MLFLOW_PORT}"
MLFLOW_EXPERIMENT_NAME = os.environ.get("MLFLOW_EXPERIMENT_NAME")
MLFLOW_PIPELINE_NAME = os.environ.get("MLFLOW_PIPELINE_NAME")
MLFLOW_MODEL_NAME = os.environ.get("MLFLOW_MODEL_NAME")

SPARK_AVRO_JAR = os.environ.get("SPARK_AVRO_JAR")
SPARK_MONGO_CONNECTOR_JAR = os.environ.get("SPARK_MONGO_CONNECTOR_JAR")
CONF = (
    SparkConf()
    .set("spark.master", "local")
    .set("spark.app.name", "BDM Spark formatted pipeline")
    .set(
        "spark.jars.packages",
        f"{SPARK_AVRO_JAR},{SPARK_MONGO_CONNECTOR_JAR}",
    )
    .set("spark.driver.extraJavaOptions", "-Dlog4j.configuration=file:log4j.properties")
    .set("spark.executor.extraJavaOptions", "-Dlog4j.configuration=file:log4j.properties")
    .set("spark.mongodb.read.connection.uri", MONGODB_URI)
)

spark = SparkSession.builder.config(conf=CONF).getOrCreate()
logger.info(f"Python version = {spark.sparkContext.pythonVer}")
logger.info(f"Spark version = {spark.version}")

logger.info("Loading data from MongoDB")
idealista_df = spark.read.format("mongodb").option("database", MONGODB_DB).option("collection", "idealista").load()

# exclude some columns that are not relevant or have too many null values
excluded_cols = [
    "_id",
    "province",
    "country",
    "operation",
    "haslift",
    "newdevelopmentfinished",
    "parkingspace_hasParkingSpace",
    "parkingspace_isParkingSpaceIncludedInPrice",
]

relevant_columns = [col_name for col_name in idealista_df.columns if col_name not in excluded_cols]
projection_sql = f"SELECT {','.join(relevant_columns)} FROM __THIS__"
projection_transformer = SQLTransformer(statement=projection_sql)

target_column = "price"

numeric_columns = [
    col_name
    for col_name in idealista_df.columns
    if col_name not in excluded_cols + [target_column] and idealista_df.schema[col_name].dataType in [DoubleType(), IntegerType()]
]

# cast all the numeric columns to double, keep the rest as is
cols = []
for col_name in relevant_columns:
    if col_name in numeric_columns:
        cols.append(f"CAST({col_name} AS DOUBLE) AS {col_name}")
    else:
        cols.append(col_name)
cols = ",".join(cols)
numeric_cast_sql = f"SELECT {cols} FROM __THIS__"
numeric_cast_transformer = SQLTransformer(statement=numeric_cast_sql)

# cast all boolean columns as double, keep the rest as is
boolean_columns = [
    col_name
    for col_name in idealista_df.columns
    if idealista_df.schema[col_name].dataType == BooleanType() and col_name not in excluded_cols + [target_column]
]
cols = []
for col_name in relevant_columns:
    if col_name in boolean_columns:
        cols.append(f"CAST({col_name} AS DOUBLE) AS {col_name}")
    else:
        cols.append(col_name)
cols = ",".join(cols)
boolean_cast_sql = f"SELECT {cols} FROM __THIS__"
boolean_cast_transformer = SQLTransformer(statement=boolean_cast_sql)

# replace null values in numeric columns with mean
numerical_imputer = Imputer(
    inputCols=numeric_columns,
    outputCols=numeric_columns,
    strategy="mean",
)

# replace null values in String columns with a placeholder string
categorical_columns = [
    col_name
    for col_name in idealista_df.columns
    if idealista_df.schema[col_name].dataType == StringType() and col_name not in excluded_cols + [target_column]
]
cols = []
UNKOWN = "'UNKNOWN'"
for col_name in relevant_columns:
    if col_name in categorical_columns:
        cols.append(f"CASE WHEN {col_name} IS NULL THEN {UNKOWN} ELSE {col_name} END AS {col_name}")
    else:
        cols.append(col_name)
cols = ",".join(cols)

categorical_replace_sql = f"SELECT {cols} FROM __THIS__"
categorical_imputer = SQLTransformer(statement=categorical_replace_sql)

# handle categorical columns with StringIndexer and OneHotEncoder
indexers = [StringIndexer(inputCol=col, outputCol=col + "_index", handleInvalid="skip") for col in categorical_columns]
encoders = [OneHotEncoder(inputCol=col + "_index", outputCol=col + "_encoded") for col in categorical_columns]

# assemble the final features vector
assembler_inputs = [col + "_encoded" for col in categorical_columns] + numeric_columns
assembler = VectorAssembler(inputCols=assembler_inputs, outputCol="features")

# put together the transformation pipeline
pipeline = Pipeline(
    stages=[projection_transformer, numeric_cast_transformer, boolean_cast_transformer]
    + [numerical_imputer, categorical_imputer]
    + indexers
    + encoders
    + [assembler]
)

pipeline_model = pipeline.fit(idealista_df)
transformed_idealista_df = pipeline_model.transform(idealista_df)

train_df, test_df = transformed_idealista_df.randomSplit([0.8, 0.2], seed=42)

lr = LinearRegression(
    featuresCol="features",
    labelCol=target_column,
    maxIter=MAX_ITER,
    regParam=REG_PARAM,
    elasticNetParam=ELASTIC_NET_PARAM,
)
mlflow.set_tracking_uri(MLFLOW_URI)
mlflow.set_experiment(f"/{MLFLOW_EXPERIMENT_NAME}")
with mlflow.start_run(run_name=MLFLOW_MODEL_NAME):

    logger.info("Fitting the model")
    lrModel = lr.fit(train_df)

    mlflow.spark.log_model(
        spark_model=lrModel,
        artifact_path=MLFLOW_MODEL_NAME,
        registered_model_name=MLFLOW_MODEL_NAME,
    )

    mlflow.spark.log_model(
        spark_model=pipeline_model,
        artifact_path=MLFLOW_PIPELINE_NAME,
        registered_model_name=MLFLOW_PIPELINE_NAME,
    )

    params = {
        "max_iter": MAX_ITER,
        "reg_param": REG_PARAM,
        "elastic_net_param": ELASTIC_NET_PARAM,
    }
    mlflow.log_params(params)

    trainingSummary = lrModel.summary
    logger.info("RMSE: %f" % trainingSummary.rootMeanSquaredError)
    logger.info("r2: %f" % trainingSummary.r2)
    mlflow.log_metric("training rmse", trainingSummary.rootMeanSquaredError)
    mlflow.log_metric("training r2", trainingSummary.r2)

    test_results = lrModel.evaluate(test_df)
    logger.info("Test RMSE: %f" % test_results.rootMeanSquaredError)
    logger.info("Test r2: %f" % test_results.r2)
    mlflow.log_metric("test rmse", test_results.rootMeanSquaredError)
    mlflow.log_metric("test r2", test_results.r2)

mlflow.end_run()
