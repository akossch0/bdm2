{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     15\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../src\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m configure_logger\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m log_env, setup_env, validate_env\n\u001b[1;32m     20\u001b[0m logger \u001b[38;5;241m=\u001b[39m configure_logger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictive\u001b[39m\u001b[38;5;124m\"\u001b[39m, logging\u001b[38;5;241m.\u001b[39mDEBUG)\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, Imputer, SQLTransformer\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, countDistinct, when\n",
    "import mlflow.spark\n",
    "# add src to path\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from ..logging_config import configure_logger\n",
    "from ..utils import log_env, setup_env, validate_env\n",
    "\n",
    "logger = configure_logger(\"predictive\", logging.DEBUG)\n",
    "\n",
    "setup_env()\n",
    "validate_env()\n",
    "log_env()\n",
    "\n",
    "MONGODB_HOST = os.environ.get(\"MONGODB_HOST\")\n",
    "MONGODB_PORT = os.environ.get(\"MONGODB_PORT\")\n",
    "MONGODB_URI = f\"mongodb://{MONGODB_HOST}:{MONGODB_PORT}/\"\n",
    "MONGODB_DB = os.environ.get(\"MONGODB_DB\")\n",
    "\n",
    "SPARK_AVRO_JAR = os.environ.get(\"SPARK_AVRO_JAR\")\n",
    "SPARK_MONGO_CONNECTOR_JAR = os.environ.get(\"SPARK_MONGO_CONNECTOR_JAR\")\n",
    "CONF = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.master\", \"local\")\n",
    "    .set(\"spark.app.name\", \"BDM Spark formatted pipeline\")\n",
    "    .set(\n",
    "        \"spark.jars.packages\",\n",
    "        f\"{SPARK_AVRO_JAR},{SPARK_MONGO_CONNECTOR_JAR}\",\n",
    "    )\n",
    "    .set(\"spark.driver.extraJavaOptions\", \"-Dlog4j.configuration=file:log4j.properties\")\n",
    "    .set(\n",
    "        \"spark.executor.extraJavaOptions\", \"-Dlog4j.configuration=file:log4j.properties\"\n",
    "    )\n",
    "    .set(\"spark.mongodb.read.connection.uri\", MONGODB_URI)\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=CONF).getOrCreate()\n",
    "logger.info(f\"Python version = {spark.sparkContext.pythonVer}\")\n",
    "logger.info(f\"Spark version = {spark.version}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "logger.info(\"Loading data from MongoDB\")\n",
    "# Load the data from MongoDB\n",
    "idealista_df = (\n",
    "    spark.read.format(\"mongodb\")\n",
    "    .option(\"database\", MONGODB_DB)\n",
    "    .option(\"collection\", \"idealista\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Cast all integer features to double\n",
    "integer_columns = [field.name for field in idealista_df.schema.fields if 'IntegerType' in str(field.dataType)]\n",
    "cast_stages = []\n",
    "for col_name in integer_columns:\n",
    "    cast_stages.append(SQLTransformer(statement=f\"SELECT *, CAST({col_name} AS DOUBLE) AS {col_name}_cast FROM __THIS__\"))\n",
    "\n",
    "# Impute numeric (integer, double) features with the mean\n",
    "numeric_columns = [field.name for field in idealista_df.schema.fields if 'IntegerType' in str(field.dataType) or 'DoubleType' in str(field.dataType)]\n",
    "imputer = Imputer(inputCols=[f\"{c}_cast\" for c in numeric_columns], outputCols=[f\"{c}_imputed\" for c in numeric_columns]).setStrategy(\"mean\")\n",
    "\n",
    "# Cast all boolean features to string (categorical encoding)\n",
    "boolean_columns = [field.name for field in idealista_df.schema.fields if 'BooleanType' in str(field.dataType)]\n",
    "for col_name in boolean_columns:\n",
    "    cast_stages.append(SQLTransformer(statement=f\"SELECT *, CAST({col_name} AS STRING) AS {col_name}_cast FROM __THIS__\"))\n",
    "\n",
    "# Impute nulls in categorical features with 'Unknown'\n",
    "categorical_columns = [field.name for field in idealista_df.schema.fields if 'StringType' in str(field.dataType)]\n",
    "impute_expr = [when(col(c).isNull(), 'Unknown').otherwise(col(c)).alias(c) for c in categorical_columns]\n",
    "impute_stage = SQLTransformer(statement=f\"SELECT *, \" + \", \".join([str(expr) for expr in impute_expr]) + \" FROM __THIS__\")\n",
    "\n",
    "# Remove columns that only have one class from categorical features\n",
    "distinct_counts = idealista_df.select([countDistinct(col(c)).alias(c) for c in categorical_columns]).collect()[0].asDict()\n",
    "columns_to_keep = [k for k, v in distinct_counts.items() if v > 1]\n",
    "idealista_df = idealista_df.select(*columns_to_keep, *numeric_columns)\n",
    "\n",
    "# Prepare the features for the model\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_indexed\") for c in categorical_columns]\n",
    "encoders = [OneHotEncoder(inputCol=f\"{c}_indexed\", outputCol=f\"{c}_encoded\") for c in categorical_columns]\n",
    "\n",
    "# Assemble all the features into a vector\n",
    "feature_cols = [f\"{c}_encoded\" for c in categorical_columns] + [f\"{c}_imputed\" for c in numeric_columns]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Define the Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=cast_stages + [impute_stage] + indexers + encoders + [imputer, assembler, lr])\n",
    "\n",
    "# Train-test split\n",
    "train_df, test_df = idealista_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "# Log the model with mlflow\n",
    "mlflow.spark.log_model(model, \"linear_regression_model\")\n",
    "\n",
    "logger.info(\"Model training complete and logged with MLflow\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "logger.info(f\"Root Mean Squared Error (RMSE) on test data = {rmse}\")\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "logger.info(f\"R-squared on test data = {r2}\")\n",
    "\n",
    "# Output the evaluation metrics\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data = {rmse}\")\n",
    "print(f\"R-squared on test data = {r2}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
