{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_HOME = /root/uni-projects/bdm2/.venv/lib/python3.10/site-packages/pyspark\n",
      "JAVA_HOME = /root/.sdkman/candidates/java/current\n",
      "PATH = /root/uni-projects/bdm2/.venv/lib/python3.10/site-packages/pyspark/bin:/root/uni-projects/bdm2/.venv/bin:/root/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/bin/remote-cli:/root/.tfenv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/Users/Akos Schneider/.jdks/oracle-8/bin:/mnt/c/Program Files/Common Files/Oracle/Java/javapath:/mnt/c/Program Files (x86)/Common Files/Oracle/Java/javapath:/mnt/c/WINDOWS/system32:/mnt/c/WINDOWS:/mnt/c/WINDOWS/System32/Wbem:/mnt/c/WINDOWS/System32/WindowsPowerShell/v1.0/:/mnt/c/WINDOWS/System32/OpenSSH/:/mnt/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/mnt/c/Program Files/Git/cmd:/mnt/c/Program Files/TortoiseGit/bin:/mnt/c/Users/Akos Schneider/apache-maven-3.8.6/bin:/mnt/c/Program Files/nodejs/:/mnt/c/ProgramData/chocolatey/bin:/mnt/c/Users/Akos Schneider/terraform-1.3.4:/mnt/c/Users/Akos Schneider/.tfenv/bin:/mnt/c/WINDOWS/system32:/mnt/c/WINDOWS:/mnt/c/WINDOWS/System32/Wbem:/mnt/c/WINDOWS/System32/WindowsPowerShell/v1.0/:/mnt/c/WINDOWS/System32/OpenSSH/:/mnt/c/Program Files/NVIDIA Corporation/NVIDIA NvDLISR:/mnt/c/Users/Akos Schneider/.pyenv/pyenv-win/bin:/mnt/c/Users/Akos Schneider/.pyenv/pyenv-win/shims:/mnt/c/Users/Akos Schneider/anaconda3/Library/bin:/mnt/c/Users/Akos Schneider/anaconda3/Scripts:/mnt/c/Users/Akos Schneider/anaconda3/condabin:/mnt/c/Program Files/Cloudflare/Cloudflare WARP/:/mnt/c/Gradle/gradle-8.3-bin/gradle-8.3/bin:/mnt/c/Users/Akos Schneider/.pyenv/pyenv-win/versions/3.10.11/Scripts:/mnt/c/Program Files/PuTTY/:/Docker/host/bin:/mnt/c/hadoop/hadoop-3.3.6/bin:/mnt/c/Users/Akos Schneider/mongosh/mongosh-2.2.5-win32-x64/bin:/mnt/c/Users/Akos Schneider/.pyenv/pyenv-win/bin:/mnt/c/Users/Akos Schneider/.pyenv/pyenv-win/shims:/mnt/c/Users/Akos Schneider/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/Akos Schneider/AppData/Roaming/npm:/mnt/c/Users/Akos Schneider/AppData/Local/Google/Cloud SDK/google-cloud-sdk/bin:/mnt/c/Users/Akos Schneider/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/Akos Schneider/AppData/Local/Pandoc/:/mnt/c/Users/Akos Schneider/AppData/Local/JetBrains/Toolbox/scripts:/mnt/c/Users/Akos Schneider/AppData/Local/GitHubDesktop/bin:/mnt/c/Users/Akos Schneider/AppData/Roaming/TinyTeX/bin/windows:/mnt/c/Users/Akos Schneider/AppData/Local/Programs/mongosh/:/snap/bin\n",
      "PYSPARK_PYTHON = /root/uni-projects/bdm2/.venv/bin/python\n",
      "PYSPARK_DRIVER_PYTHON = /root/uni-projects/bdm2/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "from utils import validate_env, setup_env, log_env\n",
    "import os\n",
    "import sys\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "setup_env()\n",
    "validate_env()\n",
    "log_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/05/16 16:24:30 WARN Utils: Your hostname, akos-sch resolves to a loopback address: 127.0.1.1; using 172.23.175.226 instead (on interface eth0)\n",
      "24/05/16 16:24:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/root/uni-projects/bdm2/.venv/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-2ffab709-be25-4bb8-b6a4-8c438572f557;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.5.1 in central\n",
      "\tfound org.tukaani#xz;1.9 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;10.3.0 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.8.2 in central\n",
      "\t[4.8.2] org.mongodb#mongodb-driver-sync;[4.8.1,4.8.99)\n",
      "\tfound org.mongodb#bson;4.8.2 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.8.2 in central\n",
      "\tfound org.mongodb#bson-record-codec;4.8.2 in central\n",
      ":: resolution report :: resolve 2558ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.spark#spark-avro_2.12;3.5.1 from central in [default]\n",
      "\torg.mongodb#bson;4.8.2 from central in [default]\n",
      "\torg.mongodb#bson-record-codec;4.8.2 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.8.2 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.8.2 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;10.3.0 from central in [default]\n",
      "\torg.tukaani#xz;1.9 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   7   |   1   |   0   |   0   ||   7   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-2ffab709-be25-4bb8-b6a4-8c438572f557\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 7 already retrieved (0kB/4ms)\n",
      "24/05/16 16:24:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/16 16:24:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version = 3.10\n",
      "Spark version = 3.5.1\n",
      "[('spark.master', 'local'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.submit.pyFiles', '/root/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.5.1.jar,/root/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-10.3.0.jar,/root/.ivy2/jars/org.tukaani_xz-1.9.jar,/root/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.8.2.jar,/root/.ivy2/jars/org.mongodb_bson-4.8.2.jar,/root/.ivy2/jars/org.mongodb_mongodb-driver-core-4.8.2.jar,/root/.ivy2/jars/org.mongodb_bson-record-codec-4.8.2.jar'), ('spark.files', 'file:///root/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.5.1.jar,file:///root/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-10.3.0.jar,file:///root/.ivy2/jars/org.tukaani_xz-1.9.jar,file:///root/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.8.2.jar,file:///root/.ivy2/jars/org.mongodb_bson-4.8.2.jar,file:///root/.ivy2/jars/org.mongodb_mongodb-driver-core-4.8.2.jar,file:///root/.ivy2/jars/org.mongodb_bson-record-codec-4.8.2.jar'), ('spark.executor.id', 'driver'), ('spark.driver.port', '46607'), ('spark.mongodb.write.connection.uri', 'mongodb://127.0.0.1:27017/'), ('spark.repl.local.jars', 'file:///root/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.5.1.jar,file:///root/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-10.3.0.jar,file:///root/.ivy2/jars/org.tukaani_xz-1.9.jar,file:///root/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.8.2.jar,file:///root/.ivy2/jars/org.mongodb_bson-4.8.2.jar,file:///root/.ivy2/jars/org.mongodb_mongodb-driver-core-4.8.2.jar,file:///root/.ivy2/jars/org.mongodb_bson-record-codec-4.8.2.jar'), ('spark.app.initial.jar.urls', 'spark://172.23.175.226:46607/jars/org.mongodb_bson-4.8.2.jar,spark://172.23.175.226:46607/jars/org.apache.spark_spark-avro_2.12-3.5.1.jar,spark://172.23.175.226:46607/jars/org.mongodb.spark_mongo-spark-connector_2.12-10.3.0.jar,spark://172.23.175.226:46607/jars/org.mongodb_mongodb-driver-sync-4.8.2.jar,spark://172.23.175.226:46607/jars/org.mongodb_mongodb-driver-core-4.8.2.jar,spark://172.23.175.226:46607/jars/org.mongodb_bson-record-codec-4.8.2.jar,spark://172.23.175.226:46607/jars/org.tukaani_xz-1.9.jar'), ('spark.app.submitTime', '1715869473544'), ('spark.app.initial.file.urls', 'file:///root/.ivy2/jars/org.mongodb_bson-record-codec-4.8.2.jar,file:///root/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.8.2.jar,file:///root/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-10.3.0.jar,file:///root/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.5.1.jar,file:///root/.ivy2/jars/org.mongodb_bson-4.8.2.jar,file:///root/.ivy2/jars/org.mongodb_mongodb-driver-core-4.8.2.jar,file:///root/.ivy2/jars/org.tukaani_xz-1.9.jar'), ('spark.rdd.compress', 'True'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.driver.host', '172.23.175.226'), ('spark.mongodb.read.connection.uri', 'mongodb://127.0.0.1:27017/'), ('spark.serializer.objectStreamReset', '100'), ('spark.jars', 'file:///root/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.5.1.jar,file:///root/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-10.3.0.jar,file:///root/.ivy2/jars/org.tukaani_xz-1.9.jar,file:///root/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.8.2.jar,file:///root/.ivy2/jars/org.mongodb_bson-4.8.2.jar,file:///root/.ivy2/jars/org.mongodb_mongodb-driver-core-4.8.2.jar,file:///root/.ivy2/jars/org.mongodb_bson-record-codec-4.8.2.jar'), ('spark.app.name', 'Spark Dataframes Tutorial'), ('spark.submit.deployMode', 'client'), ('spark.jars.packages', 'org.apache.spark:spark-avro_2.12:3.5.1,org.mongodb.spark:mongo-spark-connector_2.12:10.3.0'), ('spark.ui.showConsoleProgress', 'true'), ('spark.app.startTime', '1715869473690'), ('spark.app.id', 'local-1715869474462')]\n"
     ]
    }
   ],
   "source": [
    "if spark:\n",
    "    spark.stop()\n",
    "# Create the configuration in the local machine and give a name to the application\n",
    "conf = SparkConf() \\\n",
    "    .set(\"spark.master\", \"local\") \\\n",
    "    .set(\"spark.app.name\", \"Spark Dataframes Tutorial\") \\\n",
    "    .set(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.5.1,org.mongodb.spark:mongo-spark-connector_2.12:10.3.0\")\n",
    "\n",
    "\n",
    "# Create the session \n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .config(\"spark.mongodb.read.connection.uri\", \"mongodb://127.0.0.1:27017/\") \\\n",
    "    .config(\"spark.mongodb.write.connection.uri\", \"mongodb://127.0.0.1:27017/\") \\\n",
    "    .getOrCreate()\n",
    "print(f\"Python version = {spark.sparkContext.pythonVer}\")\n",
    "print(f\"Spark version = {spark.version}\")\n",
    "print(spark.sparkContext.getConf().getAll())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_dir = \"../data/persistent-landing-zone/opendatabcn/income\"\n",
    "# get files\n",
    "files = os.listdir(income_dir)\n",
    "# keep only those that start with a year (4 digits)\n",
    "files = [file for file in files if file[:4].isdigit()]\n",
    "years = [file[:4] for file in files]\n",
    "# add the path to the files\n",
    "files = [f\"{income_dir}/{file}\" for file in files]\n",
    "# create a list of dataframes\n",
    "income_dfs = {}\n",
    "for file, year in zip(files, years):\n",
    "    # read the file\n",
    "    df = spark.read.format(\"avro\").load(file)\n",
    "    # append the dataframe to the list\n",
    "    income_dfs[year] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change all column names to lowercase\n",
    "for year in income_dfs:\n",
    "    df = income_dfs[year]\n",
    "    for col in df.columns:\n",
    "        df = df.withColumnRenamed(col, col.lower())\n",
    "    income_dfs[year] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of 2013 is different from schema of 2008\n",
      "índex rfd barcelona = 100 != índex rfd barcelona = 100\n",
      "StringType() != DoubleType()\n",
      "True != True\n",
      "{} != {}\n",
      "\n",
      "\n",
      "Schema of 2013 is different from schema of 2017\n",
      "índex rfd barcelona = 100 != índex rfd barcelona = 100\n",
      "StringType() != DoubleType()\n",
      "True != True\n",
      "{} != {}\n",
      "\n",
      "\n",
      "Schema of 2013 is different from schema of 2016\n",
      "índex rfd barcelona = 100 != índex rfd barcelona = 100\n",
      "StringType() != DoubleType()\n",
      "True != True\n",
      "{} != {}\n",
      "\n",
      "\n",
      "Schema of 2008 is different from schema of 2012\n",
      "índex rfd barcelona = 100 != índex rfd barcelona = 100\n",
      "DoubleType() != StringType()\n",
      "True != True\n",
      "{} != {}\n",
      "\n",
      "\n",
      "Schema of 2008 is different from schema of 2014\n",
      "índex rfd barcelona = 100 != índex rfd barcelona = 100\n",
      "DoubleType() != StringType()\n",
      "True != True\n",
      "{} != {}\n",
      "\n",
      "\n",
      "Schema of 2008 is different from schema of 2011\n",
      "índex rfd barcelona = 100 != índex rfd barcelona = 100\n",
      "DoubleType() != StringType()\n",
      "True != True\n",
      "{} != {}\n",
      "\n",
      "\n",
      "Schema of 2008 is different from schema of 2015\n",
      "índex rfd barcelona = 100 != índex rfd barcelona = 100\n",
      "DoubleType() != StringType()\n",
      "True != True\n",
      "{} != {}\n",
      "\n",
      "\n",
      "Schema of 2008 is different from schema of 2009\n",
      "índex rfd barcelona = 100 != índex rfd barcelona = 100\n",
      "DoubleType() != StringType()\n",
      "True != True\n",
      "{} != {}\n",
      "\n",
      "\n",
      "Schema of 2008 is different from schema of 2007\n",
      "índex rfd barcelona = 100 != índex rfd barcelona = 100\n",
      "DoubleType() != StringType()\n",
      "True != True\n",
      "{} != {}\n",
      "\n",
      "\n",
      "Schema of 2008 is different from schema of 2010\n",
      "índex rfd barcelona = 100 != índex rfd barcelona = 100\n",
      "DoubleType() != StringType()\n",
      "True != True\n",
      "{} != {}\n",
      "\n",
      "\n",
      "Schema of 2012 is different from schema of 2017\n",
      "índex rfd barcelona = 100 != índex rfd barcelona = 100\n",
      "StringType() != DoubleType()\n",
      "True != True\n",
      "{} != {}\n",
      "\n",
      "\n",
      "Schema of 2012 is different from schema of 2016\n",
      "índex rfd barcelona = 100 != índex rfd barcelona = 100\n",
      "StringType() != DoubleType()\n",
      "True != True\n",
      "{} != {}\n",
      "\n",
      "\n",
      "Schema of 2014 is different from schema of 2017\n",
      "índex rfd barcelona = 100 != índex rfd barcelona = 100\n",
      "StringType() != DoubleType()\n",
      "True != True\n",
      "{} != {}\n",
      "\n",
      "\n",
      "Schema of 2014 is different from schema of 2016\n",
      "índex rfd barcelona = 100 != índex rfd barcelona = 100\n",
      "StringType() != DoubleType()\n",
      "True != True\n",
      "{} != {}\n",
      "\n",
      "\n",
      "Schema of 2011 is different from schema of 2017\n",
      "índex rfd barcelona = 100 != índex rfd barcelona = 100\n",
      "StringType() != DoubleType()\n",
      "True != True\n",
      "{} != {}\n",
      "\n",
      "\n",
      "Schema of 2011 is different from schema of 2016\n",
      "índex rfd barcelona = 100 != índex rfd barcelona = 100\n",
      "StringType() != DoubleType()\n",
      "True != True\n",
      "{} != {}\n",
      "\n",
      "\n",
      "Schema of 2015 is different from schema of 2017\n",
      "índex rfd barcelona = 100 != índex rfd barcelona = 100\n",
      "StringType() != DoubleType()\n",
      "True != True\n",
      "{} != {}\n",
      "\n",
      "\n",
      "Schema of 2015 is different from schema of 2016\n",
      "índex rfd barcelona = 100 != índex rfd barcelona = 100\n",
      "StringType() != DoubleType()\n",
      "True != True\n",
      "{} != {}\n",
      "\n",
      "\n",
      "Schema of 2017 is different from schema of 2009\n",
      "índex rfd barcelona = 100 != índex rfd barcelona = 100\n",
      "DoubleType() != StringType()\n",
      "True != True\n",
      "{} != {}\n",
      "\n",
      "\n",
      "Schema of 2017 is different from schema of 2007\n",
      "índex rfd barcelona = 100 != índex rfd barcelona = 100\n",
      "DoubleType() != StringType()\n",
      "True != True\n",
      "{} != {}\n",
      "\n",
      "\n",
      "Schema of 2017 is different from schema of 2010\n",
      "índex rfd barcelona = 100 != índex rfd barcelona = 100\n",
      "DoubleType() != StringType()\n",
      "True != True\n",
      "{} != {}\n",
      "\n",
      "\n",
      "Schema of 2016 is different from schema of 2009\n",
      "índex rfd barcelona = 100 != índex rfd barcelona = 100\n",
      "DoubleType() != StringType()\n",
      "True != True\n",
      "{} != {}\n",
      "\n",
      "\n",
      "Schema of 2016 is different from schema of 2007\n",
      "índex rfd barcelona = 100 != índex rfd barcelona = 100\n",
      "DoubleType() != StringType()\n",
      "True != True\n",
      "{} != {}\n",
      "\n",
      "\n",
      "Schema of 2016 is different from schema of 2010\n",
      "índex rfd barcelona = 100 != índex rfd barcelona = 100\n",
      "DoubleType() != StringType()\n",
      "True != True\n",
      "{} != {}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checked_pairs = []\n",
    "# check pairwise schema equality\n",
    "for year1 in income_dfs:\n",
    "    for year2 in income_dfs:\n",
    "        if year1 != year2 and income_dfs[year1].schema != income_dfs[year2].schema and (year1, year2) not in checked_pairs:\n",
    "            checked_pairs.append((year1, year2))\n",
    "            checked_pairs.append((year2, year1))\n",
    "            print(f\"Schema of {year1} is different from schema of {year2}\")\n",
    "            # print the diff\n",
    "            schema1 = income_dfs[year1].schema\n",
    "            schema2 = income_dfs[year2].schema\n",
    "\n",
    "            for field1, field2 in zip(schema1, schema2):\n",
    "                if field1 != field2:\n",
    "                    print(f\"{field1.name} != {field2.name}\")\n",
    "                    print(f\"{field1.dataType} != {field2.dataType}\")\n",
    "                    print(f\"{field1.nullable} != {field2.nullable}\")\n",
    "                    print(f\"{field1.metadata} != {field2.metadata}\")\n",
    "                    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(any=2008, codi_districte=1, nom_districte='Ciutat Vella', codi_barri=1, nom_barri='el Raval', població=47431, índex rfd barcelona = 100=62.6),\n",
       " Row(any=2008, codi_districte=1, nom_districte='Ciutat Vella', codi_barri=2, nom_barri='el Barri Gòtic', població=25556, índex rfd barcelona = 100=80.8),\n",
       " Row(any=2008, codi_districte=1, nom_districte='Ciutat Vella', codi_barri=3, nom_barri='la Barceloneta', població=16000, índex rfd barcelona = 100=65.9),\n",
       " Row(any=2008, codi_districte=1, nom_districte='Ciutat Vella', codi_barri=4, nom_barri='Sant Pere, Santa Caterina i la Ribera', població=22649, índex rfd barcelona = 100=81.8),\n",
       " Row(any=2008, codi_districte=2, nom_districte='Eixample', codi_barri=5, nom_barri='el Fort Pienc', població=32167, índex rfd barcelona = 100=108.3)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "income_dfs['2008'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next one, it is string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(any=2013, codi_districte=1, nom_districte='Ciutat Vella', codi_barri=1, nom_barri='el Raval', població=49225, índex rfd barcelona = 100='60.3'),\n",
       " Row(any=2013, codi_districte=1, nom_districte='Ciutat Vella', codi_barri=2, nom_barri='el Barri Gòtic', població=16327, índex rfd barcelona = 100='103.6'),\n",
       " Row(any=2013, codi_districte=1, nom_districte='Ciutat Vella', codi_barri=3, nom_barri='la Barceloneta', població=15571, índex rfd barcelona = 100='82.1'),\n",
       " Row(any=2013, codi_districte=1, nom_districte='Ciutat Vella', codi_barri=4, nom_barri='Sant Pere, Santa Caterina i la Ribera', població=22821, índex rfd barcelona = 100='91.2'),\n",
       " Row(any=2013, codi_districte=2, nom_districte='Eixample', codi_barri=5, nom_barri='el Fort Pienc', població=31754, índex rfd barcelona = 100='99.0')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "income_dfs['2013'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 2013 has 1 non float like values in the 'índex rfd barcelona = 100' column\n",
      "The values are:\n",
      "+----+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "| any|codi_districte|nom_districte|codi_barri|nom_barri|població|índex rfd barcelona = 100|\n",
      "+----+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "|2013|            99|    No consta|        99|No consta|     205|                        -|\n",
      "+----+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "\n",
      "Year 2012 has 1 non float like values in the 'índex rfd barcelona = 100' column\n",
      "The values are:\n",
      "+----+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "| any|codi_districte|nom_districte|codi_barri|nom_barri|població|índex rfd barcelona = 100|\n",
      "+----+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "|2012|            99|    No consta|        99|No consta|     145|                        -|\n",
      "+----+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "\n",
      "Year 2014 has 1 non float like values in the 'índex rfd barcelona = 100' column\n",
      "The values are:\n",
      "+----+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "| any|codi_districte|nom_districte|codi_barri|nom_barri|població|índex rfd barcelona = 100|\n",
      "+----+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "|2014|            99|    No consta|        99|No consta|       1|                        -|\n",
      "+----+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "\n",
      "Year 2011 has 1 non float like values in the 'índex rfd barcelona = 100' column\n",
      "The values are:\n",
      "+----+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "| any|codi_districte|nom_districte|codi_barri|nom_barri|població|índex rfd barcelona = 100|\n",
      "+----+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "|2011|            99|    No consta|        99|No consta|      70|                        -|\n",
      "+----+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "\n",
      "Year 2015 has 1 non float like values in the 'índex rfd barcelona = 100' column\n",
      "The values are:\n",
      "+----+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "| any|codi_districte|nom_districte|codi_barri|nom_barri|població|índex rfd barcelona = 100|\n",
      "+----+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "|2015|            99|    No consta|        99|No consta|       1|                        -|\n",
      "+----+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "\n",
      "Year 2009 has 1 non float like values in the 'índex rfd barcelona = 100' column\n",
      "The values are:\n",
      "+----+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "| any|codi_districte|nom_districte|codi_barri|nom_barri|població|índex rfd barcelona = 100|\n",
      "+----+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "|2009|            99|    No consta|        99|No consta|     499|                        -|\n",
      "+----+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "\n",
      "Year 2007 has 1 non float like values in the 'índex rfd barcelona = 100' column\n",
      "The values are:\n",
      "+----+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "| any|codi_districte|nom_districte|codi_barri|nom_barri|població|índex rfd barcelona = 100|\n",
      "+----+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "|2007|            99|    No consta|        99|No consta|     879|                        -|\n",
      "+----+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "\n",
      "Year 2010 has 1 non float like values in the 'índex rfd barcelona = 100' column\n",
      "The values are:\n",
      "+----+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "| any|codi_districte|nom_districte|codi_barri|nom_barri|població|índex rfd barcelona = 100|\n",
      "+----+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "|2010|            99|    No consta|        99|No consta|     251|                        -|\n",
      "+----+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count the number of non float like values in the column\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "for year in income_dfs:\n",
    "    # if the 'índex rfd barcelona = 100' column is of type string\n",
    "    if income_dfs[year].schema['índex rfd barcelona = 100'].dataType == StringType():\n",
    "        # count the number of non float like values\n",
    "        print(f\"Year {year} has {income_dfs[year].filter(income_dfs[year]['índex rfd barcelona = 100'].rlike('[^0-9.]')).count()} non float like values in the 'índex rfd barcelona = 100' column\")\n",
    "        print(f\"The values are:\")\n",
    "        income_dfs[year].filter(income_dfs[year]['índex rfd barcelona = 100'].rlike('[^0-9.]')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|codi_districte|count|\n",
      "+--------------+-----+\n",
      "|             1|    4|\n",
      "|             6|    5|\n",
      "|             3|    8|\n",
      "|             5|    6|\n",
      "|             9|    7|\n",
      "|             4|    3|\n",
      "|             8|   13|\n",
      "|             7|   11|\n",
      "|            10|   10|\n",
      "|             2|    6|\n",
      "|            99|    1|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "income_dfs['2012'].groupBy('codi_districte').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|codi_barri|\n",
      "+----------+\n",
      "|        99|\n",
      "|        73|\n",
      "|        72|\n",
      "|        71|\n",
      "|        70|\n",
      "|        69|\n",
      "|        68|\n",
      "|        67|\n",
      "|        66|\n",
      "|        65|\n",
      "|        64|\n",
      "|        63|\n",
      "|        62|\n",
      "|        61|\n",
      "|        60|\n",
      "|        59|\n",
      "|        58|\n",
      "|        57|\n",
      "|        56|\n",
      "|        55|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# descending\n",
    "income_dfs['2012'].select('codi_barri').distinct().sort('codi_barri', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "73 neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out from all dfs where codi_districte is not in the range of 1 and 10, and codi_barri is not in the range of 1 and 73\n",
    "for year in income_dfs:\n",
    "    df = income_dfs[year]\n",
    "    df = df.filter(df['codi_districte'].between(1, 10) & df['codi_barri'].between(1, 73))\n",
    "    income_dfs[year] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 2013 has 0 non float like values in the 'índex rfd barcelona = 100' column\n",
      "The values are:\n",
      "+---+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "|any|codi_districte|nom_districte|codi_barri|nom_barri|població|índex rfd barcelona = 100|\n",
      "+---+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "+---+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "\n",
      "Year 2012 has 0 non float like values in the 'índex rfd barcelona = 100' column\n",
      "The values are:\n",
      "+---+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "|any|codi_districte|nom_districte|codi_barri|nom_barri|població|índex rfd barcelona = 100|\n",
      "+---+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "+---+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "\n",
      "Year 2014 has 0 non float like values in the 'índex rfd barcelona = 100' column\n",
      "The values are:\n",
      "+---+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "|any|codi_districte|nom_districte|codi_barri|nom_barri|població|índex rfd barcelona = 100|\n",
      "+---+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "+---+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "\n",
      "Year 2011 has 0 non float like values in the 'índex rfd barcelona = 100' column\n",
      "The values are:\n",
      "+---+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "|any|codi_districte|nom_districte|codi_barri|nom_barri|població|índex rfd barcelona = 100|\n",
      "+---+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "+---+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "\n",
      "Year 2015 has 0 non float like values in the 'índex rfd barcelona = 100' column\n",
      "The values are:\n",
      "+---+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "|any|codi_districte|nom_districte|codi_barri|nom_barri|població|índex rfd barcelona = 100|\n",
      "+---+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "+---+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "\n",
      "Year 2009 has 0 non float like values in the 'índex rfd barcelona = 100' column\n",
      "The values are:\n",
      "+---+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "|any|codi_districte|nom_districte|codi_barri|nom_barri|població|índex rfd barcelona = 100|\n",
      "+---+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "+---+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "\n",
      "Year 2007 has 0 non float like values in the 'índex rfd barcelona = 100' column\n",
      "The values are:\n",
      "+---+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "|any|codi_districte|nom_districte|codi_barri|nom_barri|població|índex rfd barcelona = 100|\n",
      "+---+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "+---+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "\n",
      "Year 2010 has 0 non float like values in the 'índex rfd barcelona = 100' column\n",
      "The values are:\n",
      "+---+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "|any|codi_districte|nom_districte|codi_barri|nom_barri|població|índex rfd barcelona = 100|\n",
      "+---+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "+---+--------------+-------------+----------+---------+--------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count the number of non float like values in the column\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "for year in income_dfs:\n",
    "    # if the 'índex rfd barcelona = 100' column is of type string\n",
    "    if income_dfs[year].schema['índex rfd barcelona = 100'].dataType == StringType():\n",
    "        # count the number of non float like values\n",
    "        print(f\"Year {year} has {income_dfs[year].filter(income_dfs[year]['índex rfd barcelona = 100'].rlike('[^0-9.]')).count()} non float like values in the 'índex rfd barcelona = 100' column\")\n",
    "        print(f\"The values are:\")\n",
    "        income_dfs[year].filter(income_dfs[year]['índex rfd barcelona = 100'].rlike('[^0-9.]')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a udf to convert the string to a type double, given a value of string '60.3' it will return 60.3\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def str_to_double(value):\n",
    "    converted = None\n",
    "    try:\n",
    "        converted = float(value)\n",
    "    except:\n",
    "        converted = None\n",
    "    return converted\n",
    "\n",
    "str_to_double_udf = udf(str_to_double, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the udf to the column named 'índex rfd barcelona = 100'\n",
    "for year in income_dfs:\n",
    "    income_dfs[year] = income_dfs[year].withColumn('índex rfd barcelona = 100', str_to_double_udf(col('índex rfd barcelona = 100')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "checked_pairs = []\n",
    "# check pairwise schema equality\n",
    "for year1 in income_dfs:\n",
    "    for year2 in income_dfs:\n",
    "        if year1 != year2 and income_dfs[year1].schema != income_dfs[year2].schema and (year1, year2) not in checked_pairs:\n",
    "            checked_pairs.append((year1, year2))\n",
    "            checked_pairs.append((year2, year1))\n",
    "            print(f\"Schema of {year1} is different from schema of {year2}\")\n",
    "            # print the diff\n",
    "            schema1 = income_dfs[year1].schema\n",
    "            schema2 = income_dfs[year2].schema\n",
    "\n",
    "            for field1, field2 in zip(schema1, schema2):\n",
    "                if field1 != field2:\n",
    "                    print(f\"{field1.name} != {field2.name}\")\n",
    "                    print(f\"{field1.dataType} != {field2.dataType}\")\n",
    "                    print(f\"{field1.nullable} != {field2.nullable}\")\n",
    "                    print(f\"{field1.metadata} != {field2.metadata}\")\n",
    "                    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue resolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before filtering in year 2013: 73\n",
      "Rows after filtering in year 2013: 73\n",
      "Rows before filtering in year 2008: 73\n",
      "Rows after filtering in year 2008: 73\n",
      "Rows before filtering in year 2012: 73\n",
      "Rows after filtering in year 2012: 73\n",
      "Rows before filtering in year 2014: 73\n",
      "Rows after filtering in year 2014: 73\n",
      "Rows before filtering in year 2011: 73\n",
      "Rows after filtering in year 2011: 73\n",
      "Rows before filtering in year 2015: 73\n",
      "Rows after filtering in year 2015: 73\n",
      "Rows before filtering in year 2017: 73\n",
      "Rows after filtering in year 2017: 73\n",
      "Rows before filtering in year 2016: 73\n",
      "Rows after filtering in year 2016: 73\n",
      "Rows before filtering in year 2009: 73\n",
      "Rows after filtering in year 2009: 73\n",
      "Rows before filtering in year 2007: 73\n",
      "Rows after filtering in year 2007: 73\n",
      "Rows before filtering in year 2010: 73\n",
      "Rows after filtering in year 2010: 73\n"
     ]
    }
   ],
   "source": [
    "# print if the year corresponding in the dictionary and any row in the any field does not match\n",
    "for year in income_dfs:\n",
    "    df_year = income_dfs[year]\n",
    "    # filter rows where any field is not equal to the year\n",
    "    print(f\"Rows before filtering in year {year}: {df_year.count()}\")\n",
    "    df_year = df_year.filter(~(df_year['any'] != year))\n",
    "    print(f\"Rows after filtering in year {year}: {df_year.count()}\")\n",
    "    income_dfs[year] = df_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all the dataframes into a single dataframe\n",
    "income_df = None\n",
    "for year in income_dfs:\n",
    "    if income_df is None:\n",
    "        income_df = income_dfs[year]\n",
    "    else:\n",
    "        income_df = income_df.union(income_dfs[year])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n",
      "73\n",
      "73\n",
      "73\n",
      "73\n",
      "73\n",
      "73\n",
      "73\n",
      "73\n",
      "73\n",
      "73\n",
      "Sum: 803\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for year in income_dfs:\n",
    "    print(income_dfs[year].count())\n",
    "    sum += income_dfs[year].count()\n",
    "print(\"Sum:\", sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "803"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "income_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before deduplication: 803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 210:==================================================>    (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after deduplication: 803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# deduplicate the dataframe\n",
    "print(f\"Number of rows before deduplication: {income_df.count()}\")\n",
    "income_df = income_df.dropDuplicates()\n",
    "print(f\"Number of rows after deduplication: {income_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 216:==================================================>    (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+-------------+----------+-------------------+--------+-------------------------+\n",
      "| any|codi_districte|nom_districte|codi_barri|          nom_barri|població|índex rfd barcelona = 100|\n",
      "+----+--------------+-------------+----------+-------------------+--------+-------------------------+\n",
      "|2013|             8|   Nou Barris|        52|     la Prosperitat|   26320|                     56.3|\n",
      "|2013|             8|   Nou Barris|        46|el Turó de la Peira|   15307|                     51.6|\n",
      "+----+--------------+-------------+----------+-------------------+--------+-------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "income_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, NumericType, DoubleType\n",
    "from pyspark.sql.functions import approxCountDistinct\n",
    "\n",
    "def detect_continuous_variables(df, distinct_threshold, drop_vars = []):\n",
    "    continuous_columns = []\n",
    "    for column in df.drop(*drop_vars).columns:\n",
    "        dtype = df.schema[column].dataType\n",
    "        if isinstance(dtype, (IntegerType, NumericType, DoubleType)):\n",
    "            distinct_count = df.select(approxCountDistinct(column)).collect()[0][0]\n",
    "            if distinct_count > distinct_threshold:\n",
    "                continuous_columns.append(column)\n",
    "    return continuous_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/uni-projects/bdm2/.venv/lib/python3.10/site-packages/pyspark/sql/functions.py:3796: FutureWarning: Deprecated in 2.1, use approx_count_distinct instead.\n",
      "  warnings.warn(\"Deprecated in 2.1, use approx_count_distinct instead.\", FutureWarning)\n",
      "24/05/15 18:21:04 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 231:=============================================>          (9 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['any', 'població', 'índex rfd barcelona = 100']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "continuous_variables = detect_continuous_variables(income_df, 10, ['codi_districte', 'codi_barri'])\n",
    "print(continuous_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, exp, lit\n",
    "def iqr_outlier_treatment(dataframe, columns, factor=1.5, delete_outliers=False):\n",
    "    \"\"\"\n",
    "    Detects and treats outliers using IQR for multiple variables in a PySpark DataFrame.\n",
    "\n",
    "    :param dataframe: The input PySpark DataFrame\n",
    "    :param columns: A list of columns to apply IQR outlier treatment\n",
    "    :param factor: The IQR factor to use for detecting outliers (default is 1.5)\n",
    "    :return: The processed DataFrame with outliers treated\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        # Calculate Q1, Q3, and IQR\n",
    "        quantiles = dataframe.approxQuantile(column, [0.25, 0.75], 0.01)\n",
    "        q1, q3 = quantiles[0], quantiles[1]\n",
    "        iqr = q3 - q1\n",
    "\n",
    "        # Define the upper and lower bounds for outliers\n",
    "        lower_bound = q1 - factor * iqr\n",
    "        upper_bound = q3 + factor * iqr\n",
    "\n",
    "        # print thresholds and outliers\n",
    "        print(f\"Column: {column}\")\n",
    "        print(f\"Q1: {q1}\")\n",
    "        print(f\"Q3: {q3}\")\n",
    "        print(f\"IQR: {iqr}\")\n",
    "        print(f\"Lower Bound: {lower_bound}\")\n",
    "        print(f\"Upper Bound: {upper_bound}\")\n",
    "        num_outliers = dataframe.filter((col(column) < lit(lower_bound)) | (col(column) > lit(upper_bound))).count()\n",
    "        print(f\"Number of outliers: {num_outliers}\")\n",
    "        if num_outliers > 0:\n",
    "            print(f\"Outliers:\")\n",
    "            dataframe.filter((col(column) < lit(lower_bound)) | (col(column) > lit(upper_bound))).show()\n",
    "\n",
    "        # Filter outliers and update the DataFrame\n",
    "        if delete_outliers:\n",
    "            dataframe = dataframe.filter((col(column) >= lit(lower_bound)) & (col(column) <= lit(upper_bound)))\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: any\n",
      "Q1: 2009.0\n",
      "Q3: 2015.0\n",
      "IQR: 6.0\n",
      "Lower Bound: 1991.0\n",
      "Upper Bound: 2033.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: població\n",
      "Q1: 10355.0\n",
      "Q3: 30128.0\n",
      "IQR: 19773.0\n",
      "Lower Bound: -48964.0\n",
      "Upper Bound: 89447.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: índex rfd barcelona = 100\n",
      "Q1: 66.3\n",
      "Q3: 103.0\n",
      "IQR: 36.7\n",
      "Lower Bound: -43.80000000000001\n",
      "Upper Bound: 213.10000000000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 17\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 258:=============================================>          (9 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+-------------------+----------+---------------+--------+-------------------------+\n",
      "| any|codi_districte|      nom_districte|codi_barri|      nom_barri|població|índex rfd barcelona = 100|\n",
      "+----+--------------+-------------------+----------+---------------+--------+-------------------------+\n",
      "|2013|             4|          Les Corts|        21|      Pedralbes|   11784|                    243.9|\n",
      "|2013|             5|Sarrià-Sant Gervasi|        24|les Tres Torres|   16087|                    224.0|\n",
      "|2008|             5|Sarrià-Sant Gervasi|        24|les Tres Torres|   15410|                    214.7|\n",
      "|2012|             5|Sarrià-Sant Gervasi|        24|les Tres Torres|   16008|                    215.0|\n",
      "|2012|             4|          Les Corts|        21|      Pedralbes|   11778|                    240.7|\n",
      "|2014|             4|          Les Corts|        21|      Pedralbes|   11670|                    251.7|\n",
      "|2014|             5|Sarrià-Sant Gervasi|        24|les Tres Torres|   16381|                    217.8|\n",
      "|2011|             5|Sarrià-Sant Gervasi|        24|les Tres Torres|   15700|                    216.4|\n",
      "|2011|             4|          Les Corts|        21|      Pedralbes|   11629|                    246.0|\n",
      "|2015|             4|          Les Corts|        21|      Pedralbes|   11763|                    250.5|\n",
      "|2015|             5|Sarrià-Sant Gervasi|        24|les Tres Torres|   16624|                    214.1|\n",
      "|2017|             5|Sarrià-Sant Gervasi|        24|les Tres Torres|   16660|                    215.8|\n",
      "|2017|             4|          Les Corts|        21|      Pedralbes|   12117|                    248.8|\n",
      "|2016|             4|          Les Corts|        21|      Pedralbes|   11864|                    242.4|\n",
      "|2009|             5|Sarrià-Sant Gervasi|        24|les Tres Torres|   15832|                    222.6|\n",
      "|2007|             5|Sarrià-Sant Gervasi|        24|les Tres Torres|   15325|                    215.3|\n",
      "|2010|             5|Sarrià-Sant Gervasi|        24|les Tres Torres|   15807|                    225.8|\n",
      "+----+--------------+-------------------+----------+---------------+--------+-------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "income_df = iqr_outlier_treatment(income_df, continuous_variables, factor=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "income_df.write.format(\"mongodb\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"database\", \"bdm\") \\\n",
    "    .option(\"collection\", \"income\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idealista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "idealista_dir = \"../data/persistent-landing-zone/idealista\"\n",
    "# get files\n",
    "files = os.listdir(idealista_dir)\n",
    "# keep only those that start with a year (4 digits)\n",
    "files = [file for file in files if file[:4].isdigit()]\n",
    "days = [file[:10] for file in files]\n",
    "# add the path to the files\n",
    "files = [f\"{idealista_dir}/{file}\" for file in files]\n",
    "# create a list of dataframes\n",
    "idealista_dfs = {}\n",
    "for file, day in zip(files, days):\n",
    "    # read the file\n",
    "    df = spark.read.format(\"avro\").load(file)\n",
    "    # append the dataframe to the list\n",
    "    idealista_dfs[day] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idealista_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change all column names to lowercase\n",
    "for day in idealista_dfs:\n",
    "    df = idealista_dfs[day]\n",
    "    for col in df.columns:\n",
    "        df = df.withColumnRenamed(col, col.lower())\n",
    "    idealista_dfs[day] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark struct type\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "# get all fields that are roots of nested fields\n",
    "nested_fields = set()\n",
    "for day in idealista_dfs:\n",
    "    df = idealista_dfs[day]\n",
    "    for field in df.schema.fields:\n",
    "        if isinstance(field.dataType, StructType):\n",
    "            nested_fields.add(field.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detailedtype', 'parkingspace', 'suggestedtexts'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nested_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "def flatten_dataframe(df):\n",
    "    # Check for nested fields in DataFrame schema\n",
    "    complex_fields = [f.name for f in df.schema.fields if f.dataType.simpleString().startswith('struct')]\n",
    "\n",
    "    while complex_fields:\n",
    "        col_exprs = []\n",
    "        for field in df.schema.fields:\n",
    "            if field.dataType.simpleString().startswith('struct'):\n",
    "                # Add nested fields to the column expression list\n",
    "                nested_fields = field.dataType.fields\n",
    "                col_exprs += [col(f\"{field.name}.{nested_field.name}\").alias(f\"{field.name}_{nested_field.name}\") for nested_field in nested_fields]\n",
    "            else:\n",
    "                # Retain non-nested fields as is\n",
    "                col_exprs.append(col(field.name))\n",
    "        \n",
    "        # Select all columns based on the constructed column expressions\n",
    "        df = df.select(*col_exprs)\n",
    "        # Check again if there are nested fields after transformation\n",
    "        complex_fields = [f.name for f in df.schema.fields if f.dataType.simpleString().startswith('struct')]\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in idealista_dfs:\n",
    "    idealista_dfs[day] = flatten_dataframe(idealista_dfs[day])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 41 unique columns in the dataframes\n"
     ]
    }
   ],
   "source": [
    "all_columns = set()\n",
    "for day in idealista_dfs:\n",
    "    # print nr of cols\n",
    "    all_columns = all_columns.union(set(idealista_dfs[day].columns))\n",
    "    # exclude nested fields as they have been flattened\n",
    "    all_columns = all_columns - nested_fields\n",
    "print(f\"There are {len(all_columns)} unique columns in the dataframes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- propertycode: string (nullable = true)\n",
      " |-- thumbnail: string (nullable = true)\n",
      " |-- externalreference: string (nullable = true)\n",
      " |-- numphotos: integer (nullable = true)\n",
      " |-- floor: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- propertytype: string (nullable = true)\n",
      " |-- operation: string (nullable = true)\n",
      " |-- size: double (nullable = true)\n",
      " |-- exterior: boolean (nullable = true)\n",
      " |-- rooms: integer (nullable = true)\n",
      " |-- bathrooms: integer (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- district: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- neighborhood: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- showaddress: boolean (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- distance: string (nullable = true)\n",
      " |-- hasvideo: boolean (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- newdevelopment: boolean (nullable = true)\n",
      " |-- haslift: boolean (nullable = true)\n",
      " |-- pricebyarea: double (nullable = true)\n",
      " |-- hasplan: boolean (nullable = true)\n",
      " |-- has3dtour: boolean (nullable = true)\n",
      " |-- has360: boolean (nullable = true)\n",
      " |-- hasstaging: boolean (nullable = true)\n",
      " |-- topnewdevelopment: boolean (nullable = true)\n",
      " |-- detailedtype_typology: string (nullable = true)\n",
      " |-- detailedtype_subTypology: string (nullable = true)\n",
      " |-- suggestedtexts_subtitle: string (nullable = true)\n",
      " |-- suggestedtexts_title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idealista_dfs['2020_01_02'].printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'address',\n",
       " 'bathrooms',\n",
       " 'country',\n",
       " 'detailedtype_subTypology',\n",
       " 'detailedtype_typology',\n",
       " 'distance',\n",
       " 'district',\n",
       " 'exterior',\n",
       " 'externalreference',\n",
       " 'floor',\n",
       " 'has360',\n",
       " 'has3dtour',\n",
       " 'haslift',\n",
       " 'hasplan',\n",
       " 'hasstaging',\n",
       " 'hasvideo',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'municipality',\n",
       " 'neighborhood',\n",
       " 'newdevelopment',\n",
       " 'newdevelopmentfinished',\n",
       " 'numphotos',\n",
       " 'operation',\n",
       " 'parkingspace_hasParkingSpace',\n",
       " 'parkingspace_isParkingSpaceIncludedInPrice',\n",
       " 'parkingspace_parkingSpacePrice',\n",
       " 'price',\n",
       " 'pricebyarea',\n",
       " 'propertycode',\n",
       " 'propertytype',\n",
       " 'province',\n",
       " 'rooms',\n",
       " 'showaddress',\n",
       " 'size',\n",
       " 'status',\n",
       " 'suggestedtexts_subtitle',\n",
       " 'suggestedtexts_title',\n",
       " 'thumbnail',\n",
       " 'topnewdevelopment',\n",
       " 'url'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine column data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'propertycode': StringType(),\n",
       " 'thumbnail': StringType(),\n",
       " 'externalreference': StringType(),\n",
       " 'numphotos': IntegerType(),\n",
       " 'floor': StringType(),\n",
       " 'price': DoubleType(),\n",
       " 'propertytype': StringType(),\n",
       " 'operation': StringType(),\n",
       " 'size': DoubleType(),\n",
       " 'exterior': BooleanType(),\n",
       " 'rooms': IntegerType(),\n",
       " 'bathrooms': IntegerType(),\n",
       " 'address': StringType(),\n",
       " 'province': StringType(),\n",
       " 'municipality': StringType(),\n",
       " 'district': StringType(),\n",
       " 'country': StringType(),\n",
       " 'latitude': DoubleType(),\n",
       " 'longitude': DoubleType(),\n",
       " 'showaddress': BooleanType(),\n",
       " 'url': StringType(),\n",
       " 'distance': StringType(),\n",
       " 'hasvideo': BooleanType(),\n",
       " 'status': StringType(),\n",
       " 'newdevelopment': BooleanType(),\n",
       " 'haslift': BooleanType(),\n",
       " 'pricebyarea': DoubleType(),\n",
       " 'hasplan': BooleanType(),\n",
       " 'has3dtour': BooleanType(),\n",
       " 'has360': BooleanType(),\n",
       " 'hasstaging': BooleanType(),\n",
       " 'topnewdevelopment': BooleanType(),\n",
       " 'parkingspace_hasParkingSpace': BooleanType(),\n",
       " 'parkingspace_isParkingSpaceIncludedInPrice': BooleanType(),\n",
       " 'parkingspace_parkingSpacePrice': DoubleType(),\n",
       " 'detailedtype_typology': StringType(),\n",
       " 'detailedtype_subTypology': StringType(),\n",
       " 'suggestedtexts_subtitle': StringType(),\n",
       " 'suggestedtexts_title': StringType(),\n",
       " 'newdevelopmentfinished': BooleanType(),\n",
       " 'neighborhood': StringType()}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import NullType, StringType, IntegerType, DoubleType, BooleanType\n",
    "\n",
    "column_data_types = {}\n",
    "for day in idealista_dfs:\n",
    "    df = idealista_dfs[day]\n",
    "    for column in df.columns:\n",
    "        # if it has not been added and is not of Null type\n",
    "        if column not in column_data_types and df.schema[column].dataType != NullType():\n",
    "            column_data_types[column] = df.schema[column].dataType\n",
    "column_data_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance should be of type IntegerType\n",
    "column_data_types['distance'] = IntegerType()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(column_data_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 missing columns in the dataframe for day 2020_10_16, the columns are: {'neighborhood'}\n",
      "There are 2 missing columns in the dataframe for day 2020_04_05, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 2 missing columns in the dataframe for day 2020_02_10, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 2 missing columns in the dataframe for day 2020_08_17, the columns are: {'parkingspace_parkingSpacePrice', 'neighborhood'}\n",
      "There are 2 missing columns in the dataframe for day 2020_08_15, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2020_06_03, the columns are: {'newdevelopmentfinished'}\n",
      "There are 2 missing columns in the dataframe for day 2020_08_29, the columns are: {'newdevelopmentfinished', 'neighborhood'}\n",
      "There are 1 missing columns in the dataframe for day 2020_12_28, the columns are: {'neighborhood'}\n",
      "There are 2 missing columns in the dataframe for day 2020_05_29, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 2 missing columns in the dataframe for day 2020_08_21, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 3 missing columns in the dataframe for day 2020_10_17, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice', 'detailedtype_subTypology'}\n",
      "There are 3 missing columns in the dataframe for day 2021_02_24, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice', 'detailedtype_subTypology'}\n",
      "There are 2 missing columns in the dataframe for day 2020_10_24, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2020_12_12, the columns are: {'newdevelopmentfinished'}\n",
      "There are 4 missing columns in the dataframe for day 2020_01_24, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice', 'parkingspace_isParkingSpaceIncludedInPrice', 'parkingspace_hasParkingSpace'}\n",
      "There are 2 missing columns in the dataframe for day 2020_12_31, the columns are: {'parkingspace_parkingSpacePrice', 'neighborhood'}\n",
      "There are 2 missing columns in the dataframe for day 2020_03_07, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 2 missing columns in the dataframe for day 2020_02_07, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2020_05_17, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_03_05, the columns are: {'neighborhood'}\n",
      "There are 1 missing columns in the dataframe for day 2020_03_02, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_09_20, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_08_07, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_10_27, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2021_02_15, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_07_26, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_05_08, the columns are: {'neighborhood'}\n",
      "There are 2 missing columns in the dataframe for day 2020_08_12, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2020_03_28, the columns are: {'newdevelopmentfinished'}\n",
      "There are 2 missing columns in the dataframe for day 2020_09_15, the columns are: {'newdevelopmentfinished', 'neighborhood'}\n",
      "There are 1 missing columns in the dataframe for day 2020_07_19, the columns are: {'newdevelopmentfinished'}\n",
      "There are 2 missing columns in the dataframe for day 2020_04_15, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2020_07_29, the columns are: {'newdevelopmentfinished'}\n",
      "There are 2 missing columns in the dataframe for day 2021_02_12, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 2 missing columns in the dataframe for day 2020_07_17, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2020_01_30, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_06_06, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_06_04, the columns are: {'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2020_08_26, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_08_01, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_08_06, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_12_11, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2021_01_20, the columns are: {'newdevelopmentfinished'}\n",
      "There are 2 missing columns in the dataframe for day 2020_09_28, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 2 missing columns in the dataframe for day 2020_04_29, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 3 missing columns in the dataframe for day 2021_02_14, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice', 'neighborhood'}\n",
      "There are 2 missing columns in the dataframe for day 2020_06_02, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2020_05_06, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_07_14, the columns are: {'parkingspace_parkingSpacePrice'}\n",
      "There are 2 missing columns in the dataframe for day 2020_11_19, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2020_11_25, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_06_01, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2021_02_19, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_11_15, the columns are: {'newdevelopmentfinished'}\n",
      "There are 2 missing columns in the dataframe for day 2020_04_16, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2021_02_22, the columns are: {'newdevelopmentfinished'}\n",
      "There are 2 missing columns in the dataframe for day 2020_06_13, the columns are: {'parkingspace_parkingSpacePrice', 'neighborhood'}\n",
      "There are 1 missing columns in the dataframe for day 2020_09_04, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_02_11, the columns are: {'newdevelopmentfinished'}\n",
      "There are 4 missing columns in the dataframe for day 2020_01_02, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice', 'parkingspace_isParkingSpaceIncludedInPrice', 'parkingspace_hasParkingSpace'}\n",
      "There are 1 missing columns in the dataframe for day 2020_11_17, the columns are: {'newdevelopmentfinished'}\n",
      "There are 2 missing columns in the dataframe for day 2020_12_03, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 2 missing columns in the dataframe for day 2020_09_06, the columns are: {'newdevelopmentfinished', 'neighborhood'}\n",
      "There are 1 missing columns in the dataframe for day 2020_01_13, the columns are: {'newdevelopmentfinished'}\n",
      "There are 2 missing columns in the dataframe for day 2020_08_09, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2021_01_25, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_12_14, the columns are: {'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2020_05_31, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_11_01, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2021_01_07, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_12_04, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_12_09, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_07_23, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_05_12, the columns are: {'newdevelopmentfinished'}\n",
      "There are 2 missing columns in the dataframe for day 2020_10_25, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 2 missing columns in the dataframe for day 2020_04_25, the columns are: {'parkingspace_parkingSpacePrice', 'neighborhood'}\n",
      "There are 2 missing columns in the dataframe for day 2021_02_25, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2020_02_21, the columns are: {'newdevelopmentfinished'}\n",
      "There are 2 missing columns in the dataframe for day 2020_05_25, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 2 missing columns in the dataframe for day 2020_09_07, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 2 missing columns in the dataframe for day 2020_05_04, the columns are: {'newdevelopmentfinished', 'neighborhood'}\n",
      "There are 1 missing columns in the dataframe for day 2020_11_07, the columns are: {'parkingspace_parkingSpacePrice'}\n",
      "There are 2 missing columns in the dataframe for day 2020_03_03, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2020_01_08, the columns are: {'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2020_07_18, the columns are: {'neighborhood'}\n",
      "There are 2 missing columns in the dataframe for day 2020_06_22, the columns are: {'newdevelopmentfinished', 'detailedtype_subTypology'}\n",
      "There are 41 missing columns in the dataframe for day 2020_11_11, the columns are: {'externalreference', 'rooms', 'newdevelopmentfinished', 'hasvideo', 'url', 'propertycode', 'pricebyarea', 'haslift', 'numphotos', 'parkingspace_isParkingSpaceIncludedInPrice', 'parkingspace_hasParkingSpace', 'price', 'has3dtour', 'longitude', 'distance', 'province', 'showaddress', 'propertytype', 'neighborhood', 'district', 'thumbnail', 'has360', 'newdevelopment', 'hasplan', 'latitude', 'size', 'topnewdevelopment', 'operation', 'floor', 'exterior', 'status', 'parkingspace_parkingSpacePrice', 'suggestedtexts_title', 'hasstaging', 'suggestedtexts_subtitle', 'country', 'address', 'bathrooms', 'municipality', 'detailedtype_typology', 'detailedtype_subTypology'}\n",
      "There are 1 missing columns in the dataframe for day 2020_07_27, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_08_03, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_01_10, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_01_23, the columns are: {'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2020_10_14, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_10_31, the columns are: {'parkingspace_parkingSpacePrice'}\n",
      "There are 5 missing columns in the dataframe for day 2020_10_15, the columns are: {'parkingspace_parkingSpacePrice', 'newdevelopmentfinished', 'parkingspace_isParkingSpaceIncludedInPrice', 'parkingspace_hasParkingSpace', 'detailedtype_subTypology'}\n",
      "There are 2 missing columns in the dataframe for day 2020_02_26, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2020_07_15, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_12_10, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_08_02, the columns are: {'newdevelopmentfinished'}\n",
      "There are 2 missing columns in the dataframe for day 2020_10_19, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 2 missing columns in the dataframe for day 2020_11_06, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2021_01_03, the columns are: {'neighborhood'}\n",
      "There are 1 missing columns in the dataframe for day 2020_06_05, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_10_30, the columns are: {'neighborhood'}\n",
      "There are 2 missing columns in the dataframe for day 2020_08_05, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n"
     ]
    }
   ],
   "source": [
    "nr_of_incomplete_dfs = 0\n",
    "frequently_missing_cols = set()\n",
    "for day in idealista_dfs:\n",
    "    df = idealista_dfs[day]\n",
    "    missing_cols = all_columns - set(df.columns)\n",
    "    frequently_missing_cols = frequently_missing_cols.union(missing_cols)\n",
    "    if len(missing_cols) > 0:\n",
    "        nr_of_incomplete_dfs += 1\n",
    "        print(f\"There are {len(missing_cols)} missing columns in the dataframe for day {day}, the columns are: {missing_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# observe 2020_11_11\n",
    "idealista_dfs['2020_11_11'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous project, we conducted per file schema alignement, so this flew under our radar. Now that we want to merge all the datasets, we see that there is 2020_11_11 with literally 0 data, confirmed at the source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check agains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 104 incomplete dataframes'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"There are {nr_of_incomplete_dfs} incomplete dataframes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 missing columns in the dataframe for day 2020_10_16, the columns are: {'neighborhood'}\n",
      "There are 2 missing columns in the dataframe for day 2020_04_05, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 2 missing columns in the dataframe for day 2020_02_10, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 2 missing columns in the dataframe for day 2020_08_17, the columns are: {'parkingspace_parkingSpacePrice', 'neighborhood'}\n",
      "There are 2 missing columns in the dataframe for day 2020_08_15, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2020_06_03, the columns are: {'newdevelopmentfinished'}\n",
      "There are 2 missing columns in the dataframe for day 2020_08_29, the columns are: {'newdevelopmentfinished', 'neighborhood'}\n",
      "There are 1 missing columns in the dataframe for day 2020_12_28, the columns are: {'neighborhood'}\n",
      "There are 2 missing columns in the dataframe for day 2020_05_29, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 2 missing columns in the dataframe for day 2020_08_21, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 3 missing columns in the dataframe for day 2020_10_17, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice', 'detailedtype_subTypology'}\n",
      "There are 3 missing columns in the dataframe for day 2021_02_24, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice', 'detailedtype_subTypology'}\n",
      "There are 2 missing columns in the dataframe for day 2020_10_24, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2020_12_12, the columns are: {'newdevelopmentfinished'}\n",
      "There are 4 missing columns in the dataframe for day 2020_01_24, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice', 'parkingspace_isParkingSpaceIncludedInPrice', 'parkingspace_hasParkingSpace'}\n",
      "There are 2 missing columns in the dataframe for day 2020_12_31, the columns are: {'parkingspace_parkingSpacePrice', 'neighborhood'}\n",
      "There are 2 missing columns in the dataframe for day 2020_03_07, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 2 missing columns in the dataframe for day 2020_02_07, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2020_05_17, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_03_05, the columns are: {'neighborhood'}\n",
      "There are 1 missing columns in the dataframe for day 2020_03_02, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_09_20, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_08_07, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_10_27, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2021_02_15, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_07_26, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_05_08, the columns are: {'neighborhood'}\n",
      "There are 2 missing columns in the dataframe for day 2020_08_12, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2020_03_28, the columns are: {'newdevelopmentfinished'}\n",
      "There are 2 missing columns in the dataframe for day 2020_09_15, the columns are: {'newdevelopmentfinished', 'neighborhood'}\n",
      "There are 1 missing columns in the dataframe for day 2020_07_19, the columns are: {'newdevelopmentfinished'}\n",
      "There are 2 missing columns in the dataframe for day 2020_04_15, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2020_07_29, the columns are: {'newdevelopmentfinished'}\n",
      "There are 2 missing columns in the dataframe for day 2021_02_12, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 2 missing columns in the dataframe for day 2020_07_17, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2020_01_30, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_06_06, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_06_04, the columns are: {'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2020_08_26, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_08_01, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_08_06, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_12_11, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2021_01_20, the columns are: {'newdevelopmentfinished'}\n",
      "There are 2 missing columns in the dataframe for day 2020_09_28, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 2 missing columns in the dataframe for day 2020_04_29, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 3 missing columns in the dataframe for day 2021_02_14, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice', 'neighborhood'}\n",
      "There are 2 missing columns in the dataframe for day 2020_06_02, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2020_05_06, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_07_14, the columns are: {'parkingspace_parkingSpacePrice'}\n",
      "There are 2 missing columns in the dataframe for day 2020_11_19, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2020_11_25, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_06_01, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2021_02_19, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_11_15, the columns are: {'newdevelopmentfinished'}\n",
      "There are 2 missing columns in the dataframe for day 2020_04_16, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2021_02_22, the columns are: {'newdevelopmentfinished'}\n",
      "There are 2 missing columns in the dataframe for day 2020_06_13, the columns are: {'parkingspace_parkingSpacePrice', 'neighborhood'}\n",
      "There are 1 missing columns in the dataframe for day 2020_09_04, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_02_11, the columns are: {'newdevelopmentfinished'}\n",
      "There are 4 missing columns in the dataframe for day 2020_01_02, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice', 'parkingspace_isParkingSpaceIncludedInPrice', 'parkingspace_hasParkingSpace'}\n",
      "There are 1 missing columns in the dataframe for day 2020_11_17, the columns are: {'newdevelopmentfinished'}\n",
      "There are 2 missing columns in the dataframe for day 2020_12_03, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 2 missing columns in the dataframe for day 2020_09_06, the columns are: {'newdevelopmentfinished', 'neighborhood'}\n",
      "There are 1 missing columns in the dataframe for day 2020_01_13, the columns are: {'newdevelopmentfinished'}\n",
      "There are 2 missing columns in the dataframe for day 2020_08_09, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2021_01_25, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_12_14, the columns are: {'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2020_05_31, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_11_01, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2021_01_07, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_12_04, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_12_09, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_07_23, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_05_12, the columns are: {'newdevelopmentfinished'}\n",
      "There are 2 missing columns in the dataframe for day 2020_10_25, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 2 missing columns in the dataframe for day 2020_04_25, the columns are: {'parkingspace_parkingSpacePrice', 'neighborhood'}\n",
      "There are 2 missing columns in the dataframe for day 2021_02_25, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2020_02_21, the columns are: {'newdevelopmentfinished'}\n",
      "There are 2 missing columns in the dataframe for day 2020_05_25, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 2 missing columns in the dataframe for day 2020_09_07, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 2 missing columns in the dataframe for day 2020_05_04, the columns are: {'newdevelopmentfinished', 'neighborhood'}\n",
      "There are 1 missing columns in the dataframe for day 2020_11_07, the columns are: {'parkingspace_parkingSpacePrice'}\n",
      "There are 2 missing columns in the dataframe for day 2020_03_03, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2020_01_08, the columns are: {'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2020_07_18, the columns are: {'neighborhood'}\n",
      "There are 2 missing columns in the dataframe for day 2020_06_22, the columns are: {'newdevelopmentfinished', 'detailedtype_subTypology'}\n",
      "There are 1 missing columns in the dataframe for day 2020_07_27, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_08_03, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_01_10, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_01_23, the columns are: {'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2020_10_14, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_10_31, the columns are: {'parkingspace_parkingSpacePrice'}\n",
      "There are 5 missing columns in the dataframe for day 2020_10_15, the columns are: {'parkingspace_parkingSpacePrice', 'newdevelopmentfinished', 'parkingspace_isParkingSpaceIncludedInPrice', 'parkingspace_hasParkingSpace', 'detailedtype_subTypology'}\n",
      "There are 2 missing columns in the dataframe for day 2020_02_26, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2020_07_15, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_12_10, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_08_02, the columns are: {'newdevelopmentfinished'}\n",
      "There are 2 missing columns in the dataframe for day 2020_10_19, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 2 missing columns in the dataframe for day 2020_11_06, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n",
      "There are 1 missing columns in the dataframe for day 2021_01_03, the columns are: {'neighborhood'}\n",
      "There are 1 missing columns in the dataframe for day 2020_06_05, the columns are: {'newdevelopmentfinished'}\n",
      "There are 1 missing columns in the dataframe for day 2020_10_30, the columns are: {'neighborhood'}\n",
      "There are 2 missing columns in the dataframe for day 2020_08_05, the columns are: {'newdevelopmentfinished', 'parkingspace_parkingSpacePrice'}\n"
     ]
    }
   ],
   "source": [
    "nr_of_incomplete_dfs = 0\n",
    "frequently_missing_cols = set()\n",
    "for day in idealista_dfs:\n",
    "    if day == '2020_11_11':\n",
    "        continue\n",
    "    df = idealista_dfs[day]\n",
    "    missing_cols = all_columns - set(df.columns)\n",
    "    frequently_missing_cols = frequently_missing_cols.union(missing_cols)\n",
    "    if len(missing_cols) > 0:\n",
    "        nr_of_incomplete_dfs += 1\n",
    "        print(f\"There are {len(missing_cols)} missing columns in the dataframe for day {day}, the columns are: {missing_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 103 incomplete dataframes'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"There are {nr_of_incomplete_dfs} incomplete dataframes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 6 frequently missing columns'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"There are {len(frequently_missing_cols)} frequently missing columns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detailedtype_subTypology',\n",
       " 'neighborhood',\n",
       " 'newdevelopmentfinished',\n",
       " 'parkingspace_hasParkingSpace',\n",
       " 'parkingspace_isParkingSpaceIncludedInPrice',\n",
       " 'parkingspace_parkingSpacePrice'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequently_missing_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the union of the schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lit\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# in each dataframe, find columns that are missing, and create them with null values\n",
    "for day in idealista_dfs:\n",
    "    df = idealista_dfs[day]\n",
    "    missing_cols = all_columns - set(df.columns)\n",
    "    for col in missing_cols:\n",
    "        df = df.withColumn(col, lit(None))\n",
    "    # order columns in abc\n",
    "    idealista_dfs[day] = df.select(sorted(list(all_columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_of_incomplete_dfs = 0\n",
    "frequently_missing_cols = set()\n",
    "for day in idealista_dfs:\n",
    "    df = idealista_dfs[day]\n",
    "    missing_cols = all_columns - set(df.columns)\n",
    "    frequently_missing_cols = frequently_missing_cols.union(missing_cols)\n",
    "    if len(missing_cols) > 0:\n",
    "        nr_of_incomplete_dfs += 1\n",
    "        print(f\"There are {len(missing_cols)} missing columns in the dataframe for day {day}, the columns are: {missing_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 0 incomplete dataframes'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"There are {nr_of_incomplete_dfs} incomplete dataframes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 0 frequently missing columns'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"There are {len(frequently_missing_cols)} frequently missing columns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-------+------------------------+---------------------+--------+--------+--------+-----------------+-----+------+---------+-------+-------+----------+--------+--------+---------+------------+------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+-----+-----------+------------+------------+--------+-----+-----------+----+------+-----------------------+--------------------+---------+-----------------+---+\n",
      "|address|bathrooms|country|detailedtype_subTypology|detailedtype_typology|distance|district|exterior|externalreference|floor|has360|has3dtour|haslift|hasplan|hasstaging|hasvideo|latitude|longitude|municipality|neighborhood|newdevelopment|newdevelopmentfinished|numphotos|operation|parkingspace_hasParkingSpace|parkingspace_isParkingSpaceIncludedInPrice|parkingspace_parkingSpacePrice|price|pricebyarea|propertycode|propertytype|province|rooms|showaddress|size|status|suggestedtexts_subtitle|suggestedtexts_title|thumbnail|topnewdevelopment|url|\n",
      "+-------+---------+-------+------------------------+---------------------+--------+--------+--------+-----------------+-----+------+---------+-------+-------+----------+--------+--------+---------+------------+------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+-----+-----------+------------+------------+--------+-----+-----------+----+------+-----------------------+--------------------+---------+-----------------+---+\n",
      "+-------+---------+-------+------------------------+---------------------+--------+--------+--------+-----------------+-----+------+---------+-------+-------+----------+--------+--------+---------+------------+------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+-----+-----------+------------+------------+--------+-----+-----------+----+------+-----------------------+--------------------+---------+-----------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idealista_dfs['2020_11_11'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check data type equivalency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_datatype_counter = {}\n",
    "for day in idealista_dfs:\n",
    "    df = idealista_dfs[day]\n",
    "    for column in df.columns:\n",
    "        datatype = df.schema[column].dataType\n",
    "        concat_column_datatype = f\"{column} - {datatype}\"\n",
    "        if concat_column_datatype not in columns_with_datatype_counter:\n",
    "            columns_with_datatype_counter[concat_column_datatype] = 1\n",
    "        else:\n",
    "            columns_with_datatype_counter[concat_column_datatype] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'address - StringType()': 129,\n",
       " 'bathrooms - IntegerType()': 129,\n",
       " 'country - StringType()': 129,\n",
       " 'detailedtype_subTypology - StringType()': 125,\n",
       " 'detailedtype_typology - StringType()': 129,\n",
       " 'distance - StringType()': 129,\n",
       " 'district - StringType()': 129,\n",
       " 'exterior - BooleanType()': 129,\n",
       " 'externalreference - StringType()': 129,\n",
       " 'floor - StringType()': 129,\n",
       " 'has360 - BooleanType()': 129,\n",
       " 'has3dtour - BooleanType()': 129,\n",
       " 'haslift - BooleanType()': 129,\n",
       " 'hasplan - BooleanType()': 129,\n",
       " 'hasstaging - BooleanType()': 129,\n",
       " 'hasvideo - BooleanType()': 129,\n",
       " 'latitude - DoubleType()': 129,\n",
       " 'longitude - DoubleType()': 129,\n",
       " 'municipality - StringType()': 129,\n",
       " 'neighborhood - NullType()': 17,\n",
       " 'newdevelopment - BooleanType()': 129,\n",
       " 'newdevelopmentfinished - BooleanType()': 44,\n",
       " 'numphotos - IntegerType()': 129,\n",
       " 'operation - StringType()': 129,\n",
       " 'parkingspace_hasParkingSpace - BooleanType()': 126,\n",
       " 'parkingspace_isParkingSpaceIncludedInPrice - BooleanType()': 126,\n",
       " 'parkingspace_parkingSpacePrice - DoubleType()': 84,\n",
       " 'price - DoubleType()': 129,\n",
       " 'pricebyarea - DoubleType()': 129,\n",
       " 'propertycode - StringType()': 129,\n",
       " 'propertytype - StringType()': 129,\n",
       " 'province - StringType()': 129,\n",
       " 'rooms - IntegerType()': 129,\n",
       " 'showaddress - BooleanType()': 129,\n",
       " 'size - DoubleType()': 129,\n",
       " 'status - StringType()': 129,\n",
       " 'suggestedtexts_subtitle - StringType()': 129,\n",
       " 'suggestedtexts_title - StringType()': 129,\n",
       " 'thumbnail - StringType()': 129,\n",
       " 'topnewdevelopment - BooleanType()': 129,\n",
       " 'url - StringType()': 129,\n",
       " 'neighborhood - StringType()': 113,\n",
       " 'newdevelopmentfinished - NullType()': 86,\n",
       " 'parkingspace_parkingSpacePrice - NullType()': 46,\n",
       " 'detailedtype_subTypology - NullType()': 5,\n",
       " 'parkingspace_hasParkingSpace - NullType()': 4,\n",
       " 'parkingspace_isParkingSpaceIncludedInPrice - NullType()': 4,\n",
       " 'address - NullType()': 1,\n",
       " 'bathrooms - NullType()': 1,\n",
       " 'country - NullType()': 1,\n",
       " 'detailedtype_typology - NullType()': 1,\n",
       " 'distance - NullType()': 1,\n",
       " 'district - NullType()': 1,\n",
       " 'exterior - NullType()': 1,\n",
       " 'externalreference - NullType()': 1,\n",
       " 'floor - NullType()': 1,\n",
       " 'has360 - NullType()': 1,\n",
       " 'has3dtour - NullType()': 1,\n",
       " 'haslift - NullType()': 1,\n",
       " 'hasplan - NullType()': 1,\n",
       " 'hasstaging - NullType()': 1,\n",
       " 'hasvideo - NullType()': 1,\n",
       " 'latitude - NullType()': 1,\n",
       " 'longitude - NullType()': 1,\n",
       " 'municipality - NullType()': 1,\n",
       " 'newdevelopment - NullType()': 1,\n",
       " 'numphotos - NullType()': 1,\n",
       " 'operation - NullType()': 1,\n",
       " 'price - NullType()': 1,\n",
       " 'pricebyarea - NullType()': 1,\n",
       " 'propertycode - NullType()': 1,\n",
       " 'propertytype - NullType()': 1,\n",
       " 'province - NullType()': 1,\n",
       " 'rooms - NullType()': 1,\n",
       " 'showaddress - NullType()': 1,\n",
       " 'size - NullType()': 1,\n",
       " 'status - NullType()': 1,\n",
       " 'suggestedtexts_subtitle - NullType()': 1,\n",
       " 'suggestedtexts_title - NullType()': 1,\n",
       " 'thumbnail - NullType()': 1,\n",
       " 'topnewdevelopment - NullType()': 1,\n",
       " 'url - NullType()': 1}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_with_datatype_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get entries where value is not 129 and 1\n",
    "columns_needing_fix = {}\n",
    "for col_and_type in columns_with_datatype_counter.keys():\n",
    "    if columns_with_datatype_counter[col_and_type] != 129 and columns_with_datatype_counter[col_and_type] != 1:\n",
    "        columns_needing_fix[col_and_type] = columns_with_datatype_counter[col_and_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'parkingspace_parkingSpacePrice - NullType()': 46,\n",
       " 'parkingspace_parkingSpacePrice - DoubleType()': 84,\n",
       " 'parkingspace_isParkingSpaceIncludedInPrice - NullType()': 4,\n",
       " 'parkingspace_isParkingSpaceIncludedInPrice - BooleanType()': 126,\n",
       " 'parkingspace_hasParkingSpace - NullType()': 4,\n",
       " 'parkingspace_hasParkingSpace - BooleanType()': 126,\n",
       " 'newdevelopmentfinished - NullType()': 86,\n",
       " 'newdevelopmentfinished - BooleanType()': 44,\n",
       " 'neighborhood - StringType()': 113,\n",
       " 'neighborhood - NullType()': 17,\n",
       " 'detailedtype_subTypology - StringType()': 125,\n",
       " 'detailedtype_subTypology - NullType()': 5}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_needing_fix = dict(sorted(columns_needing_fix.items(), key=lambda item: item[0], reverse=True))\n",
    "columns_needing_fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of null values by casting NullType to the column_data_types mapping\n",
    "for day in idealista_dfs:\n",
    "    df = idealista_dfs[day]\n",
    "    for column in df.columns:\n",
    "        # change data type to the correct one\n",
    "        if df.schema[column].dataType == NullType() or column == 'distance':\n",
    "            df = df.withColumn(column, df[column].cast(column_data_types[column]))\n",
    "    idealista_dfs[day] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if it is fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_datatype_counter = {}\n",
    "for day in idealista_dfs:\n",
    "    df = idealista_dfs[day]\n",
    "    for column in df.columns:\n",
    "        datatype = df.schema[column].dataType\n",
    "        concat_column_datatype = f\"{column} - {datatype}\"\n",
    "        if concat_column_datatype not in columns_with_datatype_counter:\n",
    "            columns_with_datatype_counter[concat_column_datatype] = 1\n",
    "        else:\n",
    "            columns_with_datatype_counter[concat_column_datatype] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get entries where value is not 129 and 1\n",
    "columns_needing_fix = {}\n",
    "for col_and_type in columns_with_datatype_counter.keys():\n",
    "    if columns_with_datatype_counter[col_and_type] != 129 and columns_with_datatype_counter[col_and_type] != 1:\n",
    "        columns_needing_fix[col_and_type] = columns_with_datatype_counter[col_and_type]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url - StringType()': 130,\n",
       " 'topnewdevelopment - BooleanType()': 130,\n",
       " 'thumbnail - StringType()': 130,\n",
       " 'suggestedtexts_title - StringType()': 130,\n",
       " 'suggestedtexts_subtitle - StringType()': 130,\n",
       " 'status - StringType()': 130,\n",
       " 'size - DoubleType()': 130,\n",
       " 'showaddress - BooleanType()': 130,\n",
       " 'rooms - IntegerType()': 130,\n",
       " 'province - StringType()': 130,\n",
       " 'propertytype - StringType()': 130,\n",
       " 'propertycode - StringType()': 130,\n",
       " 'pricebyarea - DoubleType()': 130,\n",
       " 'price - DoubleType()': 130,\n",
       " 'parkingspace_parkingSpacePrice - DoubleType()': 130,\n",
       " 'parkingspace_isParkingSpaceIncludedInPrice - BooleanType()': 130,\n",
       " 'parkingspace_hasParkingSpace - BooleanType()': 130,\n",
       " 'operation - StringType()': 130,\n",
       " 'numphotos - IntegerType()': 130,\n",
       " 'newdevelopmentfinished - BooleanType()': 130,\n",
       " 'newdevelopment - BooleanType()': 130,\n",
       " 'neighborhood - StringType()': 130,\n",
       " 'municipality - StringType()': 130,\n",
       " 'longitude - DoubleType()': 130,\n",
       " 'latitude - DoubleType()': 130,\n",
       " 'hasvideo - BooleanType()': 130,\n",
       " 'hasstaging - BooleanType()': 130,\n",
       " 'hasplan - BooleanType()': 130,\n",
       " 'haslift - BooleanType()': 130,\n",
       " 'has3dtour - BooleanType()': 130,\n",
       " 'has360 - BooleanType()': 130,\n",
       " 'floor - StringType()': 130,\n",
       " 'externalreference - StringType()': 130,\n",
       " 'exterior - BooleanType()': 130,\n",
       " 'district - StringType()': 130,\n",
       " 'distance - IntegerType()': 130,\n",
       " 'detailedtype_typology - StringType()': 130,\n",
       " 'detailedtype_subTypology - StringType()': 130,\n",
       " 'date - StringType()': 130,\n",
       " 'country - StringType()': 130,\n",
       " 'bathrooms - IntegerType()': 130,\n",
       " 'address - StringType()': 130}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_needing_fix = dict(sorted(columns_needing_fix.items(), key=lambda item: item[0], reverse=True))\n",
    "columns_needing_fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get entries from the map where the value is not 130\n",
    "columns_needing_fix = {k: v for k, v in columns_with_datatype_counter.items() if v != 130}\n",
    "columns_needing_fix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can merge the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column to each dataframe with the date\n",
    "for day in idealista_dfs:\n",
    "    df = idealista_dfs[day]\n",
    "    df = df.withColumn('date', lit(day))\n",
    "    idealista_dfs[day] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all the dataframes into a single dataframe\n",
    "idealista_df = None\n",
    "for day in idealista_dfs:\n",
    "    if idealista_df is None:\n",
    "        idealista_df = idealista_dfs[day]\n",
    "    else:\n",
    "        idealista_df = idealista_df.union(idealista_dfs[day])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21073"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idealista_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before deduplication: 21073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:29:49 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "24/05/15 18:29:57 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after deduplication: 21065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# deduplicate the dataframe\n",
    "print(f\"Number of rows before deduplication: {idealista_df.count()}\")\n",
    "idealista_df = idealista_df.dropDuplicates()\n",
    "print(f\"Number of rows after deduplication: {idealista_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/uni-projects/bdm2/.venv/lib/python3.10/site-packages/pyspark/sql/functions.py:3796: FutureWarning: Deprecated in 2.1, use approx_count_distinct instead.\n",
      "  warnings.warn(\"Deprecated in 2.1, use approx_count_distinct instead.\", FutureWarning)\n",
      "24/05/15 18:30:01 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "24/05/15 18:30:08 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/15 18:30:09 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/15 18:30:12 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "24/05/15 18:30:20 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/15 18:30:20 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/15 18:30:24 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "24/05/15 18:30:31 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/15 18:30:32 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/15 18:30:36 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "24/05/15 18:30:44 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/15 18:30:44 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/15 18:30:48 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "24/05/15 18:30:56 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/15 18:30:56 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/15 18:31:00 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "24/05/15 18:31:08 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/15 18:31:09 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/15 18:31:13 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "24/05/15 18:31:22 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/15 18:31:22 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/15 18:31:26 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "24/05/15 18:31:36 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/15 18:31:36 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/15 18:31:40 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "24/05/15 18:31:51 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/15 18:31:52 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/15 18:31:56 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "24/05/15 18:32:08 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "[Stage 410:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bathrooms', 'distance', 'latitude', 'longitude', 'numphotos', 'parkingspace_parkingSpacePrice', 'price', 'pricebyarea', 'rooms', 'size']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:32:09 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "continuous_variables = detect_continuous_variables(idealista_df, 10)\n",
    "print(continuous_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:32:13 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "24/05/15 18:32:58 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[<Axes: title={'center': 'bathrooms'}>,\n",
       "        <Axes: title={'center': 'distance'}>,\n",
       "        <Axes: title={'center': 'latitude'}>],\n",
       "       [<Axes: title={'center': 'longitude'}>,\n",
       "        <Axes: title={'center': 'numphotos'}>,\n",
       "        <Axes: title={'center': 'parkingspace_parkingSpacePrice'}>],\n",
       "       [<Axes: title={'center': 'price'}>,\n",
       "        <Axes: title={'center': 'pricebyarea'}>,\n",
       "        <Axes: title={'center': 'rooms'}>],\n",
       "       [<Axes: title={'center': 'size'}>, <Axes: >, <Axes: >]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x2000 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the distribution of the continuous variables\n",
    "idealista_pandas_df = idealista_df.toPandas()\n",
    "idealista_pandas_df[continuous_variables].hist(bins=50, figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- address: string (nullable = true)\n",
      " |-- bathrooms: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- detailedtype_subTypology: string (nullable = true)\n",
      " |-- detailedtype_typology: string (nullable = true)\n",
      " |-- distance: string (nullable = true)\n",
      " |-- district: string (nullable = true)\n",
      " |-- exterior: boolean (nullable = true)\n",
      " |-- externalreference: string (nullable = true)\n",
      " |-- floor: string (nullable = true)\n",
      " |-- has360: boolean (nullable = true)\n",
      " |-- has3dtour: boolean (nullable = true)\n",
      " |-- haslift: boolean (nullable = true)\n",
      " |-- hasplan: boolean (nullable = true)\n",
      " |-- hasstaging: boolean (nullable = true)\n",
      " |-- hasvideo: boolean (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- neighborhood: string (nullable = true)\n",
      " |-- newdevelopment: boolean (nullable = true)\n",
      " |-- newdevelopmentfinished: boolean (nullable = true)\n",
      " |-- numphotos: integer (nullable = true)\n",
      " |-- operation: string (nullable = true)\n",
      " |-- parkingspace_hasParkingSpace: boolean (nullable = true)\n",
      " |-- parkingspace_isParkingSpaceIncludedInPrice: boolean (nullable = true)\n",
      " |-- parkingspace_parkingSpacePrice: double (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- pricebyarea: double (nullable = true)\n",
      " |-- propertycode: string (nullable = true)\n",
      " |-- propertytype: string (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- rooms: integer (nullable = true)\n",
      " |-- showaddress: boolean (nullable = true)\n",
      " |-- size: double (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- suggestedtexts_subtitle: string (nullable = true)\n",
      " |-- suggestedtexts_title: string (nullable = true)\n",
      " |-- thumbnail: string (nullable = true)\n",
      " |-- topnewdevelopment: boolean (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- date: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idealista_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:24:39 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "24/05/15 18:24:46 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+------------------------+---------------------+--------+--------------------+--------+------------------+-----+------+---------+-------+-------+----------+--------+----------+---------+--------------------+-------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+--------+-----------+------------+------------+---------+-----+-----------+-----+--------------+-----------------------+--------------------+--------------------+-----------------+--------------------+----------+\n",
      "|             address|bathrooms|country|detailedtype_subTypology|detailedtype_typology|distance|            district|exterior| externalreference|floor|has360|has3dtour|haslift|hasplan|hasstaging|hasvideo|  latitude|longitude|        municipality| neighborhood|newdevelopment|newdevelopmentfinished|numphotos|operation|parkingspace_hasParkingSpace|parkingspace_isParkingSpaceIncludedInPrice|parkingspace_parkingSpacePrice|   price|pricebyarea|propertycode|propertytype| province|rooms|showaddress| size|        status|suggestedtexts_subtitle|suggestedtexts_title|           thumbnail|topnewdevelopment|                 url|      date|\n",
      "+--------------------+---------+-------+------------------------+---------------------+--------+--------------------+--------+------------------+-----+------+---------+-------+-------+----------+--------+----------+---------+--------------------+-------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+--------+-----------+------------+------------+---------+-----+-----------+-----+--------------+-----------------------+--------------------+--------------------+-----------------+--------------------+----------+\n",
      "|Calle Santa Eulal...|        3|     es|                  duplex|                 flat|      94|       Santa Eulàlia|    true|     1º Dx tipo 5 |    1| false|    false|   true|  false|     false|   false|41.3665077|2.1236566|Hospitalet de Llo...|Santa Eulàlia|          true|                 false|        6|     sale|                        true|                                      true|                           NaN|472900.0|     3402.0|    88662745|      duplex|Barcelona|    3|       true|139.0|newdevelopment|   Santa Eulàlia, Ho...|Dúplex en Calle S...|https://img3.idea...|            false|https://www.ideal...|2021_01_13|\n",
      "|Calle Santa Eulal...|        3|     es|                  duplex|                 flat|     248|       Santa Eulàlia|    true|     1º Dx tipo 5 |    1| false|    false|   true|  false|     false|   false|41.3665077|2.1236566|Hospitalet de Llo...|Santa Eulàlia|          true|                 false|        6|     sale|                        true|                                      true|                           NaN|472900.0|     3402.0|    88662745|      duplex|Barcelona|    3|       true|139.0|newdevelopment|   Santa Eulàlia, Ho...|Dúplex en Calle S...|https://img3.idea...|            false|https://www.ideal...|2020_04_06|\n",
      "|Calle Santa Eulal...|        3|     es|                  duplex|                 flat|    1281|       Santa Eulàlia|    true|     1º Dx tipo 5 |    1| false|    false|   true|  false|     false|   false|41.3665077|2.1236566|Hospitalet de Llo...|Santa Eulàlia|          true|                 false|        6|     sale|                        true|                                      true|                           NaN|490000.0|     3525.0|    88662745|      duplex|Barcelona|    3|       true|139.0|newdevelopment|   Santa Eulàlia, Ho...|Dúplex en Calle S...|https://img3.idea...|            false|https://www.ideal...|2020_12_07|\n",
      "|Calle Santa Eulal...|        3|     es|                  duplex|                 flat|     428|       Santa Eulàlia|    true|     1º Dx tipo 5 |    1| false|    false|   true|  false|     false|   false|41.3665077|2.1236566|Hospitalet de Llo...|Santa Eulàlia|          true|                 false|        6|     sale|                        true|                                      true|                           NaN|472900.0|     3402.0|    88662745|      duplex|Barcelona|    3|       true|139.0|newdevelopment|   Santa Eulàlia, Ho...|Dúplex en Calle S...|https://img3.idea...|            false|https://www.ideal...|2020_09_14|\n",
      "|  Calle Jaume Balmes|        3|     es|                  duplex|                 flat|    2308|Plaça Catalunya -...|    true|  DUPLEXBALMESTBOI|    3| false|    false|   true|  false|     false|    true|41.3361428|2.0416883|Sant Boi de Llobr...|         NULL|         false|                  NULL|       55|     sale|                        true|                                      true|                           NaN|395000.0|     1881.0|    90999360|      duplex|Barcelona|    3|      false|210.0|          good|   Plaça Catalunya -...|Dúplex en Calle J...|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|Camps Blancs - Ca...|        1|     es|                    NULL|                 flat|    2178|Camps Blancs - Ca...|    true|            246665|    5| false|    false|   true|  false|     false|   false|41.3332408| 2.037826|Sant Boi de Llobr...|         NULL|         false|                  NULL|        6|     sale|                        NULL|                                      NULL|                           NaN|139000.0|     2356.0|    92766162|        flat|Barcelona|    3|      false| 59.0|          good|   Camps Blancs - Ca...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|Barri Antic - Centre|        1|     es|                    NULL|                 flat|    3197|Barri Antic - Centre|    true|             REF35|    2| false|    false|   true|  false|     false|   false|41.3145748|2.0143527|          Viladecans|         NULL|         false|                  NULL|       13|     sale|                        NULL|                                      NULL|                           NaN|150000.0|     1875.0|    92214081|        flat|Barcelona|    4|      false| 80.0|         renew|   Barri Antic - Cen...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|Barri Antic - Centre|        2|     es|                    NULL|                 flat|    3017|Barri Antic - Centre|   false|VI-000-030-658-216| NULL| false|    false|   NULL|   true|     false|   false|41.3193528|2.0164947|          Viladecans|         NULL|         false|                  NULL|       51|     sale|                        NULL|                                      NULL|                           NaN|264500.0|     2242.0|    92529567|        flat|Barcelona|    4|      false|118.0|          good|   Barri Antic - Cen...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|Camps Blancs - Ca...|        1|     es|               penthouse|                 flat|    2717|Camps Blancs - Ca...|    true|              3176|    4| false|    false|  false|  false|     false|   false|41.3362817|2.0324349|Sant Boi de Llobr...|         NULL|         false|                  NULL|        9|     sale|                        NULL|                                      NULL|                           NaN|146000.0|     1802.0|    87618492|   penthouse|Barcelona|    3|      false| 81.0|          good|   Camps Blancs - Ca...|               Ático|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|    Parc Empresarial|        1|     es|                    NULL|                 flat|    2210|    Parc Empresarial|   false|             01782|    1| false|    false|  false|  false|     false|   false|41.3207551|2.0264963|          Viladecans|         NULL|         false|                  NULL|       20|     sale|                        NULL|                                      NULL|                           NaN|126000.0|     1938.0|    91975057|        flat|Barcelona|    3|      false| 65.0|         renew|   Parc Empresarial,...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|Plaça Catalunya -...|        2|     es|                    NULL|               chalet|    2875|Plaça Catalunya -...|   false|            SBS029|   bj| false|    false|   NULL|  false|     false|   false|41.3421262|2.0441325|Sant Boi de Llobr...|         NULL|         false|                  NULL|       22|     sale|                        true|                                      true|                           NaN|373000.0|     2144.0|    86175413|      chalet|Barcelona|    4|      false|174.0|          good|   Plaça Catalunya -...|              Chalet|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|Camino Oliveretes...|        2|     es|                    NULL|                 flat|    2621|Campreciós - Torr...|    true|               A13| NULL| false|    false|   true|   true|     false|   false|41.3233639|2.0222618|          Viladecans|         NULL|          true|                 false|       17|     sale|                        NULL|                                      NULL|                           NaN|267000.0|     2618.0|    92794636|        flat|Barcelona|    3|       true|102.0|newdevelopment|   Campreciós - Torr...|Piso en Camino Ol...|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|Plaça Catalunya -...|        1|     es|                    NULL|                 flat|    2352|Plaça Catalunya -...|    true|              3335|    2| false|    false|  false|  false|     false|   false|41.3371495|2.0437694|Sant Boi de Llobr...|         NULL|         false|                  NULL|        0|     sale|                        NULL|                                      NULL|                           NaN|114000.0|     1541.0|    91274659|        flat|Barcelona|    3|      false| 74.0|         renew|   Plaça Catalunya -...|                Piso|                NULL|            false|https://www.ideal...|2020_10_16|\n",
      "|Plaça Catalunya -...|        2|     es|                    NULL|                 flat|    2306|Plaça Catalunya -...|    true|             A3130|    1| false|    false|   true|  false|     false|   false|41.3362403|2.0420631|Sant Boi de Llobr...|         NULL|         false|                  NULL|       22|     sale|                        NULL|                                      NULL|                           NaN|233900.0|     2599.0|    92435636|        flat|Barcelona|    3|      false| 90.0|          good|   Plaça Catalunya -...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|Plaça Catalunya -...|        2|     es|                    NULL|                 flat|    2014|Plaça Catalunya -...|   false|             05SBC|    1| false|    false|   true|  false|     false|   false|41.3338537|2.0435273|Sant Boi de Llobr...|         NULL|         false|                  NULL|       19|     sale|                        NULL|                                      NULL|                           NaN|255000.0|     2107.0|    92789619|        flat|Barcelona|    4|      false|121.0|          good|   Plaça Catalunya -...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|  Plaça de Catalunya|        2|     es|                    NULL|                 flat|    2972|  Plaça de Catalunya|    true|              5688|    2| false|    false|   true|  false|     false|   false|  41.32027|  2.08783|El Prat de Llobregat|         NULL|         false|                  NULL|       19|     sale|                        true|                                      true|                           NaN|299500.0|     2415.0|    91712430|        flat|Barcelona|    3|      false|124.0|          good|   Plaça de Cataluny...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|Mossen Jacint Ver...|        1|     es|                    NULL|                 flat|    2479|Plaça Catalunya -...|    true|             80515|    3| false|    false|   true|  false|     false|   false|  41.33803|  2.04248|Sant Boi de Llobr...|         NULL|         false|                  NULL|       10|     sale|                        NULL|                                      NULL|                           NaN|175500.0|     2250.0|    91328946|        flat|Barcelona|    3|      false| 78.0|          good|   Plaça Catalunya -...|Piso en Mossen Ja...|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|Calle Tres de Abr...|        2|     es|                    NULL|                 flat|    2769|Plaça Catalunya -...|   false|           GV36608|    4| false|    false|   NULL|   true|     false|   false|   41.3411|  2.04395|Sant Boi de Llobr...|         NULL|         false|                  NULL|        8|     sale|                        NULL|                                      NULL|                           NaN|330000.0|     3750.0|    92112622|        flat|Barcelona|    3|       true| 88.0|          good|   Plaça Catalunya -...|Piso en Calle Tre...|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|Plaça Catalunya -...|        2|     es|                    NULL|                 flat|    2911|Plaça Catalunya -...|    true|         G2_780437|    2| false|    false|   true|  false|     false|   false|41.3426378|2.0451746|Sant Boi de Llobr...|         NULL|         false|                  NULL|       31|     sale|                        NULL|                                      NULL|                           NaN|299000.0|     2670.0|    92551508|        flat|Barcelona|    3|      false|112.0|          good|   Plaça Catalunya -...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|Plaça Catalunya -...|        2|     es|                    NULL|                 flat|    2195|Plaça Catalunya -...|    true|            003266|    2| false|    false|   true|  false|     false|   false|41.3356331| 2.043653|Sant Boi de Llobr...|         NULL|         false|                  NULL|       22|     sale|                        NULL|                                      NULL|                           NaN|276000.0|     2262.0|    86454496|        flat|Barcelona|    4|      false|122.0|          good|   Plaça Catalunya -...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "+--------------------+---------+-------+------------------------+---------------------+--------+--------------------+--------+------------------+-----+------+---------+-------+-------+----------+--------+----------+---------+--------------------+-------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+--------+-----------+------------+------------+---------+-----+-----------+-----+--------------+-----------------------+--------------------+--------------------+-----------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "idealista_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:24:50 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "[Stage 338:===================================================> (126 + 1) / 129]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-------+------------------------+---------------------+--------+--------+--------+-----------------+-----+------+---------+-------+-------+----------+--------+--------+---------+------------+------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+-----+-----------+------------+------------+--------+-----+-----------+----+------+-----------------------+--------------------+---------+-----------------+---+----+\n",
      "|address|bathrooms|country|detailedtype_subTypology|detailedtype_typology|distance|district|exterior|externalreference|floor|has360|has3dtour|haslift|hasplan|hasstaging|hasvideo|latitude|longitude|municipality|neighborhood|newdevelopment|newdevelopmentfinished|numphotos|operation|parkingspace_hasParkingSpace|parkingspace_isParkingSpaceIncludedInPrice|parkingspace_parkingSpacePrice|price|pricebyarea|propertycode|propertytype|province|rooms|showaddress|size|status|suggestedtexts_subtitle|suggestedtexts_title|thumbnail|topnewdevelopment|url|date|\n",
      "+-------+---------+-------+------------------------+---------------------+--------+--------+--------+-----------------+-----+------+---------+-------+-------+----------+--------+--------+---------+------------+------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+-----+-----------+------------+------------+--------+-----+-----------+----+------+-----------------------+--------------------+---------+-----------------+---+----+\n",
      "+-------+---------+-------+------------------------+---------------------+--------+--------+--------+-----------------+-----+------+---------+-------+-------+----------+--------+--------+---------+------------+------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+-----+-----------+------------+------------+--------+-----+-----------+----+------+-----------------------+--------------------+---------+-----------------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from the distance column, print all values that are not integer like\n",
    "idealista_df.filter(idealista_df['distance'].rlike('[^0-9]')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:43:16 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "24/05/15 18:43:23 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: bathrooms\n",
      "Q1: 1.0\n",
      "Q3: 2.0\n",
      "IQR: 1.0\n",
      "Lower Bound: -2.0\n",
      "Upper Bound: 5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:43:28 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "24/05/15 18:43:34 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 332\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:43:40 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "24/05/15 18:43:46 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+------------------------+---------------------+--------+-------------------+--------+-----------------+-----+------+---------+-------+-------+----------+--------+----------+---------+--------------------+--------------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+---------+-----------+------------+------------+---------+-----+-----------+------+------+-----------------------+--------------------+--------------------+-----------------+--------------------+----------+\n",
      "|             address|bathrooms|country|detailedtype_subTypology|detailedtype_typology|distance|           district|exterior|externalreference|floor|has360|has3dtour|haslift|hasplan|hasstaging|hasvideo|  latitude|longitude|        municipality|        neighborhood|newdevelopment|newdevelopmentfinished|numphotos|operation|parkingspace_hasParkingSpace|parkingspace_isParkingSpaceIncludedInPrice|parkingspace_parkingSpacePrice|    price|pricebyarea|propertycode|propertytype| province|rooms|showaddress|  size|status|suggestedtexts_subtitle|suggestedtexts_title|           thumbnail|topnewdevelopment|                 url|      date|\n",
      "+--------------------+---------+-------+------------------------+---------------------+--------+-------------------+--------+-----------------+-----+------+---------+-------+-------+----------+--------+----------+---------+--------------------+--------------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+---------+-----------+------------+------------+---------+-----+-----------+------+------+-----------------------+--------------------+--------------------+-----------------+--------------------+----------+\n",
      "|    barrio Pedralbes|        6|     es|               penthouse|                 flat|     660|          Les Corts|    true|         BCN26015| NULL| false|    false|   true|   true|     false|   false| 41.394721| 2.118681|           Barcelona|           Pedralbes|         false|                  NULL|       59|     sale|                        true|                                      true|                          NULL|3500000.0|     8621.0|    92176342|   penthouse|Barcelona|    5|      false| 406.0|  good|   Pedralbes, Barcelona|               Ático|https://img3.idea...|            false|https://www.ideal...|2020_02_10|\n",
      "|Calle dels Cavallers|        6|     es|                  duplex|                 flat|     855|          Les Corts|    true|          D-40637|    5| false|    false|   true|  false|     false|    true| 41.393521| 2.116681|           Barcelona|           Pedralbes|         false|                  NULL|       54|     sale|                        true|                                      true|                          NULL|3500000.0|     7056.0|    91211684|      duplex|Barcelona|    6|      false| 496.0|  good|   Pedralbes, Barcelona|Dúplex en Calle d...|https://img3.idea...|            false|https://www.ideal...|2020_02_10|\n",
      "|       barrio Sarrià|        6|     es|                    NULL|               chalet|     619|Sarrià-Sant Gervasi|   false|         BCN14958| NULL| false|    false|   NULL|  false|     false|   false|41.4005037|2.1224093|           Barcelona|              Sarrià|         false|                  NULL|       35|     sale|                        true|                                      true|                          NULL|3900000.0|     7617.0|    88376448|      chalet|Barcelona|    6|      false| 512.0| renew|      Sarrià, Barcelona|              Chalet|https://img3.idea...|            false|https://www.ideal...|2020_02_10|\n",
      "|    barrio Pedralbes|        6|     es|               penthouse|                 flat|     798|          Les Corts|    true|         BCN26015| NULL| false|    false|   true|   true|     false|   false| 41.394721| 2.118681|           Barcelona|           Pedralbes|         false|                  NULL|       59|     sale|                        true|                                      true|                          NULL|3500000.0|     8621.0|    92176342|   penthouse|Barcelona|    5|      false| 406.0|  good|   Pedralbes, Barcelona|               Ático|https://img3.idea...|            false|https://www.ideal...|2020_08_15|\n",
      "|    barrio Pedralbes|        6|     es|                  duplex|                 flat|     783|          Les Corts|    true|             1204|    5| false|    false|   true|   true|     false|   false|41.3923064| 2.117372|           Barcelona|           Pedralbes|         false|                  NULL|        9|     sale|                        NULL|                                      NULL|                          NULL|2500000.0|     5593.0|    91797141|      duplex|Barcelona|    8|      false| 447.0| renew|   Pedralbes, Barcelona|              Dúplex|https://img3.idea...|            false|https://www.ideal...|2020_08_15|\n",
      "|      Calle de Villà|        6|     es|                    NULL|               chalet|     837|               Golf|   false|     VSC2010007-1| NULL| false|    false|   NULL|   true|     false|    true|41.4623287|2.0730443|Sant Cugat del Va...|                NULL|         false|                  NULL|       39|     sale|                        true|                                      true|                           NaN|1425000.0|     2321.0|    91700694|      chalet|Barcelona|    6|      false| 614.0|  good|   Golf, Sant Cugat ...|Chalet en Calle d...|https://img3.idea...|            false|https://www.ideal...|2020_08_29|\n",
      "|                Golf|        6|     es|                    NULL|               chalet|     662|               Golf|   false|       VSC2101002| NULL| false|    false|   NULL|   true|     false|    true|41.4597204|2.0854806|Sant Cugat del Va...|                NULL|         false|                  NULL|       50|     sale|                        true|                                      true|                           NaN|2565000.0|     4750.0|    92606986|      chalet|Barcelona|    7|      false| 540.0|  good|   Golf, Sant Cugat ...|              Chalet|https://img3.idea...|            false|https://www.ideal...|2020_08_29|\n",
      "|              Arxius|        6|     es|                    NULL|               chalet|    1641|             Arxius|   false|            10329| NULL| false|     true|   NULL|   true|     false|    true|41.4680996| 2.067239|Sant Cugat del Va...|                NULL|         false|                  NULL|       66|     sale|                        true|                                      true|                           NaN|2800000.0|     4921.0|    92442950|      chalet|Barcelona|    6|      false| 569.0|  good|   Arxius, Sant Cuga...|              Chalet|https://img3.idea...|            false|https://www.ideal...|2020_08_29|\n",
      "|           Can Matas|        7|     es|                    NULL|               chalet|    2153|          Can Matas|   false|     VSC2003002-5| NULL| false|    false|   NULL|   true|     false|    true|41.4718127|2.0635986|Sant Cugat del Va...|                NULL|         false|                  NULL|       32|     sale|                        true|                                      true|                           NaN|1990000.0|     2535.0|    91784793|      chalet|Barcelona|    9|      false| 785.0|  good|   Can Matas, Sant C...|              Chalet|https://img3.idea...|            false|https://www.ideal...|2020_08_29|\n",
      "|Calle de l'Oceà G...|        6|     es|                    NULL|               chalet|     320|               Golf|   false|     VSC2006005-4| NULL| false|    false|   NULL|   true|     false|    true|41.4559742|2.0751854|Sant Cugat del Va...|                NULL|         false|                  NULL|       52|     sale|                        true|                                      true|                           NaN|2200000.0|     2095.0|    90461192|      chalet|Barcelona|    6|      false|1050.0|  good|   Golf, Sant Cugat ...|Chalet en Calle d...|https://img3.idea...|            false|https://www.ideal...|2020_08_29|\n",
      "|Calle de Ferran S...|        7|     es|                    NULL|               chalet|    1385|             Arxius|   false|       VSC2012006| NULL| false|    false|   NULL|   true|     false|    true|41.4664996| 2.069539|Sant Cugat del Va...|                NULL|         false|                  NULL|       41|     sale|                        true|                                      true|                           NaN|2800000.0|     4921.0|    92248016|      chalet|Barcelona|    5|      false| 569.0|  good|   Arxius, Sant Cuga...|Chalet en Calle d...|https://img3.idea...|            false|https://www.ideal...|2020_08_29|\n",
      "|Avenida del Tibidabo|        8|     es|                    NULL|               chalet|     800|Sarrià-Sant Gervasi|   false|           BCN648|   bj| false|    false|   NULL|   true|     false|    true|41.4137088|2.1320765|           Barcelona|Sant Gervasi - La...|         false|                  NULL|       48|     sale|                        true|                                      true|                          NULL|5600000.0|     7245.0|    89131054|      chalet|Barcelona|   11|      false| 773.0|  good|   Sant Gervasi - La...|Chalet en Avenida...|https://img3.idea...|            false|https://www.ideal...|2020_05_29|\n",
      "|barrio La Nova Es...|        6|     es|               penthouse|                 flat|     214|           Eixample|    true|         BCN21193| NULL|  true|    false|   true|   true|     false|   false|41.3843004|2.1499029|           Barcelona|La Nova Esquerra ...|         false|                  NULL|       42|     sale|                        true|                                      true|                          NULL|1890000.0|     5625.0|    87601397|   penthouse|Barcelona|    6|      false| 336.0|  good|   La Nova Esquerra ...|               Ático|https://img3.idea...|            false|https://www.ideal...|2020_08_21|\n",
      "|       barrio Sarrià|        6|     es|                    NULL|               chalet|     297|Sarrià-Sant Gervasi|   false|      VB1810042-1| NULL| false|    false|   NULL|   true|     false|    true|41.4023037|2.1226093|           Barcelona|              Sarrià|         false|                  NULL|       42|     sale|                        true|                                      true|                           NaN|3900000.0|     7156.0|    89228498|      chalet|Barcelona|    9|      false| 545.0|  good|      Sarrià, Barcelona|              Chalet|https://img3.idea...|            false|https://www.ideal...|2020_12_12|\n",
      "|       barrio Sarrià|        7|     es|        independantHouse|               chalet|     658|Sarrià-Sant Gervasi|   false|         W-02I9O9| NULL| false|    false|   NULL|   true|     false|   false|41.4064439|2.1159086|           Barcelona|              Sarrià|         false|                  NULL|       19|     sale|                        true|                                      true|                           NaN|5250000.0|     8373.0|    91820359|      chalet|Barcelona|    7|      false| 627.0|  good|      Sarrià, Barcelona|  Casa independiente|https://img3.idea...|            false|https://www.ideal...|2020_12_12|\n",
      "|       barrio Sarrià|        6|     es|                    NULL|               chalet|     260|Sarrià-Sant Gervasi|   false|         BCN14958| NULL| false|    false|   NULL|  false|     false|   false|41.4005037|2.1224093|           Barcelona|              Sarrià|         false|                  NULL|       35|     sale|                        true|                                      true|                           NaN|3900000.0|     7617.0|    88376448|      chalet|Barcelona|    6|      false| 512.0| renew|      Sarrià, Barcelona|              Chalet|https://img3.idea...|            false|https://www.ideal...|2020_12_12|\n",
      "|       barrio Sarrià|        7|     es|                    NULL|               chalet|     745|Sarrià-Sant Gervasi|   false|             1459| NULL| false|    false|   NULL|  false|     false|    true| 41.407502|2.1166121|           Barcelona|              Sarrià|         false|                  NULL|        7|     sale|                        true|                                      true|                           NaN|5250000.0|     8373.0|    92201110|      chalet|Barcelona|    7|      false| 627.0|  good|      Sarrià, Barcelona|              Chalet|https://img3.idea...|            false|https://www.ideal...|2020_12_12|\n",
      "|    barrio Pedralbes|        6|     es|               penthouse|                 flat|     754|          Les Corts|    true|           AVP-66|    6| false|    false|   true|  false|     false|    true|41.3963077|2.1130438|           Barcelona|           Pedralbes|         false|                  NULL|       27|     sale|                        true|                                      true|                           NaN|2900000.0|     5235.0|    92568316|   penthouse|Barcelona|    5|      false| 554.0|  good|   Pedralbes, Barcelona|               Ático|https://img3.idea...|            false|https://www.ideal...|2020_12_12|\n",
      "|    barrio Pedralbes|        6|     es|               penthouse|                 flat|     715|          Les Corts|    true|         BCN26015| NULL| false|    false|   true|   true|     false|   false| 41.394721| 2.118681|           Barcelona|           Pedralbes|         false|                  NULL|       59|     sale|                        true|                                      true|                           NaN|3500000.0|     8621.0|    92176342|   penthouse|Barcelona|    5|      false| 406.0|  good|   Pedralbes, Barcelona|               Ático|https://img3.idea...|            false|https://www.ideal...|2020_12_12|\n",
      "|barrio Sant Gerva...|        8|     es|                    NULL|               chalet|     579|Sarrià-Sant Gervasi|   false|          BCN2468| NULL| false|    false|   NULL|  false|     false|   false|41.4061366|2.1213645|           Barcelona|Sant Gervasi - La...|         false|                  NULL|       32|     sale|                        true|                                      true|                           NaN|2600000.0|     5328.0|    33743918|      chalet|Barcelona|    7|      false| 488.0|  good|   Sant Gervasi - La...|              Chalet|https://img3.idea...|            false|https://www.ideal...|2020_12_12|\n",
      "+--------------------+---------+-------+------------------------+---------------------+--------+-------------------+--------+-----------------+-----+------+---------+-------+-------+----------+--------+----------+---------+--------------------+--------------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+---------+-----------+------------+------------+---------+-----+-----------+------+------+-----------------------+--------------------+--------------------+-----------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:43:50 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "24/05/15 18:43:58 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: distance\n",
      "Q1: 504.0\n",
      "Q3: 2621.0\n",
      "IQR: 2117.0\n",
      "Lower Bound: -5847.0\n",
      "Upper Bound: 8972.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:44:02 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:44:14 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "24/05/15 18:44:21 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: latitude\n",
      "Q1: 41.3619074\n",
      "Q3: 41.3857747\n",
      "IQR: 0.023867299999999148\n",
      "Lower Bound: 41.2903055\n",
      "Upper Bound: 41.457376599999996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:44:25 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "24/05/15 18:44:32 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 979\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:44:37 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "24/05/15 18:44:44 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+------------------------+---------------------+--------+----------------+--------+-----------------+-----+------+---------+-------+-------+----------+--------+----------+---------+--------------------+------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+---------+-----------+------------+------------+---------+-----+-----------+------+------+-----------------------+--------------------+--------------------+-----------------+--------------------+----------+\n",
      "|             address|bathrooms|country|detailedtype_subTypology|detailedtype_typology|distance|        district|exterior|externalreference|floor|has360|has3dtour|haslift|hasplan|hasstaging|hasvideo|  latitude|longitude|        municipality|neighborhood|newdevelopment|newdevelopmentfinished|numphotos|operation|parkingspace_hasParkingSpace|parkingspace_isParkingSpaceIncludedInPrice|parkingspace_parkingSpacePrice|    price|pricebyarea|propertycode|propertytype| province|rooms|showaddress|  size|status|suggestedtexts_subtitle|suggestedtexts_title|           thumbnail|topnewdevelopment|                 url|      date|\n",
      "+--------------------+---------+-------+------------------------+---------------------+--------+----------------+--------+-----------------+-----+------+---------+-------+-------+----------+--------+----------+---------+--------------------+------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+---------+-----------+------------+------------+---------+-----+-----------+------+------+-----------------------+--------------------+--------------------+-----------------+--------------------+----------+\n",
      "|   josep tarradellas|        2|     es|                    NULL|                 flat|    2647|       Can Matas|    true|            V7271|    2| false|     true|   true|   true|     false|    true|  41.47064|2.0536886|Sant Cugat del Va...|        NULL|         false|                  NULL|       46|     sale|                        true|                                      true|                           NaN| 395000.0|     4877.0|    91671800|        flat|Barcelona|    2|      false|  81.0|  good|   Can Matas, Sant C...|Piso en josep tar...|https://img3.idea...|            false|https://www.ideal...|2020_08_29|\n",
      "|              Arxius|        2|     es|               penthouse|                 flat|    1694|          Arxius|    true|            10573|    3| false|     true|   true|   true|     false|   false|41.4663651|2.0637786|Sant Cugat del Va...|        NULL|         false|                  NULL|       50|     sale|                        true|                                      true|                           NaN| 595000.0|     4577.0|    93071499|   penthouse|Barcelona|    4|      false| 130.0|  good|   Arxius, Sant Cuga...|               Ático|https://img3.idea...|            false|https://www.ideal...|2020_08_29|\n",
      "|      Calle d'Estapé|        4|     es|                    NULL|               chalet|    2490|    Sant Domènec|   false|     VSC2002001-5| NULL| false|    false|   NULL|   true|     false|    true|41.4785622|2.0762325|Sant Cugat del Va...|        NULL|         false|                  NULL|       32|     sale|                        true|                                      true|                           NaN|1175000.0|      894.0|    91784755|      chalet|Barcelona|    5|      false|1314.0|  good|   Sant Domènec, San...|Chalet en Calle d...|https://img3.idea...|            false|https://www.ideal...|2020_08_29|\n",
      "|     Cristòfor Colom|        2|     es|               penthouse|                 flat|    1847|Centre - Estació|    true|             RT23|    2| false|    false|   true|   true|     false|   false|41.4728741|2.0787685|Sant Cugat del Va...|        NULL|         false|                  NULL|       28|     sale|                        true|                                     false|                       16000.0| 530000.0|     3193.0|    91451364|   penthouse|Barcelona|    3|      false| 166.0|  good|   Centre - Estació,...|Ático en Cristòfo...|https://img3.idea...|            false|https://www.ideal...|2020_08_29|\n",
      "|        Sant Domènec|        2|     es|                    NULL|                 flat|    2408|    Sant Domènec|    true|            v7344|    1| false|     true|   true|   true|     false|    true|41.4778391|2.0764996|Sant Cugat del Va...|        NULL|         false|                  NULL|       54|     sale|                        NULL|                                      NULL|                           NaN| 545000.0|     4225.0|    93064705|        flat|Barcelona|    4|      false| 129.0|  good|   Sant Domènec, San...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_08_29|\n",
      "|              Arxius|        2|     es|                    NULL|                 flat|    1766|          Arxius|    true|             8839| NULL| false|     true|   true|   true|     false|    true|41.4689651|2.0662786|Sant Cugat del Va...|        NULL|         false|                  NULL|       48|     sale|                        true|                                      true|                           NaN| 535000.0|     4084.0|    92713087|        flat|Barcelona|    3|      false| 131.0|  good|   Arxius, Sant Cuga...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_08_29|\n",
      "|                Golf|        4|     es|                    NULL|               chalet|     894|            Golf|   false|     VSC2009014-2| NULL| false|    false|   NULL|   true|     false|    true|41.4642858|2.0782842|Sant Cugat del Va...|        NULL|         false|                  NULL|       44|     sale|                        true|                                      true|                           NaN|1285000.0|     3127.0|    91876867|      chalet|Barcelona|    5|      false| 411.0|  good|   Golf, Sant Cugat ...|              Chalet|https://img3.idea...|            false|https://www.ideal...|2020_08_29|\n",
      "|            Eixample|        5|     es|       semidetachedHouse|               chalet|    1479|        Eixample|   false|         W-02J79L|   bj|  true|    false|   NULL|   true|     false|   false|41.4689446|2.0843837|Sant Cugat del Va...|        NULL|         false|                  NULL|       61|     sale|                        true|                                      true|                           NaN|1180000.0|     3933.0|    90420675|      chalet|Barcelona|    5|      false| 300.0|  good|   Eixample, Sant Cu...|      Chalet pareado|https://img3.idea...|            false|https://www.ideal...|2020_08_29|\n",
      "|             Mirasol|        3|     es|                    NULL|               chalet|    3017|         Mirasol|   false|            10436| NULL| false|     true|   NULL|   true|     false|    true|41.4718819|2.0493932|Sant Cugat del Va...|        NULL|         false|                  NULL|       51|     sale|                        true|                                      true|                           NaN| 660000.0|     3837.0|    92486421|      chalet|Barcelona|    4|      false| 172.0|  good|   Mirasol, Sant Cug...|              Chalet|https://img3.idea...|            false|https://www.ideal...|2020_08_29|\n",
      "|            Calle Or|        4|     es|        independantHouse|               chalet|    2197|       Can Matas|   false|          IM-2921| NULL| false|    false|   NULL|   true|     false|   false|  41.47122|  2.06178|Sant Cugat del Va...|        NULL|         false|                  NULL|       14|     sale|                        NULL|                                      NULL|                           NaN|1750000.0|     5208.0|    93163389|      chalet|Barcelona|    5|      false| 336.0|  good|   Can Matas, Sant C...|Casa independient...|https://img3.idea...|            false|https://www.ideal...|2020_08_29|\n",
      "|          PAU CASALS|        3|     es|                    NULL|                 flat|    2153|     Torreblanca|    true|             NULL|    3|  true|    false|   true|   true|     false|    true|41.4718769|2.0943056|Sant Cugat del Va...|        NULL|         false|                  NULL|       42|     sale|                        true|                                      true|                           NaN| 650000.0|     4221.0|    92877956|        flat|Barcelona|    4|      false| 154.0|  good|   Torreblanca, Sant...|  Piso en PAU CASALS|https://img3.idea...|            false|https://www.ideal...|2020_08_29|\n",
      "|      Calle de Villà|        6|     es|                    NULL|               chalet|     837|            Golf|   false|     VSC2010007-1| NULL| false|    false|   NULL|   true|     false|    true|41.4623287|2.0730443|Sant Cugat del Va...|        NULL|         false|                  NULL|       39|     sale|                        true|                                      true|                           NaN|1425000.0|     2321.0|    91700694|      chalet|Barcelona|    6|      false| 614.0|  good|   Golf, Sant Cugat ...|Chalet en Calle d...|https://img3.idea...|            false|https://www.ideal...|2020_08_29|\n",
      "|   Avenida de Madrid|        3|     es|                    NULL|               chalet|    2868|         Mirasol|   false|       VSC2102003| NULL| false|    false|   NULL|  false|     false|    true|41.4664928|2.0474107|Sant Cugat del Va...|        NULL|         false|                  NULL|       38|     sale|                        NULL|                                      NULL|                           NaN| 725000.0|     4119.0|    92742965|      chalet|Barcelona|    5|      false| 176.0|  good|   Mirasol, Sant Cug...|Chalet en Avenida...|https://img3.idea...|            false|https://www.ideal...|2020_08_29|\n",
      "|             Mirasol|        2|     es|                    NULL|               chalet|    1891|         Mirasol|   false|            10371| NULL| false|     true|   NULL|   true|     false|    true|41.4652441|2.0597371|Sant Cugat del Va...|        NULL|         false|                  NULL|       34|     sale|                        true|                                      true|                           NaN| 630000.0|     4200.0|    92713068|      chalet|Barcelona|    5|      false| 150.0|  good|   Mirasol, Sant Cug...|              Chalet|https://img3.idea...|            false|https://www.ideal...|2020_08_29|\n",
      "|                Golf|        6|     es|                    NULL|               chalet|     662|            Golf|   false|       VSC2101002| NULL| false|    false|   NULL|   true|     false|    true|41.4597204|2.0854806|Sant Cugat del Va...|        NULL|         false|                  NULL|       50|     sale|                        true|                                      true|                           NaN|2565000.0|     4750.0|    92606986|      chalet|Barcelona|    7|      false| 540.0|  good|   Golf, Sant Cugat ...|              Chalet|https://img3.idea...|            false|https://www.ideal...|2020_08_29|\n",
      "|              Arxius|        3|     es|           terracedHouse|               chalet|    1437|          Arxius|   false|            10706| NULL| false|     true|   NULL|   true|     false|   false|41.4686859|2.0742353|Sant Cugat del Va...|        NULL|         false|                  NULL|       46|     sale|                        true|                                      true|                           NaN| 820000.0|     3661.0|    93180929|      chalet|Barcelona|    4|      false| 224.0|  good|   Arxius, Sant Cuga...|      Chalet adosado|https://img3.idea...|            false|https://www.ideal...|2020_08_29|\n",
      "|Calle de Víctor C...|        2|     es|                  duplex|                 flat|    2546|    Sant Domènec|    true|            01283|    2|  true|     true|   true|   true|     false|    true|41.4782839|2.0706207|Sant Cugat del Va...|        NULL|         false|                  NULL|       35|     sale|                        true|                                      true|                           NaN| 650000.0|     3988.0|    39650907|      duplex|Barcelona|    4|      false| 163.0|  good|   Sant Domènec, San...|Dúplex en Calle d...|https://img3.idea...|            false|https://www.ideal...|2020_08_29|\n",
      "|      Calle de Verdi|        3|     es|                    NULL|               chalet|    1124|        Eixample|   false|     VSC2010012-1| NULL|  true|    false|   NULL|   true|     false|    true| 41.466069|2.0823066|Sant Cugat del Va...|        NULL|         false|                  NULL|       47|     sale|                        true|                                      true|                           NaN|1090000.0|     3406.0|    91700700|      chalet|Barcelona|    4|      false| 320.0|  good|   Eixample, Sant Cu...|Chalet en Calle d...|https://img3.idea...|            false|https://www.ideal...|2020_08_29|\n",
      "|              Arxius|        6|     es|                    NULL|               chalet|    1641|          Arxius|   false|            10329| NULL| false|     true|   NULL|   true|     false|    true|41.4680996| 2.067239|Sant Cugat del Va...|        NULL|         false|                  NULL|       66|     sale|                        true|                                      true|                           NaN|2800000.0|     4921.0|    92442950|      chalet|Barcelona|    6|      false| 569.0|  good|   Arxius, Sant Cuga...|              Chalet|https://img3.idea...|            false|https://www.ideal...|2020_08_29|\n",
      "|           Can Matas|        7|     es|                    NULL|               chalet|    2153|       Can Matas|   false|     VSC2003002-5| NULL| false|    false|   NULL|   true|     false|    true|41.4718127|2.0635986|Sant Cugat del Va...|        NULL|         false|                  NULL|       32|     sale|                        true|                                      true|                           NaN|1990000.0|     2535.0|    91784793|      chalet|Barcelona|    9|      false| 785.0|  good|   Can Matas, Sant C...|              Chalet|https://img3.idea...|            false|https://www.ideal...|2020_08_29|\n",
      "+--------------------+---------+-------+------------------------+---------------------+--------+----------------+--------+-----------------+-----+------+---------+-------+-------+----------+--------+----------+---------+--------------------+------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+---------+-----------+------------+------------+---------+-----+-----------+------+------+-----------------------+--------------------+--------------------+-----------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:44:48 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "24/05/15 18:44:55 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: longitude\n",
      "Q1: 2.0780549\n",
      "Q3: 2.1348477\n",
      "IQR: 0.056792799999999755\n",
      "Lower Bound: 1.9076765000000009\n",
      "Upper Bound: 2.305226099999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:45:00 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:45:11 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "24/05/15 18:45:18 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: numphotos\n",
      "Q1: 14.0\n",
      "Q3: 30.0\n",
      "IQR: 16.0\n",
      "Lower Bound: -34.0\n",
      "Upper Bound: 78.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:45:23 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "24/05/15 18:45:29 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 55\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:45:34 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "24/05/15 18:45:41 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+------------------------+---------------------+--------+--------------------+--------+-----------------+-----+------+---------+-------+-------+----------+--------+----------+---------+--------------------+--------------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+---------+-----------+------------+------------+---------+-----+-----------+-----+------+-----------------------+--------------------+--------------------+-----------------+--------------------+----------+\n",
      "|             address|bathrooms|country|detailedtype_subTypology|detailedtype_typology|distance|            district|exterior|externalreference|floor|has360|has3dtour|haslift|hasplan|hasstaging|hasvideo|  latitude|longitude|        municipality|        neighborhood|newdevelopment|newdevelopmentfinished|numphotos|operation|parkingspace_hasParkingSpace|parkingspace_isParkingSpaceIncludedInPrice|parkingspace_parkingSpacePrice|    price|pricebyarea|propertycode|propertytype| province|rooms|showaddress| size|status|suggestedtexts_subtitle|suggestedtexts_title|           thumbnail|topnewdevelopment|                 url|      date|\n",
      "+--------------------+---------+-------+------------------------+---------------------+--------+--------------------+--------+-----------------+-----+------+---------+-------+-------+----------+--------+----------+---------+--------------------+--------------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+---------+-----------+------------+------------+---------+-----+-----------+-----+------+-----------------------+--------------------+--------------------+-----------------+--------------------+----------+\n",
      "|La Floresta - Les...|        4|     es|        independantHouse|               chalet|    1144|La Floresta - Les...|   false|            V7347| NULL| false|     true|   NULL|   true|     false|    true|  41.44627|  2.07569|Sant Cugat del Va...|                NULL|         false|                  NULL|       94|     sale|                        true|                                      true|                           NaN| 980000.0|     2168.0|    93169921|      chalet|Barcelona|    5|      false|452.0|  good|   Can Matas, Sant C...|              Chalet|https://img3.idea...|            false|https://www.ideal...|2020_08_29|\n",
      "|Calle de les Ones...|        4|     es|        independantHouse|               chalet|     541|                Golf|   false|        CASA00035| NULL| false|     true|   NULL|   true|     false|    true|41.4550834|2.0853114|Sant Cugat del Va...|                NULL|         false|                  NULL|      124|     sale|                        true|                                      true|                           NaN| 975000.0|     2083.0|    89034383|      chalet|Barcelona|    6|       true|468.0|  good|   Golf, Sant Cugat ...|Casa independient...|https://img3.idea...|            false|https://www.ideal...|2020_08_29|\n",
      "|             BV-1462|        2|     es|                    NULL|                 flat|    2330|Parc Central - Co...|    true|         PISO0044|    2| false|     true|   true|   true|     false|    true|41.4757571|2.0687498|Sant Cugat del Va...|                NULL|         false|                  NULL|       92|     sale|                        true|                                      true|                           NaN| 460000.0|     4144.0|    87533753|        flat|Barcelona|    4|      false|111.0|  good|   Parc Central - Co...|     Piso en BV-1462|https://img3.idea...|            false|https://www.ideal...|2020_08_29|\n",
      "|Carretera de la a...|        1|     es|                    NULL|                 flat|    1002|Once de Septiembr...|    true|             NULL|    2| false|    false|   true|  false|     false|   false|41.3167975|2.0935192|El Prat de Llobregat|                NULL|         false|                  NULL|      122|     sale|                        NULL|                                      NULL|                           NaN| 230000.0|     3485.0|    92576834|        flat|Barcelona|    3|      false| 66.0|  good|   Once de Septiembr...|Piso en Carretera...|https://img3.idea...|            false|https://www.ideal...|2020_12_28|\n",
      "|        Eusebi Güell|        3|     es|        independantHouse|               chalet|    2253|            Marianao|   false|             3728| NULL| false|     true|   NULL|  false|     false|    true|41.3480285|2.0298978|Sant Boi de Llobr...|                NULL|         false|                  NULL|       81|     sale|                        true|                                      true|                           NaN|1300000.0|     3073.0|    91968115|      chalet|Barcelona|    6|      false|423.0|  good|   Marianao, Sant Bo...|Casa independient...|https://img3.idea...|            false|https://www.ideal...|2020_03_05|\n",
      "|Calle dels Castan...|        2|     es|        independantHouse|               chalet|    3235|            Marianao|   false|         CASA0047| NULL| false|     true|   NULL|   true|     false|    true|41.3530266|2.0202011|Sant Boi de Llobr...|                NULL|         false|                  NULL|      124|     sale|                        true|                                      true|                           NaN| 499000.0|     1801.0|    90336450|      chalet|Barcelona|    4|       true|277.0|  good|   Marianao, Sant Bo...|Casa independient...|https://img3.idea...|            false|https://www.ideal...|2020_03_05|\n",
      "|Calle dels Castan...|        2|     es|        independantHouse|               chalet|    3235|            Marianao|   false|         CASA0047| NULL| false|     true|   NULL|   true|     false|    true|41.3530266|2.0202011|Sant Boi de Llobr...|                NULL|         false|                  NULL|      124|     sale|                        true|                                      true|                           NaN| 499000.0|     1801.0|    90336450|      chalet|Barcelona|    4|       true|277.0|  good|   Marianao, Sant Bo...|Casa independient...|https://img3.idea...|            false|https://www.ideal...|2020_05_08|\n",
      "|        Eusebi Güell|        3|     es|        independantHouse|               chalet|    2253|            Marianao|   false|             3728| NULL| false|     true|   NULL|  false|     false|    true|41.3480285|2.0298978|Sant Boi de Llobr...|                NULL|         false|                  NULL|       81|     sale|                        true|                                      true|                           NaN|1300000.0|     3073.0|    91968115|      chalet|Barcelona|    6|      false|423.0|  good|   Marianao, Sant Bo...|Casa independient...|https://img3.idea...|            false|https://www.ideal...|2020_05_08|\n",
      "|Calle de les Ones...|        4|     es|        independantHouse|               chalet|    1688|                Golf|   false|        CASA00035| NULL| false|     true|   NULL|   true|     false|    true|41.4550834|2.0853114|Sant Cugat del Va...|                NULL|         false|                  NULL|      124|     sale|                        true|                                      true|                           NaN| 975000.0|     2083.0|    89034383|      chalet|Barcelona|    6|       true|468.0|  good|   Golf, Sant Cugat ...|Casa independient...|https://img3.idea...|            false|https://www.ideal...|2020_09_15|\n",
      "|La Floresta - Les...|        4|     es|        independantHouse|               chalet|    2722|La Floresta - Les...|   false|            V7347| NULL| false|     true|   NULL|   true|     false|    true|  41.44627|  2.07569|Sant Cugat del Va...|                NULL|         false|                  NULL|       94|     sale|                        true|                                      true|                           NaN| 980000.0|     2168.0|    93169921|      chalet|Barcelona|    5|      false|452.0|  good|   Centre - Estació,...|              Chalet|https://img3.idea...|            false|https://www.ideal...|2020_09_15|\n",
      "|Calle de les Ones...|        4|     es|        independantHouse|               chalet|     541|                Golf|   false|        CASA00035| NULL| false|     true|   NULL|   true|     false|    true|41.4550834|2.0853114|Sant Cugat del Va...|                NULL|         false|                  NULL|      124|     sale|                        true|                                      true|                           NaN| 975000.0|     2083.0|    89034383|      chalet|Barcelona|    6|       true|468.0|  good|   Golf, Sant Cugat ...|Casa independient...|https://img3.idea...|            false|https://www.ideal...|2020_08_30|\n",
      "|La Floresta - Les...|        4|     es|           terracedHouse|                 flat|    1144|La Floresta - Les...|   false|            V7347| NULL| false|     true|   NULL|   true|     false|    true|  41.44627|  2.07569|Sant Cugat del Va...|                NULL|         false|                  NULL|       94|     sale|                        true|                                      true|                           NaN| 980000.0|     2168.0|    93169921|      chalet|Barcelona|    5|      false|452.0|  good|   Can Matas, Sant C...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_08_30|\n",
      "|     ENRIC LANFRANCO|        5|     es|        independantHouse|               chalet|     490|                Golf|   false|    ENRIC  - GOLF| NULL| false|    false|   NULL|  false|     false|   false| 41.457551|2.0733786|Sant Cugat del Va...|                NULL|         false|                  NULL|       92|     sale|                        true|                                      true|                           NaN| 650000.0|     1161.0|    87325831|      chalet|Barcelona|    6|      false|560.0|  good|   Golf, Sant Cugat ...|Casa independient...|https://img3.idea...|            false|https://www.ideal...|2020_08_30|\n",
      "|             BV-1462|        2|     es|                    NULL|                 flat|    2330|Parc Central - Co...|    true|         PISO0044|    2| false|     true|   true|   true|     false|    true|41.4757571|2.0687498|Sant Cugat del Va...|                NULL|         false|                  NULL|       92|     sale|                        true|                                      true|                           NaN| 460000.0|     4144.0|    87533753|        flat|Barcelona|    4|      false|111.0|  good|   Parc Central - Co...|     Piso en BV-1462|https://img3.idea...|            false|https://www.ideal...|2020_08_30|\n",
      "|Calle de Villà, 8...|        7|     es|           terracedHouse|               chalet|     796|                Golf|   false|         CASA0055|   bj| false|    false|   NULL|   true|     false|    true|41.4631898|2.0765739|Sant Cugat del Va...|                NULL|         false|                  NULL|      136|     sale|                        true|                                      true|                           NaN|1380000.0|     2438.0|    91527712|      chalet|Barcelona|    6|       true|566.0|  good|   Golf, Sant Cugat ...|Chalet adosado en...|https://img3.idea...|            false|https://www.ideal...|2020_08_30|\n",
      "|Calle de Villà, 8...|        6|     es|                    NULL|               chalet|     796|                Golf|   false|         CASA0054|   bj| false|     true|   NULL|   true|     false|   false|41.4631898|2.0765739|Sant Cugat del Va...|                NULL|         false|                  NULL|      140|     sale|                        true|                                      true|                           NaN|1425000.0|     2518.0|    91500488|      chalet|Barcelona|    6|       true|566.0|  good|   Golf, Sant Cugat ...|Chalet en Calle d...|https://img3.idea...|            false|https://www.ideal...|2020_08_30|\n",
      "|          Valldoreix|        5|     es|        independantHouse|               chalet|    2061|          Valldoreix|   false|             4188| NULL| false|    false|   NULL|   true|     false|    true|  41.45064|  2.05543|Sant Cugat del Va...|                NULL|         false|                  NULL|       94|     sale|                        NULL|                                      NULL|                           NaN|1445000.0|     2737.0|    39705259|      chalet|Barcelona|    6|      false|528.0|  good|   Valldoreix, Sant ...|  Casa independiente|https://img3.idea...|            false|https://www.ideal...|2020_08_30|\n",
      "|barrio Vallvidrer...|        3|     es|        independantHouse|               chalet|    2945| Sarrià-Sant Gervasi|   false|         W-02DIY5| NULL| false|     true|   NULL|   true|     false|   false|41.4207587|2.1075265|           Barcelona|Vallvidrera - El ...|         false|                  NULL|       91|     sale|                        true|                                      true|                          NULL|1750000.0|     3708.0|    83236188|      chalet|Barcelona|    7|      false|472.0|  good|   Vallvidrera - El ...|  Casa independiente|https://img3.idea...|            false|https://www.ideal...|2020_06_04|\n",
      "|La Floresta - Les...|        4|     es|           terracedHouse|               chalet|    1132|La Floresta - Les...|   false|            V7347| NULL| false|     true|   NULL|   true|     false|    true|  41.44627|  2.07569|Sant Cugat del Va...|                NULL|         false|                  NULL|       94|     sale|                        true|                                      true|                          NULL| 980000.0|     2168.0|    93169921|      chalet|Barcelona|    5|      false|452.0|  good|   Arxius, Sant Cuga...|      Chalet adosado|https://img3.idea...|            false|https://www.ideal...|2020_06_04|\n",
      "|Calle de Villà, 8...|        7|     es|           terracedHouse|               chalet|    2986|                Golf|   false|         CASA0055|   bj| false|    false|   NULL|   true|     false|    true|41.4631898|2.0765739|Sant Cugat del Va...|                NULL|         false|                  NULL|      136|     sale|                        true|                                      true|                          NULL|1380000.0|     2438.0|    91527712|      chalet|Barcelona|    6|       true|566.0|  good|   Golf, Sant Cugat ...|Chalet adosado en...|https://img3.idea...|            false|https://www.ideal...|2020_06_04|\n",
      "+--------------------+---------+-------+------------------------+---------------------+--------+--------------------+--------+-----------------+-----+------+---------+-------+-------+----------+--------+----------+---------+--------------------+--------------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+---------+-----------+------------+------------+---------+-----+-----------+-----+------+-----------------------+--------------------+--------------------+-----------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:45:45 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "24/05/15 18:45:52 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: parkingspace_parkingSpacePrice\n",
      "Q1: 16000.0\n",
      "Q3: 25000.0\n",
      "IQR: 9000.0\n",
      "Lower Bound: -11000.0\n",
      "Upper Bound: 52000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:45:56 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/05/15 18:46:01 WARN DAGScheduler: Broadcasting large task binary with size 1597.4 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 17672\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:46:05 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/05/15 18:46:09 WARN DAGScheduler: Broadcasting large task binary with size 1614.3 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+------------------------+---------------------+--------+--------------------+--------+------------------+-----+------+---------+-------+-------+----------+--------+----------+---------+--------------------+-------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+--------+-----------+------------+------------+---------+-----+-----------+-----+--------------+-----------------------+--------------------+--------------------+-----------------+--------------------+----------+\n",
      "|             address|bathrooms|country|detailedtype_subTypology|detailedtype_typology|distance|            district|exterior| externalreference|floor|has360|has3dtour|haslift|hasplan|hasstaging|hasvideo|  latitude|longitude|        municipality| neighborhood|newdevelopment|newdevelopmentfinished|numphotos|operation|parkingspace_hasParkingSpace|parkingspace_isParkingSpaceIncludedInPrice|parkingspace_parkingSpacePrice|   price|pricebyarea|propertycode|propertytype| province|rooms|showaddress| size|        status|suggestedtexts_subtitle|suggestedtexts_title|           thumbnail|topnewdevelopment|                 url|      date|\n",
      "+--------------------+---------+-------+------------------------+---------------------+--------+--------------------+--------+------------------+-----+------+---------+-------+-------+----------+--------+----------+---------+--------------------+-------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+--------+-----------+------------+------------+---------+-----+-----------+-----+--------------+-----------------------+--------------------+--------------------+-----------------+--------------------+----------+\n",
      "|Calle Santa Eulal...|        3|     es|                  duplex|                 flat|      94|       Santa Eulàlia|    true|     1º Dx tipo 5 |    1| false|    false|   true|  false|     false|   false|41.3665077|2.1236566|Hospitalet de Llo...|Santa Eulàlia|          true|                 false|        6|     sale|                        true|                                      true|                           NaN|472900.0|     3402.0|    88662745|      duplex|Barcelona|    3|       true|139.0|newdevelopment|   Santa Eulàlia, Ho...|Dúplex en Calle S...|https://img3.idea...|            false|https://www.ideal...|2021_01_13|\n",
      "|Calle Santa Eulal...|        3|     es|                  duplex|                 flat|     248|       Santa Eulàlia|    true|     1º Dx tipo 5 |    1| false|    false|   true|  false|     false|   false|41.3665077|2.1236566|Hospitalet de Llo...|Santa Eulàlia|          true|                 false|        6|     sale|                        true|                                      true|                           NaN|472900.0|     3402.0|    88662745|      duplex|Barcelona|    3|       true|139.0|newdevelopment|   Santa Eulàlia, Ho...|Dúplex en Calle S...|https://img3.idea...|            false|https://www.ideal...|2020_04_06|\n",
      "|Calle Santa Eulal...|        3|     es|                  duplex|                 flat|    1281|       Santa Eulàlia|    true|     1º Dx tipo 5 |    1| false|    false|   true|  false|     false|   false|41.3665077|2.1236566|Hospitalet de Llo...|Santa Eulàlia|          true|                 false|        6|     sale|                        true|                                      true|                           NaN|490000.0|     3525.0|    88662745|      duplex|Barcelona|    3|       true|139.0|newdevelopment|   Santa Eulàlia, Ho...|Dúplex en Calle S...|https://img3.idea...|            false|https://www.ideal...|2020_12_07|\n",
      "|Calle Santa Eulal...|        3|     es|                  duplex|                 flat|     428|       Santa Eulàlia|    true|     1º Dx tipo 5 |    1| false|    false|   true|  false|     false|   false|41.3665077|2.1236566|Hospitalet de Llo...|Santa Eulàlia|          true|                 false|        6|     sale|                        true|                                      true|                           NaN|472900.0|     3402.0|    88662745|      duplex|Barcelona|    3|       true|139.0|newdevelopment|   Santa Eulàlia, Ho...|Dúplex en Calle S...|https://img3.idea...|            false|https://www.ideal...|2020_09_14|\n",
      "|Camino Oliveretes...|        2|     es|                    NULL|                 flat|    2621|Campreciós - Torr...|    true|               A13| NULL| false|    false|   true|   true|     false|   false|41.3233639|2.0222618|          Viladecans|         NULL|          true|                 false|       17|     sale|                        NULL|                                      NULL|                           NaN|267000.0|     2618.0|    92794636|        flat|Barcelona|    3|       true|102.0|newdevelopment|   Campreciós - Torr...|Piso en Camino Ol...|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|Camino Vell de Vi...|        1|     es|                    NULL|                 flat|    1931|Plaça Catalunya -...|   false|     11697-21535-V| NULL| false|    false|   NULL|  false|     false|   false|41.3338004|2.0464347|Sant Boi de Llobr...|         NULL|         false|                  NULL|       19|     sale|                        NULL|                                      NULL|                           NaN|124000.0|     1908.0|    92745196|        flat|Barcelona|    3|      false| 65.0|         renew|   Plaça Catalunya -...|Piso en Camino Ve...|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|    Parc Empresarial|        2|     es|                    NULL|                 flat|    2091|    Parc Empresarial|    true|              3350|    1| false|    false|   true|  false|     false|   false|41.3210463|2.0280296|          Viladecans|         NULL|         false|                  NULL|       13|     sale|                        NULL|                                      NULL|                           NaN|282000.0|     3279.0|    91923763|        flat|Barcelona|    3|      false| 86.0|          good|   Parc Empresarial,...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|  Calle del Montsant|        1|     es|                    NULL|                 flat|    3032|  Plaça de Catalunya|   false|         1105217_V|    1| false|    false|   true|  false|     false|   false|41.3200335|2.0885909|El Prat de Llobregat|         NULL|         false|                  NULL|       17|     sale|                        NULL|                                      NULL|                           NaN|161000.0|     2333.0|    91942929|        flat|Barcelona|    3|      false| 69.0|          good|   Plaça de Cataluny...|Piso en Calle del...|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|Plaça Catalunya -...|        2|     es|                    NULL|                 flat|    1973|Plaça Catalunya -...|    true|            003409|    4| false|    false|   true|  false|     false|   false|41.3330656|2.0423616|Sant Boi de Llobr...|         NULL|         false|                  NULL|       15|     sale|                        NULL|                                      NULL|                           NaN|359000.0|     2258.0|    90986335|        flat|Barcelona|    4|      false|159.0|          good|   Plaça Catalunya -...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|Plaça Catalunya -...|        2|     es|               penthouse|                 flat|    2418|Plaça Catalunya -...|    true|            003340|    2| false|    false|  false|  false|     false|   false|41.3379944|2.0447407|Sant Boi de Llobr...|         NULL|         false|                  NULL|       16|     sale|                        NULL|                                      NULL|                           NaN|326000.0|     2763.0|    88580541|   penthouse|Barcelona|    4|      false|118.0|          good|   Plaça Catalunya -...|               Ático|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|Mossèn Antoni Sol...|        2|     es|                    NULL|                 flat|    2190|Plaça Catalunya -...|    true|            SB2948|    1| false|    false|   true|  false|     false|   false|41.3354973|2.0433332|Sant Boi de Llobr...|         NULL|         false|                  NULL|       25|     sale|                        NULL|                                      NULL|                           NaN|260000.0|     1970.0|    92292107|        flat|Barcelona|    4|      false|132.0|          good|   Plaça Catalunya -...|Piso en Mossèn An...|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|Campreciós - Torr...|        3|     es|                    NULL|               chalet|    2569|Campreciós - Torr...|   false|            VD489D|   bj| false|    false|   NULL|  false|     false|   false|41.3200837|2.0219946|          Viladecans|         NULL|         false|                  NULL|       34|     sale|                        NULL|                                      NULL|                           NaN|512500.0|     1986.0|    92721581|      chalet|Barcelona|    5|      false|258.0|          good|   Campreciós - Torr...|              Chalet|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|Plaça Catalunya -...|        1|     es|               penthouse|                 flat|    2902|Plaça Catalunya -...|    true|            003735|    4| false|    false|   true|  false|     false|   false| 41.341548|2.0405448|Sant Boi de Llobr...|         NULL|         false|                  NULL|       11|     sale|                        NULL|                                      NULL|                           NaN|217000.0|     2713.0|    86333666|   penthouse|Barcelona|    2|      false| 80.0|          good|   Plaça Catalunya -...|               Ático|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|        C-245, 11 -3|        1|     es|                    NULL|                 flat|    2502|    Parc Empresarial|    true|          73233123|    4| false|    false|  false|  false|     false|   false|41.3174363|2.0225362|          Viladecans|         NULL|         false|                  NULL|        4|     sale|                        NULL|                                      NULL|                           NaN|129200.0|     2118.0|    92672048|        flat|Barcelona|    3|       true| 61.0|          good|   Parc Empresarial,...|Piso en C-245, 11 -3|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|Plaça Catalunya -...|        1|     es|                    NULL|                 flat|    2541|Plaça Catalunya -...|    true|             A2873|    2| false|    false|   true|  false|     false|   false|41.3382892|2.0413099|Sant Boi de Llobr...|         NULL|         false|                  NULL|        8|     sale|                        NULL|                                      NULL|                           NaN|169000.0|     2061.0|    85809431|        flat|Barcelona|    3|      false| 82.0|          good|   Plaça Catalunya -...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|Carretera Santa C...|        1|     es|                    NULL|                 flat|    2520|Plaça Catalunya -...|   false|     11697-21570-V| NULL| false|    false|   true|  false|     false|   false|41.3395233|2.0487389|Sant Boi de Llobr...|         NULL|         false|                  NULL|       18|     sale|                        NULL|                                      NULL|                           NaN|165000.0|     2171.0|    92745137|        flat|Barcelona|    3|      false| 76.0|          good|   Plaça Catalunya -...|Piso en Carretera...|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|Calle de Guifré e...|        1|     es|                    NULL|                 flat|    2362|    Parc Empresarial|   false|           7262222|    1| false|    false|   true|  false|     false|   false|41.3152119|2.0243162|          Viladecans|         NULL|         false|                  NULL|        9|     sale|                        NULL|                                      NULL|                           NaN|163900.0|     2875.0|    89188138|        flat|Barcelona|    2|      false| 57.0|          good|   Parc Empresarial,...|Piso en Calle de ...|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|Plaça Catalunya -...|        1|     es|                    NULL|                 flat|    2545|Plaça Catalunya -...|    true|              3336|    3| false|    false|  false|  false|     false|   false|41.3393231|2.0455107|Sant Boi de Llobr...|         NULL|         false|                  NULL|       10|     sale|                        NULL|                                      NULL|                           NaN|144000.0|     1946.0|    91234936|        flat|Barcelona|    3|      false| 74.0|          good|   Plaça Catalunya -...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|Plaça Catalunya -...|        1|     es|                    NULL|                 flat|    2369|Plaça Catalunya -...|   false|VI-000-030-647-778| NULL| false|    false|   NULL|   true|     false|   false|41.3375648|2.0449091|Sant Boi de Llobr...|         NULL|         false|                  NULL|       15|     sale|                        NULL|                                      NULL|                           NaN|159500.0|     2155.0|    91101662|        flat|Barcelona|    3|      false| 74.0|          good|   Plaça Catalunya -...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|Barri Antic - Centre|        2|     es|                    NULL|                 flat|    2775|Barri Antic - Centre|   false|              NULL| NULL| false|    false|   true|  false|     false|   false|  41.31591|   2.0193|          Viladecans|         NULL|          true|                 false|        4|     sale|                        NULL|                                      NULL|                           NaN|265000.0|     3081.0|    91582321|        flat|Barcelona|    3|      false| 86.0|newdevelopment|   Barri Antic - Cen...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "+--------------------+---------+-------+------------------------+---------------------+--------+--------------------+--------+------------------+-----+------+---------+-------+-------+----------+--------+----------+---------+--------------------+-------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+--------+-----------+------------+------------+---------+-----+-----------+-----+--------------+-----------------------+--------------------+--------------------+-----------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:46:14 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "24/05/15 18:46:21 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: price\n",
      "Q1: 170000.0\n",
      "Q3: 495000.0\n",
      "IQR: 325000.0\n",
      "Lower Bound: -805000.0\n",
      "Upper Bound: 1470000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:46:26 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "24/05/15 18:46:33 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 1026\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:46:38 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "24/05/15 18:46:45 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+------------------------+---------------------+--------+-------------------+--------+-----------------+-----+------+---------+-------+-------+----------+--------+----------+---------+------------+--------------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+---------+-----------+------------+------------+---------+-----+-----------+-----+------+-----------------------+--------------------+--------------------+-----------------+--------------------+----------+\n",
      "|             address|bathrooms|country|detailedtype_subTypology|detailedtype_typology|distance|           district|exterior|externalreference|floor|has360|has3dtour|haslift|hasplan|hasstaging|hasvideo|  latitude|longitude|municipality|        neighborhood|newdevelopment|newdevelopmentfinished|numphotos|operation|parkingspace_hasParkingSpace|parkingspace_isParkingSpaceIncludedInPrice|parkingspace_parkingSpacePrice|    price|pricebyarea|propertycode|propertytype| province|rooms|showaddress| size|status|suggestedtexts_subtitle|suggestedtexts_title|           thumbnail|topnewdevelopment|                 url|      date|\n",
      "+--------------------+---------+-------+------------------------+---------------------+--------+-------------------+--------+-----------------+-----+------+---------+-------+-------+----------+--------+----------+---------+------------+--------------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+---------+-----------+------------+------------+---------+-----+-----------+-----+------+-----------------------+--------------------+--------------------+-----------------+--------------------+----------+\n",
      "|    barrio Pedralbes|        6|     es|               penthouse|                 flat|     660|          Les Corts|    true|         BCN26015| NULL| false|    false|   true|   true|     false|   false| 41.394721| 2.118681|   Barcelona|           Pedralbes|         false|                  NULL|       59|     sale|                        true|                                      true|                          NULL|3500000.0|     8621.0|    92176342|   penthouse|Barcelona|    5|      false|406.0|  good|   Pedralbes, Barcelona|               Ático|https://img3.idea...|            false|https://www.ideal...|2020_02_10|\n",
      "|barrio Sant Gerva...|        5|     es|                    NULL|                 flat|     848|Sarrià-Sant Gervasi|    true|         BCN23066| NULL| false|    false|   true|  false|     false|   false|41.3964254|2.1366062|   Barcelona|Sant Gervasi - Ga...|         false|                  NULL|       35|     sale|                        true|                                      true|                          NULL|2200000.0|     6667.0|    89237439|        flat|Barcelona|    5|      false|330.0|  good|   Sant Gervasi - Ga...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_02_10|\n",
      "|barrio Les Tres T...|        3|     es|           terracedHouse|               chalet|     677|Sarrià-Sant Gervasi|   false|            VPV06| NULL| false|    false|   NULL|  false|     false|   false|41.3970757|2.1344179|   Barcelona|     Les Tres Torres|         false|                  NULL|       30|     sale|                        true|                                      true|                          NULL|2750000.0|     9821.0|    91965832|      chalet|Barcelona|    3|      false|280.0|  good|   Les Tres Torres, ...|      Chalet adosado|https://img3.idea...|            false|https://www.ideal...|2020_02_10|\n",
      "|Pasaje claudio güell|        4|     es|       semidetachedHouse|               chalet|     423|Sarrià-Sant Gervasi|   false|           VA1163| NULL| false|    false|   NULL|   true|     false|   false|41.3927514|2.1234886|   Barcelona|              Sarrià|         false|                  NULL|       19|     sale|                        true|                                      true|                          NULL|2280000.0|     8444.0|    86563923|      chalet|Barcelona|    6|      false|270.0|  good|      Sarrià, Barcelona|Chalet pareado en...|https://img3.idea...|            false|https://www.ideal...|2020_02_10|\n",
      "|         Via Augusta|        4|     es|               penthouse|                 flat|     792|Sarrià-Sant Gervasi|    true|            10650| NULL| false|     true|   true|   true|     false|   false|41.3989684|2.1350011|   Barcelona|Sant Gervasi - Ga...|         false|                  NULL|       62|     sale|                        true|                                      true|                          NULL|1690000.0|     5828.0|    92890890|   penthouse|Barcelona|    4|      false|290.0|  good|   Sant Gervasi - Ga...|Ático en Via Augusta|https://img3.idea...|            false|https://www.ideal...|2020_02_10|\n",
      "|Calle dels Cavallers|        6|     es|                  duplex|                 flat|     855|          Les Corts|    true|          D-40637|    5| false|    false|   true|  false|     false|    true| 41.393521| 2.116681|   Barcelona|           Pedralbes|         false|                  NULL|       54|     sale|                        true|                                      true|                          NULL|3500000.0|     7056.0|    91211684|      duplex|Barcelona|    6|      false|496.0|  good|   Pedralbes, Barcelona|Dúplex en Calle d...|https://img3.idea...|            false|https://www.ideal...|2020_02_10|\n",
      "|       barrio Sarrià|        6|     es|                    NULL|               chalet|     619|Sarrià-Sant Gervasi|   false|         BCN14958| NULL| false|    false|   NULL|  false|     false|   false|41.4005037|2.1224093|   Barcelona|              Sarrià|         false|                  NULL|       35|     sale|                        true|                                      true|                          NULL|3900000.0|     7617.0|    88376448|      chalet|Barcelona|    6|      false|512.0| renew|      Sarrià, Barcelona|              Chalet|https://img3.idea...|            false|https://www.ideal...|2020_02_10|\n",
      "|barrio Les Tres T...|        3|     es|                    NULL|               chalet|     577|Sarrià-Sant Gervasi|   false|         BCN11347| NULL| false|    false|   NULL|   true|     false|   false| 41.396468|2.1333346|   Barcelona|     Les Tres Torres|         false|                  NULL|       45|     sale|                        true|                                      true|                          NULL|2750000.0|    12277.0|    81345140|      chalet|Barcelona|    2|      false|224.0|  good|   Les Tres Torres, ...|              Chalet|https://img3.idea...|            false|https://www.ideal...|2020_02_10|\n",
      "|barrio Les Tres T...|        5|     es|                    NULL|                 flat|     648|Sarrià-Sant Gervasi|    true|         BCN24619| NULL| false|    false|   true|   true|     false|   false| 41.401307|2.1291549|   Barcelona|     Les Tres Torres|         false|                  NULL|       43|     sale|                        true|                                      true|                          NULL|2300000.0|     6553.0|    90571031|        flat|Barcelona|    6|      false|351.0| renew|   Les Tres Torres, ...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_02_10|\n",
      "|       Angel Guimerá|        5|     es|        independantHouse|               chalet|     655|Sarrià-Sant Gervasi|   false|           C.A.G.| NULL| false|    false|   NULL|   true|     false|    true|41.3971669|2.1341217|   Barcelona|     Les Tres Torres|         false|                  NULL|       39|     sale|                        NULL|                                      NULL|                          NULL|2490000.0|     7324.0|    93086874|      chalet|Barcelona|    5|      false|340.0|  good|   Les Tres Torres, ...|Casa independient...|https://img3.idea...|            false|https://www.ideal...|2020_02_10|\n",
      "|    barrio Pedralbes|        6|     es|               penthouse|                 flat|     798|          Les Corts|    true|         BCN26015| NULL| false|    false|   true|   true|     false|   false| 41.394721| 2.118681|   Barcelona|           Pedralbes|         false|                  NULL|       59|     sale|                        true|                                      true|                          NULL|3500000.0|     8621.0|    92176342|   penthouse|Barcelona|    5|      false|406.0|  good|   Pedralbes, Barcelona|               Ático|https://img3.idea...|            false|https://www.ideal...|2020_08_15|\n",
      "|     Calle de Tòquio|        5|     es|                    NULL|                 flat|     698|          Les Corts|    true|           V-2061|    4| false|    false|   true|  false|     false|   false|41.3892084|2.1182765|   Barcelona|           Pedralbes|         false|                  NULL|       27|     sale|                        true|                                      true|                          NULL|1800000.0|     5000.0|    89747920|        flat|Barcelona|    6|      false|360.0| renew|   Pedralbes, Barcelona|Piso en Calle de ...|https://img3.idea...|            false|https://www.ideal...|2020_08_15|\n",
      "|    barrio Pedralbes|        6|     es|                  duplex|                 flat|     783|          Les Corts|    true|             1204|    5| false|    false|   true|   true|     false|   false|41.3923064| 2.117372|   Barcelona|           Pedralbes|         false|                  NULL|        9|     sale|                        NULL|                                      NULL|                          NULL|2500000.0|     5593.0|    91797141|      duplex|Barcelona|    8|      false|447.0| renew|   Pedralbes, Barcelona|              Dúplex|https://img3.idea...|            false|https://www.ideal...|2020_08_15|\n",
      "|Pasaje claudio güell|        4|     es|       semidetachedHouse|               chalet|     349|Sarrià-Sant Gervasi|   false|           VA1163| NULL| false|    false|   NULL|   true|     false|   false|41.3927514|2.1234886|   Barcelona|              Sarrià|         false|                  NULL|       19|     sale|                        true|                                      true|                          NULL|2280000.0|     8444.0|    86563923|      chalet|Barcelona|    6|      false|270.0|  good|      Sarrià, Barcelona|Chalet pareado en...|https://img3.idea...|            false|https://www.ideal...|2020_08_15|\n",
      "|         Via Augusta|        4|     es|               penthouse|                 flat|     862|Sarrià-Sant Gervasi|    true|            10510| NULL| false|     true|   true|   true|     false|   false|41.4010399|2.1443707|   Barcelona|Sant Gervasi - Ga...|         false|                  NULL|       62|     sale|                        true|                                      true|                           NaN|1690000.0|     5828.0|    92680123|   penthouse|Barcelona|    4|      false|290.0|  good|   Sant Gervasi - Ga...|Ático en Via Augusta|https://img3.idea...|            false|https://www.ideal...|2020_06_03|\n",
      "|     Calle de Balmes|        4|     es|                    NULL|                 flat|     841|Sarrià-Sant Gervasi|    true|             4165|    5|  true|    false|   true|   true|     false|   false|41.4033211|2.1450538|   Barcelona|Sant Gervasi - Ga...|         false|                  NULL|       35|     sale|                        true|                                      true|                           NaN|1900000.0|     6859.0|    87837396|        flat|Barcelona|    5|      false|277.0|  good|   Sant Gervasi - Ga...|Piso en Calle de ...|https://img3.idea...|            false|https://www.ideal...|2020_06_03|\n",
      "|barrio La Dreta d...|        3|     es|                    NULL|                 flat|     747|           Eixample|    true|         BCN19869| NULL| false|    false|   true|   true|     false|   false|41.3948533|2.1578979|   Barcelona|La Dreta de l'Eix...|         false|                  NULL|       24|     sale|                        NULL|                                      NULL|                           NaN|1850000.0|    10632.0|    91178037|        flat|Barcelona|    3|      false|174.0|  good|   La Dreta de l'Eix...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_06_03|\n",
      "|barrio La Dreta d...|        4|     es|               penthouse|                 flat|     675|           Eixample|   false|         W-02KD38| NULL| false|    false|   true|   true|     false|   false|41.3959627|2.1589645|   Barcelona|La Dreta de l'Eix...|         false|                  NULL|       38|     sale|                        true|                                      true|                           NaN|    1.2E7|    17143.0|    92232387|   penthouse|Barcelona|    4|      false|700.0|  good|   La Dreta de l'Eix...|               Ático|https://img3.idea...|            false|https://www.ideal...|2020_06_03|\n",
      "|barrio La Dreta d...|        2|     es|                    NULL|                 flat|     741|           Eixample|    true|           019197|    4|  true|    false|   true|   true|     false|   false|41.3978964|2.1624854|   Barcelona|La Dreta de l'Eix...|         false|                  NULL|       40|     sale|                        NULL|                                      NULL|                           NaN|1600000.0|     9143.0|    86560022|        flat|Barcelona|    3|      false|175.0|  good|   La Dreta de l'Eix...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_06_03|\n",
      "|barrio La Dreta d...|        3|     es|                    NULL|                 flat|     774|           Eixample|   false|         W-02JSFC|    1| false|    false|   true|   true|     false|   false|41.3966291|2.1617948|   Barcelona|La Dreta de l'Eix...|         false|                  NULL|       31|     sale|                        true|                                      true|                           NaN|1500000.0|    10000.0|    92847091|        flat|Barcelona|    3|      false|150.0|  good|   La Dreta de l'Eix...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_06_03|\n",
      "+--------------------+---------+-------+------------------------+---------------------+--------+-------------------+--------+-----------------+-----+------+---------+-------+-------+----------+--------+----------+---------+------------+--------------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+---------+-----------+------------+------------+---------+-----+-----------+-----+------+-----------------------+--------------------+--------------------+-----------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:46:49 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "24/05/15 18:46:56 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: pricebyarea\n",
      "Q1: 2315.0\n",
      "Q3: 4119.0\n",
      "IQR: 1804.0\n",
      "Lower Bound: -3097.0\n",
      "Upper Bound: 9531.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:47:01 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "24/05/15 18:47:08 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 251\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:47:13 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "24/05/15 18:47:20 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+------------------------+---------------------+--------+-------------------+--------+-----------------+-----+------+---------+-------+-------+----------+--------+----------+---------+------------+--------------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+---------+-----------+------------+------------+---------+-----+-----------+-----+------+-----------------------+--------------------+--------------------+-----------------+--------------------+----------+\n",
      "|             address|bathrooms|country|detailedtype_subTypology|detailedtype_typology|distance|           district|exterior|externalreference|floor|has360|has3dtour|haslift|hasplan|hasstaging|hasvideo|  latitude|longitude|municipality|        neighborhood|newdevelopment|newdevelopmentfinished|numphotos|operation|parkingspace_hasParkingSpace|parkingspace_isParkingSpaceIncludedInPrice|parkingspace_parkingSpacePrice|    price|pricebyarea|propertycode|propertytype| province|rooms|showaddress| size|status|suggestedtexts_subtitle|suggestedtexts_title|           thumbnail|topnewdevelopment|                 url|      date|\n",
      "+--------------------+---------+-------+------------------------+---------------------+--------+-------------------+--------+-----------------+-----+------+---------+-------+-------+----------+--------+----------+---------+------------+--------------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+---------+-----------+------------+------------+---------+-----+-----------+-----+------+-----------------------+--------------------+--------------------+-----------------+--------------------+----------+\n",
      "|    barrio Les Corts|        2|     es|                    NULL|                 flat|     310|          Les Corts|    true|         BCN25172| NULL| false|    false|   true|  false|     false|   false|41.3866784|2.1303448|   Barcelona|           Les Corts|         false|                  NULL|       30|     sale|                        NULL|                                      NULL|                          NULL| 978000.0|    11241.0|    90996261|        flat|Barcelona|    3|      false| 87.0|  good|   Les Corts, Barcelona|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_04_05|\n",
      "|Calle de Can Segalar|        2|     es|                    NULL|                 flat|     451|          Les Corts|    true|           019258|    6| false|    false|   true|   true|     false|   false|41.3889708|2.1313964|   Barcelona|           Les Corts|         false|                  NULL|       30|     sale|                        NULL|                                      NULL|                          NULL|1454000.0|    12017.0|    88551642|        flat|Barcelona|    3|      false|121.0|  good|   Les Corts, Barcelona|Piso en Calle de ...|https://img3.idea...|            false|https://www.ideal...|2020_04_05|\n",
      "|Calle de Can Segalar|        2|     es|                    NULL|                 flat|     197|          Les Corts|   false|            GV614|    6| false|    false|   true|   true|     false|    true|  41.38687|2.1325946|   Barcelona|           Les Corts|         false|                  NULL|       15|     sale|                        NULL|                                      NULL|                          NULL|1454000.0|    12017.0|    92056660|        flat|Barcelona|    3|      false|121.0|  good|   Les Corts, Barcelona|Piso en Calle de ...|https://img3.idea...|            false|https://www.ideal...|2020_04_05|\n",
      "|Calle de Can Segalar|        2|     es|                    NULL|                 flat|     207|          Les Corts|    true|           019107|    5| false|    false|   true|   true|     false|   false|41.3871037|2.1336023|   Barcelona|           Les Corts|         false|                  NULL|       29|     sale|                        NULL|                                      NULL|                          NULL|1082000.0|    11389.0|    39916328|        flat|Barcelona|    3|      false| 95.0|  good|   Les Corts, Barcelona|Piso en Calle de ...|https://img3.idea...|            false|https://www.ideal...|2020_04_05|\n",
      "|Calle de Can Segalar|        2|     es|                    NULL|                 flat|     241|          Les Corts|    true|           019116|    4| false|    false|   true|  false|     false|   false|41.3874037|2.1334023|   Barcelona|           Les Corts|         false|                  NULL|       26|     sale|                        NULL|                                      NULL|                          NULL| 862000.0|    10512.0|    40123363|        flat|Barcelona|    2|      false| 82.0|  good|   Les Corts, Barcelona|Piso en Calle de ...|https://img3.idea...|            false|https://www.ideal...|2020_04_05|\n",
      "|    barrio Les Corts|        2|     es|                    NULL|                 flat|     202|          Les Corts|    true|         BCN25173| NULL| false|    false|   true|  false|     false|   false|41.3864784|2.1317448|   Barcelona|           Les Corts|         false|                  NULL|       30|     sale|                        NULL|                                      NULL|                          NULL|1095500.0|    10740.0|    90996277|        flat|Barcelona|    3|      false|102.0|  good|   Les Corts, Barcelona|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_04_05|\n",
      "|    barrio Les Corts|        2|     es|                    NULL|                 flat|     483|          Les Corts|    true|         BCN27268| NULL| false|    false|   true|  false|     false|   false|  41.38907|2.1307946|   Barcelona|           Les Corts|         false|                  NULL|       31|     sale|                        NULL|                                      NULL|                          NULL|1252000.0|     9631.0|    92647696|        flat|Barcelona|    3|      false|130.0|  good|   Les Corts, Barcelona|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_04_05|\n",
      "|    barrio Les Corts|        2|     es|                    NULL|                 flat|     766|          Les Corts|   false|            GV615|    4| false|    false|   true|   true|     false|    true|  41.39007|2.1314946|   Barcelona|           Les Corts|         false|                  NULL|       16|     sale|                        NULL|                                      NULL|                          NULL| 862000.0|    10512.0|    92132103|        flat|Barcelona|    2|      false| 82.0|  good|   Les Corts, Barcelona|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_02_10|\n",
      "|    barrio Les Corts|        2|     es|                    NULL|                 flat|     834|          Les Corts|    true|         BCN27268| NULL| false|    false|   true|  false|     false|   false|  41.38907|2.1307946|   Barcelona|           Les Corts|         false|                  NULL|       31|     sale|                        NULL|                                      NULL|                          NULL|1252000.0|     9631.0|    92647696|        flat|Barcelona|    3|      false|130.0|  good|   Les Corts, Barcelona|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_02_10|\n",
      "|barrio Les Tres T...|        3|     es|           terracedHouse|               chalet|     677|Sarrià-Sant Gervasi|   false|            VPV06| NULL| false|    false|   NULL|  false|     false|   false|41.3970757|2.1344179|   Barcelona|     Les Tres Torres|         false|                  NULL|       30|     sale|                        true|                                      true|                          NULL|2750000.0|     9821.0|    91965832|      chalet|Barcelona|    3|      false|280.0|  good|   Les Tres Torres, ...|      Chalet adosado|https://img3.idea...|            false|https://www.ideal...|2020_02_10|\n",
      "|barrio Les Tres T...|        3|     es|                    NULL|               chalet|     577|Sarrià-Sant Gervasi|   false|         BCN11347| NULL| false|    false|   NULL|   true|     false|   false| 41.396468|2.1333346|   Barcelona|     Les Tres Torres|         false|                  NULL|       45|     sale|                        true|                                      true|                          NULL|2750000.0|    12277.0|    81345140|      chalet|Barcelona|    2|      false|224.0|  good|   Les Tres Torres, ...|              Chalet|https://img3.idea...|            false|https://www.ideal...|2020_02_10|\n",
      "|Calle de Can Segalar|        2|     es|                    NULL|                 flat|     675|          Les Corts|    true|           019116|    4| false|    false|   true|  false|     false|   false|41.3874037|2.1334023|   Barcelona|           Les Corts|         false|                  NULL|       26|     sale|                        NULL|                                      NULL|                          NULL| 862000.0|    10512.0|    40123363|        flat|Barcelona|    2|      false| 82.0|  good|   Les Corts, Barcelona|Piso en Calle de ...|https://img3.idea...|            false|https://www.ideal...|2020_08_15|\n",
      "|    barrio Les Corts|        2|     es|                    NULL|                 flat|     396|          Les Corts|    true|         BCN27268| NULL| false|    false|   true|  false|     false|   false|  41.38907|2.1307946|   Barcelona|           Les Corts|         false|                  NULL|       31|     sale|                        NULL|                                      NULL|                          NULL|1252000.0|     9631.0|    92647696|        flat|Barcelona|    3|      false|130.0|  good|   Les Corts, Barcelona|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_08_15|\n",
      "|Calle de Can Segalar|        2|     es|                    NULL|                 flat|     446|          Les Corts|    true|           019258|    6| false|    false|   true|   true|     false|   false|41.3889708|2.1313964|   Barcelona|           Les Corts|         false|                  NULL|       30|     sale|                        NULL|                                      NULL|                          NULL|1454000.0|    12017.0|    88551642|        flat|Barcelona|    3|      false|121.0|  good|   Les Corts, Barcelona|Piso en Calle de ...|https://img3.idea...|            false|https://www.ideal...|2020_08_15|\n",
      "|Calle de Can Segalar|        2|     es|                    NULL|                 flat|     707|          Les Corts|    true|           019107|    5| false|    false|   true|   true|     false|   false|41.3871037|2.1336023|   Barcelona|           Les Corts|         false|                  NULL|       29|     sale|                        NULL|                                      NULL|                          NULL|1082000.0|    11389.0|    39916328|        flat|Barcelona|    3|      false| 95.0|  good|   Les Corts, Barcelona|Piso en Calle de ...|https://img3.idea...|            false|https://www.ideal...|2020_08_15|\n",
      "|Calle de Can Segalar|        2|     es|                    NULL|                 flat|     654|          Les Corts|   false|            GV614|    6| false|    false|   true|   true|     false|    true|  41.38687|2.1325946|   Barcelona|           Les Corts|         false|                  NULL|       15|     sale|                        NULL|                                      NULL|                          NULL|1454000.0|    12017.0|    92056660|        flat|Barcelona|    3|      false|121.0|  good|   Les Corts, Barcelona|Piso en Calle de ...|https://img3.idea...|            false|https://www.ideal...|2020_08_15|\n",
      "|    barrio Les Corts|        2|     es|                    NULL|                 flat|     630|          Les Corts|    true|         BCN25173| NULL| false|    false|   true|  false|     false|   false|41.3864784|2.1317448|   Barcelona|           Les Corts|         false|                  NULL|       30|     sale|                        NULL|                                      NULL|                          NULL|1095500.0|    10740.0|    90996277|        flat|Barcelona|    3|      false|102.0|  good|   Les Corts, Barcelona|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_08_15|\n",
      "|    barrio Les Corts|        2|     es|                    NULL|                 flat|     423|          Les Corts|   false|            GV615|    4| false|    false|   true|   true|     false|    true|  41.39007|2.1314946|   Barcelona|           Les Corts|         false|                  NULL|       16|     sale|                        NULL|                                      NULL|                          NULL| 862000.0|    10512.0|    92132103|        flat|Barcelona|    2|      false| 82.0|  good|   Les Corts, Barcelona|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_08_15|\n",
      "|    barrio Les Corts|        2|     es|                    NULL|                 flat|     537|          Les Corts|    true|         BCN25172| NULL| false|    false|   true|  false|     false|   false|41.3866784|2.1303448|   Barcelona|           Les Corts|         false|                  NULL|       30|     sale|                        NULL|                                      NULL|                          NULL| 978000.0|    11241.0|    90996261|        flat|Barcelona|    3|      false| 87.0|  good|   Les Corts, Barcelona|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_08_15|\n",
      "|barrio La Dreta d...|        3|     es|                    NULL|                 flat|     747|           Eixample|    true|         BCN19869| NULL| false|    false|   true|   true|     false|   false|41.3948533|2.1578979|   Barcelona|La Dreta de l'Eix...|         false|                  NULL|       24|     sale|                        NULL|                                      NULL|                           NaN|1850000.0|    10632.0|    91178037|        flat|Barcelona|    3|      false|174.0|  good|   La Dreta de l'Eix...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_06_03|\n",
      "+--------------------+---------+-------+------------------------+---------------------+--------+-------------------+--------+-----------------+-----+------+---------+-------+-------+----------+--------+----------+---------+------------+--------------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+---------+-----------+------------+------------+---------+-----+-----------+-----+------+-----------------------+--------------------+--------------------+-----------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:47:24 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "24/05/15 18:47:31 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: rooms\n",
      "Q1: 3.0\n",
      "Q3: 4.0\n",
      "IQR: 1.0\n",
      "Lower Bound: 0.0\n",
      "Upper Bound: 7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:47:36 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "24/05/15 18:47:42 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 144\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:47:47 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "24/05/15 18:47:54 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+------------------------+---------------------+--------+--------------------+--------+-----------------+-----+------+---------+-------+-------+----------+--------+----------+---------+--------------------+--------------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+---------+-----------+------------+------------+---------+-----+-----------+------+------+-----------------------+--------------------+--------------------+-----------------+--------------------+----------+\n",
      "|             address|bathrooms|country|detailedtype_subTypology|detailedtype_typology|distance|            district|exterior|externalreference|floor|has360|has3dtour|haslift|hasplan|hasstaging|hasvideo|  latitude|longitude|        municipality|        neighborhood|newdevelopment|newdevelopmentfinished|numphotos|operation|parkingspace_hasParkingSpace|parkingspace_isParkingSpaceIncludedInPrice|parkingspace_parkingSpacePrice|    price|pricebyarea|propertycode|propertytype| province|rooms|showaddress|  size|status|suggestedtexts_subtitle|suggestedtexts_title|           thumbnail|topnewdevelopment|                 url|      date|\n",
      "+--------------------+---------+-------+------------------------+---------------------+--------+--------------------+--------+-----------------+-----+------+---------+-------+-------+----------+--------+----------+---------+--------------------+--------------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+---------+-----------+------------+------------+---------+-----+-----------+------+------+-----------------------+--------------------+--------------------+-----------------+--------------------+----------+\n",
      "|    barrio Pedralbes|        6|     es|                  duplex|                 flat|     783|           Les Corts|    true|             1204|    5| false|    false|   true|   true|     false|   false|41.3923064| 2.117372|           Barcelona|           Pedralbes|         false|                  NULL|        9|     sale|                        NULL|                                      NULL|                          NULL|2500000.0|     5593.0|    91797141|      duplex|Barcelona|    8|      false| 447.0| renew|   Pedralbes, Barcelona|              Dúplex|https://img3.idea...|            false|https://www.ideal...|2020_08_15|\n",
      "|barrio Vila de Gr...|        4|     es|                    NULL|                 flat|      61|              Gràcia|    true|         BCN26639| NULL| false|    false|   true|   true|     false|   false|41.4008588| 2.155351|           Barcelona|      Vila de Gràcia|         false|                  NULL|       28|     sale|                        NULL|                                      NULL|                           NaN|1280000.0|     4183.0|    92480222|        flat|Barcelona|    8|      false| 306.0| renew|   Vila de Gràcia, B...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_06_03|\n",
      "|           Can Matas|        7|     es|                    NULL|               chalet|    2153|           Can Matas|   false|     VSC2003002-5| NULL| false|    false|   NULL|   true|     false|    true|41.4718127|2.0635986|Sant Cugat del Va...|                NULL|         false|                  NULL|       32|     sale|                        true|                                      true|                           NaN|1990000.0|     2535.0|    91784793|      chalet|Barcelona|    9|      false| 785.0|  good|   Can Matas, Sant C...|              Chalet|https://img3.idea...|            false|https://www.ideal...|2020_08_29|\n",
      "|Avenida del Tibidabo|        8|     es|                    NULL|               chalet|     800| Sarrià-Sant Gervasi|   false|           BCN648|   bj| false|    false|   NULL|   true|     false|    true|41.4137088|2.1320765|           Barcelona|Sant Gervasi - La...|         false|                  NULL|       48|     sale|                        true|                                      true|                          NULL|5600000.0|     7245.0|    89131054|      chalet|Barcelona|   11|      false| 773.0|  good|   Sant Gervasi - La...|Chalet en Avenida...|https://img3.idea...|            false|https://www.ideal...|2020_05_29|\n",
      "|       barrio Sarrià|        6|     es|                    NULL|               chalet|     297| Sarrià-Sant Gervasi|   false|      VB1810042-1| NULL| false|    false|   NULL|   true|     false|    true|41.4023037|2.1226093|           Barcelona|              Sarrià|         false|                  NULL|       42|     sale|                        true|                                      true|                           NaN|3900000.0|     7156.0|    89228498|      chalet|Barcelona|    9|      false| 545.0|  good|      Sarrià, Barcelona|              Chalet|https://img3.idea...|            false|https://www.ideal...|2020_12_12|\n",
      "|barrio Sant Gerva...|        3|     es|                    NULL|                 flat|     858| Sarrià-Sant Gervasi|    true|          BCN8212| NULL| false|    false|   true|  false|     false|   false|41.3950225|2.1437586|           Barcelona|Sant Gervasi - Ga...|         false|                  NULL|       25|     sale|                        NULL|                                      NULL|                          NULL|1950000.0|     5891.0|    38615912|        flat|Barcelona|   10|      false| 331.0| renew|   Sant Gervasi - Ga...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_02_07|\n",
      "|          rambla s/n|        2|     es|        independantHouse|               chalet|    1467|Centre - Casco An...|   false|              397| NULL| false|    false|   NULL|   true|     false|    true|41.3474579|2.0418653|Sant Boi de Llobr...|                NULL|         false|                  NULL|       59|     sale|                        true|                                      true|                           NaN| 404000.0|     1154.0|    91573302|      chalet|Barcelona|    8|       true| 350.0|  good|   Centre - Casco An...|Casa independient...|https://img3.idea...|            false|https://www.ideal...|2020_03_05|\n",
      "|barrio Vila de Gr...|        4|     es|                    NULL|                 flat|     642|              Gràcia|    true|         BCN26639| NULL| false|    false|   true|   true|     false|   false|41.4008588| 2.155351|           Barcelona|      Vila de Gràcia|         false|                  NULL|       28|     sale|                        NULL|                                      NULL|                           NaN|1280000.0|     4183.0|    92480222|        flat|Barcelona|    8|      false| 306.0| renew|   Vila de Gràcia, B...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_09_20|\n",
      "|barrio Vila de Gr...|        4|     es|                    NULL|                 flat|     560|              Gràcia|    true|         BCN26639| NULL| false|    false|   true|   true|     false|   false|41.4008588| 2.155351|           Barcelona|      Vila de Gràcia|         false|                  NULL|       28|     sale|                        NULL|                                      NULL|                           NaN|1280000.0|     4183.0|    92480222|        flat|Barcelona|    8|      false| 306.0| renew|   Vila de Gràcia, B...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_08_07|\n",
      "|Camps Blancs - Ca...|        3|     es|                    NULL|               chalet|    2920|Camps Blancs - Ca...|   false|          01657AM|   bj| false|    false|   NULL|  false|     false|   false| 41.342763|2.0183804|Sant Boi de Llobr...|                NULL|         false|                  NULL|       48|     sale|                        true|                                      true|                           NaN| 464000.0|      777.0|    92528193|      chalet|Barcelona|   11|      false| 597.0|  good|   Camps Blancs - Ca...|              Chalet|https://img3.idea...|            false|https://www.ideal...|2020_05_08|\n",
      "|          rambla s/n|        2|     es|        independantHouse|               chalet|    1467|Centre - Casco An...|   false|      CASA ST BOI| NULL| false|    false|   NULL|   true|     false|    true|41.3474579|2.0418653|Sant Boi de Llobr...|                NULL|         false|                  NULL|       59|     sale|                        true|                                      true|                           NaN| 445000.0|     1271.0|    91573302|      chalet|Barcelona|    8|       true| 350.0|  good|   Centre - Casco An...|Casa independient...|https://img3.idea...|            false|https://www.ideal...|2020_05_08|\n",
      "|Calle de Sant Ant...|        4|     es|                    NULL|                 flat|     337|        Ciutat Vella|    true|         SE-40665|    2| false|    false|   true|   true|     false|    true|41.3779211|2.1648076|           Barcelona|            El Raval|         false|                  NULL|       33|     sale|                        NULL|                                      NULL|                           NaN| 880000.0|     4757.0|    91627999|        flat|Barcelona|    8|      false| 185.0|  good|    El Raval, Barcelona|Piso en Calle de ...|https://img3.idea...|            false|https://www.ideal...|2020_07_19|\n",
      "|Avenida del Tibidabo|        8|     es|                    NULL|               chalet|    2521| Sarrià-Sant Gervasi|   false|           BCN648|   bj| false|    false|   NULL|   true|     false|    true|41.4137088|2.1320765|           Barcelona|Sant Gervasi - La...|         false|                  NULL|       48|     sale|                        true|                                      true|                           NaN|5600000.0|     7245.0|    89131054|      chalet|Barcelona|   11|      false| 773.0|  good|   Sant Gervasi - La...|Chalet en Avenida...|https://img3.idea...|            false|https://www.ideal...|2020_07_29|\n",
      "|          Valldoreix|       19|     es|        independantHouse|               chalet|    2456|          Valldoreix|   false|            5015B| NULL| false|    false|   NULL|  false|     false|   false|  41.45227|  2.05002|Sant Cugat del Va...|                NULL|         false|                  NULL|       12|     sale|                        NULL|                                      NULL|                           NaN|4500000.0|     3404.0|    92916712|      chalet|Barcelona|   14|      false|1322.0|  good|   Valldoreix, Sant ...|  Casa independiente|https://img3.idea...|            false|https://www.ideal...|2020_08_30|\n",
      "|  Passatge Vall d'Or|        5|     es|                    NULL|               chalet|    1853|          Valldoreix|   false|     VSC2008005-3| NULL| false|    false|   NULL|  false|     false|    true|41.4568386|2.0567838|Sant Cugat del Va...|                NULL|         false|                  NULL|       27|     sale|                        true|                                      true|                           NaN| 955000.0|      287.0|    91521602|      chalet|Barcelona|    9|      false|3330.0|  good|   Valldoreix, Sant ...|Chalet en Passatg...|https://img3.idea...|            false|https://www.ideal...|2020_08_30|\n",
      "|          Valldoreix|        3|     es|        independantHouse|               chalet|    2371|          Valldoreix|   false|         W-0275DQ| NULL| false|    false|   NULL|   true|     false|   false|41.4525254| 2.050987|Sant Cugat del Va...|                NULL|         false|                  NULL|       50|     sale|                        true|                                      true|                           NaN| 790000.0|     1955.0|    85640557|      chalet|Barcelona|    8|      false| 404.0|  good|   Valldoreix, Sant ...|  Casa independiente|https://img3.idea...|            false|https://www.ideal...|2020_08_30|\n",
      "|          Valldoreix|        5|     es|                    NULL|               chalet|    1949|          Valldoreix|   false|     VSC2002010-5| NULL| false|    false|   NULL|  false|     false|    true|41.4544206|2.0557409|Sant Cugat del Va...|                NULL|         false|                  NULL|       54|     sale|                        true|                                      true|                           NaN|1590750.0|     2678.0|    91784790|      chalet|Barcelona|    9|      false| 594.0|  good|   Valldoreix, Sant ...|              Chalet|https://img3.idea...|            false|https://www.ideal...|2020_08_30|\n",
      "|        Paseo Calado|        3|     es|                    NULL|               chalet|    1918|La Floresta - Les...|   false|           013753|   bj| false|    false|   NULL|  false|     false|   false| 41.464632|2.0991448|Sant Cugat del Va...|                NULL|         false|                  NULL|       37|     sale|                        NULL|                                      NULL|                           NaN|2400000.0|     3409.0|    87942444|      chalet|Barcelona|    8|      false| 704.0| renew|   La Floresta - Les...|Chalet en Paseo C...|https://img3.idea...|            false|https://www.ideal...|2020_08_30|\n",
      "|                Golf|       10|     es|                castillo|         countryHouse|     841|                Golf|   false|               S5| NULL| false|    false|   NULL|  false|     false|   false|41.4590692|2.0883896|Sant Cugat del Va...|                NULL|         false|                  NULL|       30|     sale|                        NULL|                                      NULL|                           NaN|3500000.0|     1750.0|    90723562|countryHouse|Barcelona|   10|      false|2000.0|  good|   Golf, Sant Cugat ...|            Castillo|https://img3.idea...|            false|https://www.ideal...|2020_08_30|\n",
      "|           Can Matas|        7|     es|                    NULL|               chalet|    2153|           Can Matas|   false|     VSC2003002-5| NULL| false|    false|   NULL|   true|     false|    true|41.4718127|2.0635986|Sant Cugat del Va...|                NULL|         false|                  NULL|       32|     sale|                        true|                                      true|                           NaN|1990000.0|     2535.0|    91784793|      chalet|Barcelona|    9|      false| 785.0|  good|   Can Matas, Sant C...|              Chalet|https://img3.idea...|            false|https://www.ideal...|2020_08_30|\n",
      "+--------------------+---------+-------+------------------------+---------------------+--------+--------------------+--------+-----------------+-----+------+---------+-------+-------+----------+--------+----------+---------+--------------------+--------------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+---------+-----------+------------+------------+---------+-----+-----------+------+------+-----------------------+--------------------+--------------------+-----------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:47:58 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "24/05/15 18:48:06 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: size\n",
      "Q1: 67.0\n",
      "Q3: 126.0\n",
      "IQR: 59.0\n",
      "Lower Bound: -110.0\n",
      "Upper Bound: 303.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:48:10 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "24/05/15 18:48:17 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 1454\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:48:22 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "24/05/15 18:48:29 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+------------------------+---------------------+--------+--------------------+--------+-----------------+-----+------+---------+-------+-------+----------+--------+----------+---------+--------------------+--------------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+---------+-----------+------------+------------+---------+-----+-----------+--------+------+-----------------------+--------------------+--------------------+-----------------+--------------------+----------+\n",
      "|             address|bathrooms|country|detailedtype_subTypology|detailedtype_typology|distance|            district|exterior|externalreference|floor|has360|has3dtour|haslift|hasplan|hasstaging|hasvideo|  latitude|longitude|        municipality|        neighborhood|newdevelopment|newdevelopmentfinished|numphotos|operation|parkingspace_hasParkingSpace|parkingspace_isParkingSpaceIncludedInPrice|parkingspace_parkingSpacePrice|    price|pricebyarea|propertycode|propertytype| province|rooms|showaddress|    size|status|suggestedtexts_subtitle|suggestedtexts_title|           thumbnail|topnewdevelopment|                 url|      date|\n",
      "+--------------------+---------+-------+------------------------+---------------------+--------+--------------------+--------+-----------------+-----+------+---------+-------+-------+----------+--------+----------+---------+--------------------+--------------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+---------+-----------+------------+------------+---------+-----+-----------+--------+------+-----------------------+--------------------+--------------------+-----------------+--------------------+----------+\n",
      "|Centre - Casco An...|        4|     es|           terracedHouse|               chalet|    2995|Centre - Casco An...|   false|            A2619| NULL| false|    false|   NULL|  false|     false|   false|41.3424491|2.0405794|Sant Boi de Llobr...|                NULL|         false|                  NULL|       30|     sale|                        true|                                      true|                           NaN|1030000.0|     1157.0|    87864853|      chalet|Barcelona|    5|      false|   890.0|  good|   Centre - Casco An...|      Chalet adosado|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|Barri Antic - Centre|        2|     es|           terracedHouse|               chalet|    3091|Barri Antic - Centre|   false|      Nova0018899|   bj| false|    false|   NULL|  false|     false|   false|41.3161268|2.0155012|          Viladecans|                NULL|         false|                  NULL|       33|     sale|                        true|                                      true|                           NaN| 608000.0|      993.0|    90837209|      chalet|Barcelona|    5|      false|   612.0|  good|   Barri Antic - Cen...|      Chalet adosado|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|Centre - Casco An...|        4|     es|       semidetachedHouse|               chalet|    3235|Centre - Casco An...|   false|         REF17C02| NULL| false|    false|   NULL|  false|     false|   false| 41.343926|2.0376823|Sant Boi de Llobr...|                NULL|         false|                  NULL|       74|     sale|                        true|                                      true|                           NaN|1030000.0|     1288.0|    83045510|      chalet|Barcelona|    7|      false|   800.0|  good|   Centre - Casco An...|      Chalet pareado|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|  Plaça de Catalunya|        2|     es|                    NULL|                 flat|    2907|  Plaça de Catalunya|    true|             3408|   bj| false|    false|   true|  false|     false|   false|41.3196421|2.0871466|El Prat de Llobregat|                NULL|         false|                  NULL|       30|     sale|                        true|                                      true|                           NaN| 190000.0|        1.0|    92668915|        flat|Barcelona|    2|      false|144000.0|  good|   Plaça de Cataluny...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|Barri Antic - Centre|        2|     es|        independantHouse|               chalet|    3027|Barri Antic - Centre|   false|           2697-1| NULL|  true|    false|   NULL|   true|     false|   false|41.3139692| 2.016479|          Viladecans|                NULL|         false|                  NULL|       22|     sale|                        NULL|                                      NULL|                           NaN| 499000.0|     1386.0|    91747407|      chalet|Barcelona|    4|      false|   360.0|  good|   Barri Antic - Cen...|  Casa independiente|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|Plaça Catalunya -...|        4|     es|           terracedHouse|               chalet|    2892|Plaça Catalunya -...|   false|             2331| NULL| false|    false|   NULL|  false|     false|   false|41.3420164|2.0428542|Sant Boi de Llobr...|                NULL|         false|                  NULL|       35|     sale|                        true|                                      true|                           NaN|1030000.0|     1144.0|    38095571|      chalet|Barcelona|    5|      false|   900.0|  good|   Plaça Catalunya -...|      Chalet adosado|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|Centre - Casco An...|        4|     es|                    NULL|               chalet|    3051|Centre - Casco An...|   false|           158924|   bj| false|    false|   NULL|  false|     false|   false|41.3431801| 2.041406|Sant Boi de Llobr...|                NULL|         false|                  NULL|       25|     sale|                        NULL|                                      NULL|                           NaN|1030000.0|     1653.0|    33477641|      chalet|Barcelona|    5|      false|   623.0|  good|   Centre - Casco An...|              Chalet|https://img3.idea...|            false|https://www.ideal...|2020_10_16|\n",
      "|     Calle d'Alcolea|        3|     es|        independantHouse|               chalet|     744|      Sants-Montjuïc|   false|    CB_BCN_0297_V| NULL| false|    false|   NULL|   true|     false|    true|41.3785656|2.1343299|           Barcelona|               Sants|         false|                  NULL|       39|     sale|                        NULL|                                      NULL|                          NULL|1450000.0|     3021.0|    90204735|      chalet|Barcelona|    4|      false|   480.0|  good|       Sants, Barcelona|Casa independient...|https://img3.idea...|            false|https://www.ideal...|2020_04_05|\n",
      "|    barrio Pedralbes|        6|     es|               penthouse|                 flat|     660|           Les Corts|    true|         BCN26015| NULL| false|    false|   true|   true|     false|   false| 41.394721| 2.118681|           Barcelona|           Pedralbes|         false|                  NULL|       59|     sale|                        true|                                      true|                          NULL|3500000.0|     8621.0|    92176342|   penthouse|Barcelona|    5|      false|   406.0|  good|   Pedralbes, Barcelona|               Ático|https://img3.idea...|            false|https://www.ideal...|2020_02_10|\n",
      "|barrio Sant Gerva...|        5|     es|                    NULL|                 flat|     848| Sarrià-Sant Gervasi|    true|         BCN23066| NULL| false|    false|   true|  false|     false|   false|41.3964254|2.1366062|           Barcelona|Sant Gervasi - Ga...|         false|                  NULL|       35|     sale|                        true|                                      true|                          NULL|2200000.0|     6667.0|    89237439|        flat|Barcelona|    5|      false|   330.0|  good|   Sant Gervasi - Ga...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_02_10|\n",
      "|Calle dels Cavallers|        6|     es|                  duplex|                 flat|     855|           Les Corts|    true|          D-40637|    5| false|    false|   true|  false|     false|    true| 41.393521| 2.116681|           Barcelona|           Pedralbes|         false|                  NULL|       54|     sale|                        true|                                      true|                          NULL|3500000.0|     7056.0|    91211684|      duplex|Barcelona|    6|      false|   496.0|  good|   Pedralbes, Barcelona|Dúplex en Calle d...|https://img3.idea...|            false|https://www.ideal...|2020_02_10|\n",
      "|       barrio Sarrià|        6|     es|                    NULL|               chalet|     619| Sarrià-Sant Gervasi|   false|         BCN14958| NULL| false|    false|   NULL|  false|     false|   false|41.4005037|2.1224093|           Barcelona|              Sarrià|         false|                  NULL|       35|     sale|                        true|                                      true|                          NULL|3900000.0|     7617.0|    88376448|      chalet|Barcelona|    6|      false|   512.0| renew|      Sarrià, Barcelona|              Chalet|https://img3.idea...|            false|https://www.ideal...|2020_02_10|\n",
      "|Calle de Sol i Pa...|        3|     es|                    NULL|                 flat|     696| Sarrià-Sant Gervasi|    true|PRI-SOLIPADRIS001|    1| false|     true|   true|   true|     false|    true|41.4004686| 2.120838|           Barcelona|              Sarrià|         false|                  NULL|       54|     sale|                        true|                                      true|                          NULL|1180000.0|     3565.0|    91299611|        flat|Barcelona|    5|      false|   331.0|  good|      Sarrià, Barcelona|Piso en Calle de ...|https://img3.idea...|            false|https://www.ideal...|2020_02_10|\n",
      "|barrio Les Tres T...|        5|     es|                    NULL|                 flat|     648| Sarrià-Sant Gervasi|    true|         BCN24619| NULL| false|    false|   true|   true|     false|   false| 41.401307|2.1291549|           Barcelona|     Les Tres Torres|         false|                  NULL|       43|     sale|                        true|                                      true|                          NULL|2300000.0|     6553.0|    90571031|        flat|Barcelona|    6|      false|   351.0| renew|   Les Tres Torres, ...|                Piso|https://img3.idea...|            false|https://www.ideal...|2020_02_10|\n",
      "|       Angel Guimerá|        5|     es|        independantHouse|               chalet|     655| Sarrià-Sant Gervasi|   false|           C.A.G.| NULL| false|    false|   NULL|   true|     false|    true|41.3971669|2.1341217|           Barcelona|     Les Tres Torres|         false|                  NULL|       39|     sale|                        NULL|                                      NULL|                          NULL|2490000.0|     7324.0|    93086874|      chalet|Barcelona|    5|      false|   340.0|  good|   Les Tres Torres, ...|Casa independient...|https://img3.idea...|            false|https://www.ideal...|2020_02_10|\n",
      "|    barrio Pedralbes|        6|     es|               penthouse|                 flat|     798|           Les Corts|    true|         BCN26015| NULL| false|    false|   true|   true|     false|   false| 41.394721| 2.118681|           Barcelona|           Pedralbes|         false|                  NULL|       59|     sale|                        true|                                      true|                          NULL|3500000.0|     8621.0|    92176342|   penthouse|Barcelona|    5|      false|   406.0|  good|   Pedralbes, Barcelona|               Ático|https://img3.idea...|            false|https://www.ideal...|2020_08_15|\n",
      "|     Calle de Tòquio|        5|     es|                    NULL|                 flat|     698|           Les Corts|    true|           V-2061|    4| false|    false|   true|  false|     false|   false|41.3892084|2.1182765|           Barcelona|           Pedralbes|         false|                  NULL|       27|     sale|                        true|                                      true|                          NULL|1800000.0|     5000.0|    89747920|        flat|Barcelona|    6|      false|   360.0| renew|   Pedralbes, Barcelona|Piso en Calle de ...|https://img3.idea...|            false|https://www.ideal...|2020_08_15|\n",
      "|    barrio Pedralbes|        6|     es|                  duplex|                 flat|     783|           Les Corts|    true|             1204|    5| false|    false|   true|   true|     false|   false|41.3923064| 2.117372|           Barcelona|           Pedralbes|         false|                  NULL|        9|     sale|                        NULL|                                      NULL|                          NULL|2500000.0|     5593.0|    91797141|      duplex|Barcelona|    8|      false|   447.0| renew|   Pedralbes, Barcelona|              Dúplex|https://img3.idea...|            false|https://www.ideal...|2020_08_15|\n",
      "|Calle del Torrent...|        3|     es|               penthouse|                 flat|     694|              Gràcia|    true|        VB2103006|    3| false|    false|   true|   true|     false|    true| 41.406338|2.1501168|           Barcelona|      Vila de Gràcia|         false|                  NULL|       56|     sale|                        NULL|                                      NULL|                           NaN|1100000.0|     2750.0|    93132862|   penthouse|Barcelona|    4|      false|   400.0| renew|   Vila de Gràcia, B...|Ático en Calle de...|https://img3.idea...|            false|https://www.ideal...|2020_06_03|\n",
      "|barrio La Dreta d...|        4|     es|               penthouse|                 flat|     675|            Eixample|   false|         W-02KD38| NULL| false|    false|   true|   true|     false|   false|41.3959627|2.1589645|           Barcelona|La Dreta de l'Eix...|         false|                  NULL|       38|     sale|                        true|                                      true|                           NaN|    1.2E7|    17143.0|    92232387|   penthouse|Barcelona|    4|      false|   700.0|  good|   La Dreta de l'Eix...|               Ático|https://img3.idea...|            false|https://www.ideal...|2020_06_03|\n",
      "+--------------------+---------+-------+------------------------+---------------------+--------+--------------------+--------+-----------------+-----+------+---------+-------+-------+----------+--------+----------+---------+--------------------+--------------------+--------------+----------------------+---------+---------+----------------------------+------------------------------------------+------------------------------+---------+-----------+------------+------------+---------+-----+-----------+--------+------+-----------------------+--------------------+--------------------+-----------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "idealista_df = iqr_outlier_treatment(idealista_df, continuous_variables, factor=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a lot of columns removing univariate outliers is premature at this state, we leave it for later stages of the data management backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 18:50:42 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "24/05/15 18:50:50 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "idealista_df.write.format(\"mongodb\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"database\", \"bdm\") \\\n",
    "    .option(\"collection\", \"idealista\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Air quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "airquality_dir = \"../data/persistent-landing-zone/opendatabcn/airquality\"\n",
    "# get files\n",
    "files = os.listdir(airquality_dir)\n",
    "# keep only those that start with a year (4 digits)\n",
    "files = [file for file in files if file[:4].isdigit()]\n",
    "months = [file[:7] for file in files]\n",
    "# add the path to the files\n",
    "files = [f\"{airquality_dir}/{file}\" for file in files]\n",
    "# create a list of dataframes\n",
    "airquality_dfs = {}\n",
    "for file, month in zip(files, months):\n",
    "    # read the file\n",
    "    df = spark.read.format(\"avro\").load(file)\n",
    "    # append the dataframe to the list\n",
    "    airquality_dfs[month] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(airquality_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nom_cabina: string (nullable = true)\n",
      " |-- qualitat_aire: string (nullable = true)\n",
      " |-- codi_dtes: string (nullable = true)\n",
      " |-- zqa: integer (nullable = true)\n",
      " |-- codi_eoi: integer (nullable = true)\n",
      " |-- longitud: double (nullable = true)\n",
      " |-- latitud: double (nullable = true)\n",
      " |-- hora_o3: string (nullable = true)\n",
      " |-- qualitat_o3: string (nullable = true)\n",
      " |-- valor_o3: string (nullable = true)\n",
      " |-- hora_no2: string (nullable = true)\n",
      " |-- qualitat_no2: string (nullable = true)\n",
      " |-- valor_no2: string (nullable = true)\n",
      " |-- hora_pm10: string (nullable = true)\n",
      " |-- qualitat_pm10: string (nullable = true)\n",
      " |-- valor_pm10: string (nullable = true)\n",
      " |-- generat: string (nullable = true)\n",
      " |-- dateTime: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airquality_dfs['2018_06'].printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change all column names to lowercase\n",
    "for month in airquality_dfs:\n",
    "    df = airquality_dfs[month]\n",
    "    for col in df.columns:\n",
    "        df = df.withColumnRenamed(col, col.lower())\n",
    "    airquality_dfs[month] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2018_06 has 18 columns\n",
      "Month 2018_07 has 18 columns\n",
      "Month 2018_08 has 18 columns\n",
      "Month 2018_09 has 18 columns\n",
      "Month 2018_10 has 18 columns\n",
      "Month 2018_11 has 18 columns\n",
      "Month 2018_12 has 18 columns\n",
      "Month 2019_01 has 18 columns\n",
      "Month 2019_02 has 18 columns\n",
      "Month 2019_03 has 18 columns\n",
      "Month 2019_04 has 57 columns\n",
      "Month 2019_05 has 57 columns\n",
      "Month 2019_06 has 1 columns\n",
      "Month 2019_07 has 1 columns\n",
      "Month 2019_08 has 57 columns\n",
      "Month 2019_09 has 57 columns\n",
      "Month 2019_10 has 57 columns\n",
      "Month 2019_11 has 57 columns\n",
      "Month 2019_12 has 57 columns\n",
      "Month 2020_01 has 57 columns\n",
      "Month 2020_02 has 57 columns\n",
      "Month 2020_03 has 57 columns\n",
      "Month 2020_04 has 57 columns\n",
      "Month 2020_05 has 57 columns\n",
      "Month 2020_06 has 57 columns\n",
      "Month 2020_07 has 57 columns\n",
      "Month 2020_08 has 57 columns\n",
      "Month 2020_09 has 57 columns\n",
      "Month 2020_10 has 57 columns\n",
      "Month 2020_11 has 57 columns\n",
      "Month 2020_12 has 57 columns\n",
      "Month 2021_01 has 57 columns\n",
      "Month 2021_02 has 57 columns\n",
      "Month 2021_03 has 57 columns\n",
      "Month 2021_04 has 57 columns\n",
      "Month 2021_05 has 57 columns\n",
      "Month 2021_06 has 57 columns\n",
      "Month 2021_07 has 57 columns\n",
      "Month 2021_08 has 57 columns\n",
      "Month 2021_09 has 57 columns\n",
      "Month 2021_10 has 57 columns\n",
      "Month 2021_11 has 57 columns\n",
      "Month 2021_12 has 57 columns\n",
      "Month 2022_01 has 57 columns\n",
      "Month 2022_02 has 57 columns\n",
      "Month 2022_03 has 57 columns\n",
      "Month 2022_04 has 57 columns\n",
      "Month 2022_05 has 57 columns\n",
      "Month 2022_06 has 57 columns\n",
      "Month 2022_07 has 57 columns\n",
      "Month 2022_08 has 57 columns\n",
      "Month 2022_09 has 57 columns\n",
      "Month 2022_10 has 57 columns\n",
      "Month 2022_11 has 57 columns\n",
      "Month 2022_12 has 57 columns\n",
      "Month 2023_01 has 57 columns\n",
      "Month 2023_02 has 57 columns\n",
      "Month 2023_03 has 57 columns\n",
      "Month 2023_04 has 57 columns\n",
      "Month 2023_05 has 57 columns\n",
      "Month 2023_06 has 57 columns\n",
      "Month 2023_07 has 57 columns\n",
      "Month 2023_08 has 57 columns\n",
      "Month 2023_09 has 57 columns\n",
      "Month 2023_10 has 57 columns\n",
      "Month 2023_11 has 57 columns\n",
      "Month 2023_12 has 57 columns\n",
      "Month 2024_01 has 57 columns\n",
      "Month 2024_02 has 57 columns\n"
     ]
    }
   ],
   "source": [
    "print_by_month = {}\n",
    "for month in airquality_dfs:\n",
    "    # print len of columns\n",
    "    print_by_month[month] = len(airquality_dfs[month].columns)\n",
    "\n",
    "# order by key\n",
    "print_by_month = dict(sorted(print_by_month.items(), key=lambda item: item[0]))\n",
    "for month in print_by_month:\n",
    "    print(f\"Month {month} has {print_by_month[month]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+---------+---+--------+--------+-------+-------+-----------+----------+--------+------------+----------+---------+-------------+----------+---------------+----------+\n",
      "|          nom_cabina|qualitat_aire|codi_dtes|zqa|codi_eoi|longitud|latitud|hora_o3|qualitat_o3|  valor_o3|hora_no2|qualitat_no2| valor_no2|hora_pm10|qualitat_pm10|valor_pm10|        generat|  datetime|\n",
      "+--------------------+-------------+---------+---+--------+--------+-------+-------+-----------+----------+--------+------------+----------+---------+-------------+----------+---------------+----------+\n",
      "|   Barcelona - Sants|           --|       ID|  1| 8019042|  2.1331|41.3788|   NULL|       NULL|      NULL|    NULL|        NULL|      NULL|     NULL|         NULL|      NULL|01/02/2019 0:00|1548975902|\n",
      "|Barcelona - Eixample|           --|       IH|  1| 8019043|  2.1538|41.3853|   NULL|       NULL|      NULL|    NULL|        NULL|      NULL|      21h|           --|        --|01/02/2019 0:00|1548975902|\n",
      "| Barcelona - GrÃ cia|           --|       IJ|  1| 8019044|  2.1534|41.3987|   NULL|       NULL|      NULL|    NULL|        NULL|      NULL|      21h|           --|        --|01/02/2019 0:00|1548975902|\n",
      "|Barcelona - Ciuta...|           --|       IL|  1| 8019050|  2.1874|41.3864|   NULL|       NULL|      NULL|    NULL|        NULL|      NULL|     NULL|         NULL|      NULL|01/02/2019 0:00|1548975902|\n",
      "|Barcelona - Vall ...|           --|       IN|  1| 8019054|   2.148|41.4261|   NULL|       NULL|      NULL|    NULL|        NULL|      NULL|      21h|           --|        --|01/02/2019 0:00|1548975902|\n",
      "|Barcelona - Palau...|           --|       IZ|  1| 8019057|  2.1151|41.3875|   NULL|       NULL|      NULL|    NULL|        NULL|      NULL|      21h|           --|        --|01/02/2019 0:00|1548975902|\n",
      "|Barcelona - Poblenou|           --|       I2|  1| 8019004|  2.2045|41.4039|   NULL|       NULL|      NULL|    NULL|        NULL|      NULL|      21h|           --|        --|01/02/2019 0:00|1548975902|\n",
      "|Barcelona - Obser...|           --|       OF|  1| 8019058|  2.1239|41.4183|   NULL|       NULL|      NULL|    NULL|        NULL|      NULL|      21h|           --|        --|01/02/2019 0:00|1548975902|\n",
      "|   Barcelona - Sants|         Bona|       ID|  1| 8019042|  2.1331|41.3788|   NULL|       NULL|      NULL|      0h|        Bona|57 Âµg/mÂ³|     NULL|         NULL|      NULL|01/02/2019 1:00|1548979502|\n",
      "|Barcelona - Eixample|         Bona|       IH|  1| 8019043|  2.1538|41.3853|     0h|       Bona|32 Âµg/mÂ³|      0h|        Bona|45 Âµg/mÂ³|      22h|           --|        --|01/02/2019 1:00|1548979502|\n",
      "| Barcelona - GrÃ cia|         Bona|       IJ|  1| 8019044|  2.1534|41.3987|     0h|       Bona|12 Âµg/mÂ³|      0h|        Bona|73 Âµg/mÂ³|      22h|           --|        --|01/02/2019 1:00|1548979502|\n",
      "|Barcelona - Ciuta...|         Bona|       IL|  1| 8019050|  2.1874|41.3864|     0h|       Bona|38 Âµg/mÂ³|      0h|        Bona|33 Âµg/mÂ³|     NULL|         NULL|      NULL|01/02/2019 1:00|1548979502|\n",
      "|Barcelona - Vall ...|         Bona|       IN|  1| 8019054|   2.148|41.4261|     0h|       Bona|39 Âµg/mÂ³|      0h|        Bona|34 Âµg/mÂ³|      22h|           --|        --|01/02/2019 1:00|1548979502|\n",
      "|Barcelona - Palau...|         Bona|       IZ|  1| 8019057|  2.1151|41.3875|     0h|       Bona|45 Âµg/mÂ³|      0h|        Bona|19 Âµg/mÂ³|      22h|           --|        --|01/02/2019 1:00|1548979502|\n",
      "|Barcelona - Poblenou|         Bona|       I2|  1| 8019004|  2.2045|41.4039|   NULL|       NULL|      NULL|      0h|        Bona|76 Âµg/mÂ³|      22h|           --|        --|01/02/2019 1:00|1548979502|\n",
      "|Barcelona - Obser...|         Bona|       OF|  1| 8019058|  2.1239|41.4183|     0h|       Bona|57 Âµg/mÂ³|      0h|        Bona|12 Âµg/mÂ³|      22h|           --|        --|01/02/2019 1:00|1548979502|\n",
      "|   Barcelona - Sants|           --|       ID|  1| 8019042|  2.1331|41.3788|   NULL|       NULL|      NULL|     23h|          --|        --|     NULL|         NULL|      NULL|01/02/2019 2:00|1548983102|\n",
      "|Barcelona - Eixample|           --|       IH|  1| 8019043|  2.1538|41.3853|    23h|         --|        --|     23h|          --|        --|      23h|           --|        --|01/02/2019 2:00|1548983102|\n",
      "| Barcelona - GrÃ cia|           --|       IJ|  1| 8019044|  2.1534|41.3987|    23h|         --|        --|     23h|          --|        --|      23h|           --|        --|01/02/2019 2:00|1548983102|\n",
      "|Barcelona - Ciuta...|           --|       IL|  1| 8019050|  2.1874|41.3864|    23h|         --|        --|     23h|          --|        --|     NULL|         NULL|      NULL|01/02/2019 2:00|1548983102|\n",
      "+--------------------+-------------+---------+---+--------+--------+-------+-------+-----------+----------+--------+------------+----------+---------+-------------+----------+---------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airquality_dfs['2019_02'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# month smaller than or equal to 2019_03\n",
    "airquality_dfs_old = {month: airquality_dfs[month] for month in airquality_dfs if month <= '2019_03'}\n",
    "airquality_dfs_new = {month: airquality_dfs[month] for month in airquality_dfs if month > '2019_03'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airquality dataframes before 2019_03: 10\n",
      "Airquality dataframes after 2019_03: 59\n"
     ]
    }
   ],
   "source": [
    "print(f\"Airquality dataframes before 2019_03: {len(airquality_dfs_old)}\")\n",
    "print(f\"Airquality dataframes after 2019_03: {len(airquality_dfs_new)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18 unique columns in the dataframes\n"
     ]
    }
   ],
   "source": [
    "all_columns = set()\n",
    "for month in airquality_dfs_old:\n",
    "    # print nr of cols\n",
    "    all_columns = all_columns.union(set(airquality_dfs_old[month].columns))\n",
    "    # exclude nested fields as they have been flattened\n",
    "    all_columns = all_columns\n",
    "print(f\"There are {len(all_columns)} unique columns in the dataframes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nom_cabina': StringType(),\n",
       " 'qualitat_aire': StringType(),\n",
       " 'codi_dtes': StringType(),\n",
       " 'zqa': IntegerType(),\n",
       " 'codi_eoi': IntegerType(),\n",
       " 'longitud': DoubleType(),\n",
       " 'latitud': DoubleType(),\n",
       " 'hora_o3': StringType(),\n",
       " 'qualitat_o3': StringType(),\n",
       " 'valor_o3': StringType(),\n",
       " 'hora_no2': StringType(),\n",
       " 'qualitat_no2': StringType(),\n",
       " 'valor_no2': StringType(),\n",
       " 'hora_pm10': StringType(),\n",
       " 'qualitat_pm10': StringType(),\n",
       " 'valor_pm10': StringType(),\n",
       " 'generat': StringType(),\n",
       " 'datetime': IntegerType()}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import NullType, StringType, IntegerType, DoubleType, BooleanType\n",
    "\n",
    "column_data_types = {}\n",
    "for month in airquality_dfs_old:\n",
    "    df = airquality_dfs_old[month]\n",
    "    for column in df.columns:\n",
    "        # if it has not been added and is not of Null type\n",
    "        if column not in column_data_types and df.schema[column].dataType != NullType():\n",
    "            column_data_types[column] = df.schema[column].dataType\n",
    "column_data_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_of_incomplete_dfs = 0\n",
    "frequently_missing_cols = set()\n",
    "for month in airquality_dfs_old:\n",
    "    df = airquality_dfs_old[month]\n",
    "    missing_cols = all_columns - set(df.columns)\n",
    "    frequently_missing_cols = frequently_missing_cols.union(missing_cols)\n",
    "    if len(missing_cols) > 0:\n",
    "        nr_of_incomplete_dfs += 1\n",
    "        print(f\"There are {len(missing_cols)} missing columns in the dataframe for month {month}, the columns are: {missing_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are good here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_datatype_counter = {}\n",
    "for month in airquality_dfs_old:\n",
    "    df = airquality_dfs_old[month]\n",
    "    for column in df.columns:\n",
    "        datatype = df.schema[column].dataType\n",
    "        concat_column_datatype = f\"{column} - {datatype}\"\n",
    "        if concat_column_datatype not in columns_with_datatype_counter:\n",
    "            columns_with_datatype_counter[concat_column_datatype] = 1\n",
    "        else:\n",
    "            columns_with_datatype_counter[concat_column_datatype] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'zqa - IntegerType()': 10,\n",
       " 'valor_pm10 - StringType()': 10,\n",
       " 'valor_o3 - StringType()': 9,\n",
       " 'valor_o3 - DoubleType()': 1,\n",
       " 'valor_no2 - StringType()': 9,\n",
       " 'valor_no2 - DoubleType()': 1,\n",
       " 'qualitat_pm10 - StringType()': 10,\n",
       " 'qualitat_o3 - StringType()': 9,\n",
       " 'qualitat_o3 - DoubleType()': 1,\n",
       " 'qualitat_no2 - StringType()': 9,\n",
       " 'qualitat_no2 - DoubleType()': 1,\n",
       " 'qualitat_aire - StringType()': 10,\n",
       " 'nom_cabina - StringType()': 10,\n",
       " 'longitud - DoubleType()': 10,\n",
       " 'latitud - DoubleType()': 10,\n",
       " 'hora_pm10 - StringType()': 10,\n",
       " 'hora_o3 - StringType()': 9,\n",
       " 'hora_o3 - DoubleType()': 1,\n",
       " 'hora_no2 - StringType()': 9,\n",
       " 'hora_no2 - DoubleType()': 1,\n",
       " 'generat - StringType()': 10,\n",
       " 'datetime - IntegerType()': 10,\n",
       " 'codi_eoi - IntegerType()': 10,\n",
       " 'codi_dtes - StringType()': 10}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get entries where value is not 129 and 1\n",
    "columns_needing_fix = {}\n",
    "for col_and_type in columns_with_datatype_counter.keys():\n",
    "    columns_needing_fix[col_and_type] = columns_with_datatype_counter[col_and_type]\n",
    "columns_needing_fix = dict(sorted(columns_needing_fix.items(), key=lambda item: item[0], reverse=True))\n",
    "columns_needing_fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of null values by casting NullType to the column_data_types mapping\n",
    "for month in airquality_dfs_old:\n",
    "    df = airquality_dfs_old[month]\n",
    "    for column in df.columns:\n",
    "        # change data type to the correct one\n",
    "        df = df.withColumn(column, df[column].cast(column_data_types[column]))\n",
    "    airquality_dfs_old[month] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_datatype_counter = {}\n",
    "for month in airquality_dfs_old:\n",
    "    df = airquality_dfs_old[month]\n",
    "    for column in df.columns:\n",
    "        datatype = df.schema[column].dataType\n",
    "        concat_column_datatype = f\"{column} - {datatype}\"\n",
    "        if concat_column_datatype not in columns_with_datatype_counter:\n",
    "            columns_with_datatype_counter[concat_column_datatype] = 1\n",
    "        else:\n",
    "            columns_with_datatype_counter[concat_column_datatype] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'zqa - IntegerType()': 10,\n",
       " 'valor_pm10 - StringType()': 10,\n",
       " 'valor_o3 - StringType()': 10,\n",
       " 'valor_no2 - StringType()': 10,\n",
       " 'qualitat_pm10 - StringType()': 10,\n",
       " 'qualitat_o3 - StringType()': 10,\n",
       " 'qualitat_no2 - StringType()': 10,\n",
       " 'qualitat_aire - StringType()': 10,\n",
       " 'nom_cabina - StringType()': 10,\n",
       " 'longitud - DoubleType()': 10,\n",
       " 'latitud - DoubleType()': 10,\n",
       " 'hora_pm10 - StringType()': 10,\n",
       " 'hora_o3 - StringType()': 10,\n",
       " 'hora_no2 - StringType()': 10,\n",
       " 'generat - StringType()': 10,\n",
       " 'datetime - IntegerType()': 10,\n",
       " 'codi_eoi - IntegerType()': 10,\n",
       " 'codi_dtes - StringType()': 10}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get entries where value is not 129 and 1\n",
    "columns_needing_fix = {}\n",
    "for col_and_type in columns_with_datatype_counter.keys():\n",
    "    columns_needing_fix[col_and_type] = columns_with_datatype_counter[col_and_type]\n",
    "columns_needing_fix = dict(sorted(columns_needing_fix.items(), key=lambda item: item[0], reverse=True))\n",
    "columns_needing_fix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop entries that have one column dataframe\n",
    "airquality_dfs_new = {month: airquality_dfs_new[month] for month in airquality_dfs_new if len(airquality_dfs_new[month].columns) > 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57 unique columns in the dataframes\n"
     ]
    }
   ],
   "source": [
    "all_columns = set()\n",
    "for month in airquality_dfs_new:\n",
    "    # print nr of cols\n",
    "    all_columns = all_columns.union(set(airquality_dfs_new[month].columns))\n",
    "    # exclude nested fields as they have been flattened\n",
    "    all_columns = all_columns\n",
    "print(f\"There are {len(all_columns)} unique columns in the dataframes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'codi_provincia': IntegerType(),\n",
       " 'provincia': StringType(),\n",
       " 'codi_municipi': IntegerType(),\n",
       " 'municipi': StringType(),\n",
       " 'estacio': IntegerType(),\n",
       " 'codi_contaminant': IntegerType(),\n",
       " 'any': IntegerType(),\n",
       " 'mes': IntegerType(),\n",
       " 'dia': IntegerType(),\n",
       " 'h01': DoubleType(),\n",
       " 'v01': StringType(),\n",
       " 'h02': DoubleType(),\n",
       " 'v02': StringType(),\n",
       " 'h03': DoubleType(),\n",
       " 'v03': StringType(),\n",
       " 'h04': DoubleType(),\n",
       " 'v04': StringType(),\n",
       " 'h05': DoubleType(),\n",
       " 'v05': StringType(),\n",
       " 'h06': DoubleType(),\n",
       " 'v06': StringType(),\n",
       " 'h07': DoubleType(),\n",
       " 'v07': StringType(),\n",
       " 'h08': DoubleType(),\n",
       " 'v08': StringType(),\n",
       " 'h09': DoubleType(),\n",
       " 'v09': StringType(),\n",
       " 'h10': DoubleType(),\n",
       " 'v10': StringType(),\n",
       " 'h11': DoubleType(),\n",
       " 'v11': StringType(),\n",
       " 'h12': DoubleType(),\n",
       " 'v12': StringType(),\n",
       " 'h13': DoubleType(),\n",
       " 'v13': StringType(),\n",
       " 'h14': DoubleType(),\n",
       " 'v14': StringType(),\n",
       " 'h15': DoubleType(),\n",
       " 'v15': StringType(),\n",
       " 'h16': DoubleType(),\n",
       " 'v16': StringType(),\n",
       " 'h17': DoubleType(),\n",
       " 'v17': StringType(),\n",
       " 'h18': DoubleType(),\n",
       " 'v18': StringType(),\n",
       " 'h19': DoubleType(),\n",
       " 'v19': StringType(),\n",
       " 'h20': DoubleType(),\n",
       " 'v20': StringType(),\n",
       " 'h21': DoubleType(),\n",
       " 'v21': StringType(),\n",
       " 'h22': DoubleType(),\n",
       " 'v22': StringType(),\n",
       " 'h23': DoubleType(),\n",
       " 'v23': StringType(),\n",
       " 'h24': DoubleType(),\n",
       " 'v24': StringType()}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import NullType, StringType, IntegerType, DoubleType, BooleanType\n",
    "\n",
    "column_data_types = {}\n",
    "for month in airquality_dfs_new:\n",
    "    df = airquality_dfs_new[month]\n",
    "    for column in df.columns:\n",
    "        # if it has not been added and is not of Null type\n",
    "        if column not in column_data_types and df.schema[column].dataType != NullType():\n",
    "            column_data_types[column] = df.schema[column].dataType\n",
    "column_data_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_of_incomplete_dfs = 0\n",
    "frequently_missing_cols = set()\n",
    "for month in airquality_dfs_new:\n",
    "    df = airquality_dfs_new[month]\n",
    "    missing_cols = all_columns - set(df.columns)\n",
    "    frequently_missing_cols = frequently_missing_cols.union(missing_cols)\n",
    "    if len(missing_cols) > 0:\n",
    "        nr_of_incomplete_dfs += 1\n",
    "        print(f\"There are {len(missing_cols)} missing columns in the dataframe for month {month}, the columns are: {missing_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equal number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_datatype_counter = {}\n",
    "for month in airquality_dfs_old:\n",
    "    df = airquality_dfs_old[month]\n",
    "    for column in df.columns:\n",
    "        datatype = df.schema[column].dataType\n",
    "        concat_column_datatype = f\"{column} - {datatype}\"\n",
    "        if concat_column_datatype not in columns_with_datatype_counter:\n",
    "            columns_with_datatype_counter[concat_column_datatype] = 1\n",
    "        else:\n",
    "            columns_with_datatype_counter[concat_column_datatype] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'zqa - IntegerType()': 10,\n",
       " 'valor_pm10 - StringType()': 10,\n",
       " 'valor_o3 - StringType()': 10,\n",
       " 'valor_no2 - StringType()': 10,\n",
       " 'qualitat_pm10 - StringType()': 10,\n",
       " 'qualitat_o3 - StringType()': 10,\n",
       " 'qualitat_no2 - StringType()': 10,\n",
       " 'qualitat_aire - StringType()': 10,\n",
       " 'nom_cabina - StringType()': 10,\n",
       " 'longitud - DoubleType()': 10,\n",
       " 'latitud - DoubleType()': 10,\n",
       " 'hora_pm10 - StringType()': 10,\n",
       " 'hora_o3 - StringType()': 10,\n",
       " 'hora_no2 - StringType()': 10,\n",
       " 'generat - StringType()': 10,\n",
       " 'datetime - IntegerType()': 10,\n",
       " 'codi_eoi - IntegerType()': 10,\n",
       " 'codi_dtes - StringType()': 10}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get entries where value is not 129 and 1\n",
    "columns_needing_fix = {}\n",
    "for col_and_type in columns_with_datatype_counter.keys():\n",
    "    columns_needing_fix[col_and_type] = columns_with_datatype_counter[col_and_type]\n",
    "columns_needing_fix = dict(sorted(columns_needing_fix.items(), key=lambda item: item[0], reverse=True))\n",
    "columns_needing_fix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema is aligned, we can merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2018_12 has 5848 rows\n",
      "Month 2019_03 has 5928 rows\n",
      "Month 2018_07 has 3649 rows\n",
      "Month 2019_01 has 5904 rows\n",
      "Month 2018_09 has 5736 rows\n",
      "Month 2018_10 has 5944 rows\n",
      "Month 2019_02 has 5376 rows\n",
      "Month 2018_06 has 3262 rows\n",
      "Month 2018_11 has 5744 rows\n",
      "Month 2018_08 has 5800 rows\n",
      "Month 2022_10 has 2015 rows\n",
      "Month 2022_03 has 1987 rows\n",
      "Month 2022_05 has 2015 rows\n",
      "Month 2019_12 has 1276 rows\n",
      "Month 2023_12 has 2015 rows\n",
      "Month 2019_11 has 1215 rows\n",
      "Month 2021_05 has 1674 rows\n",
      "Month 2023_08 has 2015 rows\n",
      "Month 2021_12 has 1705 rows\n",
      "Month 2020_07 has 1519 rows\n",
      "Month 2022_07 has 2015 rows\n",
      "Month 2023_07 has 2039 rows\n",
      "Month 2021_11 has 1587 rows\n",
      "Month 2021_04 has 1480 rows\n",
      "Month 2021_07 has 1674 rows\n",
      "Month 2021_01 has 1519 rows\n",
      "Month 2020_03 has 1395 rows\n",
      "Month 2022_06 has 1950 rows\n",
      "Month 2024_02 has 1856 rows\n",
      "Month 2019_09 has 1131 rows\n",
      "Month 2023_06 has 1980 rows\n",
      "Month 2021_02 has 1372 rows\n",
      "Month 2021_03 has 1519 rows\n",
      "Month 2019_04 has 1131 rows\n",
      "Month 2019_10 has 1209 rows\n",
      "Month 2022_12 has 2015 rows\n",
      "Month 2022_01 has 1705 rows\n",
      "Month 2019_08 has 1209 rows\n",
      "Month 2024_01 has 1987 rows\n",
      "Month 2021_09 has 1620 rows\n",
      "Month 2023_05 has 2046 rows\n",
      "Month 2021_10 has 1674 rows\n",
      "Month 2023_04 has 1980 rows\n",
      "Month 2022_11 has 1950 rows\n",
      "Month 2023_11 has 1950 rows\n",
      "Month 2020_05 has 1519 rows\n",
      "Month 2020_08 has 1372 rows\n",
      "Month 2020_02 has 1305 rows\n",
      "Month 2023_10 has 2015 rows\n",
      "Month 2023_01 has 2015 rows\n",
      "Month 2019_05 has 1092 rows\n",
      "Month 2023_03 has 2029 rows\n",
      "Month 2020_11 has 1274 rows\n",
      "Month 2020_10 has 1519 rows\n",
      "Month 2020_09 has 1470 rows\n",
      "Month 2020_06 has 1470 rows\n",
      "Month 2020_12 has 1519 rows\n",
      "Month 2022_02 has 1562 rows\n",
      "Month 2020_01 has 1279 rows\n",
      "Month 2022_08 has 2015 rows\n",
      "Month 2022_04 has 1950 rows\n",
      "Month 2020_04 has 1378 rows\n",
      "Month 2023_02 has 1820 rows\n",
      "Month 2021_06 has 1620 rows\n",
      "Month 2022_09 has 1950 rows\n",
      "Month 2021_08 has 1674 rows\n",
      "Month 2023_09 has 1950 rows\n"
     ]
    }
   ],
   "source": [
    "# print the lens of old and new airquality dataframes\n",
    "old_size = 0\n",
    "new_size = 0\n",
    "for month in airquality_dfs_old:\n",
    "    old_size += airquality_dfs_old[month].count()\n",
    "    print(f\"Month {month} has {airquality_dfs_old[month].count()} rows\")\n",
    "for month in airquality_dfs_new:\n",
    "    new_size += airquality_dfs_new[month].count()\n",
    "    print(f\"Month {month} has {airquality_dfs_new[month].count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old total rows: 53191\n",
      "New total rows: 95226\n"
     ]
    }
   ],
   "source": [
    "print(f\"Old total rows: {old_size}\")\n",
    "print(f\"New total rows: {new_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the new airquality dataframe: 95226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# add a column to each dataframe with the date\n",
    "for month in airquality_dfs_new:\n",
    "    df = airquality_dfs_new[month]\n",
    "    df = df.withColumn('month', lit(month))\n",
    "    airquality_dfs_new[month] = df\n",
    "# merge all the dataframes into a single dataframe\n",
    "airquality_df_new = None\n",
    "for month in airquality_dfs_new:\n",
    "    if airquality_df_new is None:\n",
    "        airquality_df_new = airquality_dfs_new[month]\n",
    "    else:\n",
    "        airquality_df_new = airquality_df_new.union(airquality_dfs_new[month])\n",
    "print(f\"Number of rows in the new airquality dataframe: {airquality_df_new.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the old airquality dataframe: 53191\n"
     ]
    }
   ],
   "source": [
    "# add a column to each dataframe with the date\n",
    "for month in airquality_dfs_old:\n",
    "    df = airquality_dfs_old[month]\n",
    "    df = df.withColumn('month', lit(month))\n",
    "    airquality_dfs_old[month] = df\n",
    "# merge all the dataframes into a single dataframe\n",
    "airquality_df_old = None\n",
    "for month in airquality_dfs_old:\n",
    "    if airquality_df_old is None:\n",
    "        airquality_df_old = airquality_dfs_old[month]\n",
    "    else:\n",
    "        airquality_df_old = airquality_df_old.union(airquality_dfs_old[month])\n",
    "print(f\"Number of rows in the old airquality dataframe: {airquality_df_old.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+---------+---+--------+--------+-------+-------+-----------+----------+--------+------------+----------+---------+-------------+----------+---------------+----------+-------+\n",
      "|          nom_cabina|qualitat_aire|codi_dtes|zqa|codi_eoi|longitud|latitud|hora_o3|qualitat_o3|  valor_o3|hora_no2|qualitat_no2| valor_no2|hora_pm10|qualitat_pm10|valor_pm10|        generat|  datetime|  month|\n",
      "+--------------------+-------------+---------+---+--------+--------+-------+-------+-----------+----------+--------+------------+----------+---------+-------------+----------+---------------+----------+-------+\n",
      "|   Barcelona - Sants|         Bona|       ID|  1| 8019042|  2.1331|41.3788|   NULL|       NULL|      NULL|     23h|        Bona|39 Âµg/mÂ³|     NULL|         NULL|      NULL|01/12/2018 0:00|1543619102|2018_12|\n",
      "|Barcelona - Eixample|         Bona|       IH|  1| 8019043|  2.1538|41.3853|   NULL|       NULL|      NULL|    NULL|        NULL|      NULL|      23h|         Bona|36 Âµg/mÂ³|01/12/2018 0:00|1543619102|2018_12|\n",
      "| Barcelona - GrÃ cia|         Bona|       IJ|  1| 8019044|  2.1534|41.3987|    23h|       Bona|15 Âµg/mÂ³|     23h|        Bona|52 Âµg/mÂ³|      23h|         Bona|22 Âµg/mÂ³|01/12/2018 0:00|1543619102|2018_12|\n",
      "|Barcelona - Ciuta...|         Bona|       IL|  1| 8019050|  2.1874|41.3864|    23h|       Bona|18 Âµg/mÂ³|     23h|        Bona|39 Âµg/mÂ³|     NULL|         NULL|      NULL|01/12/2018 0:00|1543619102|2018_12|\n",
      "|Barcelona - Vall ...|         Bona|       IN|  1| 8019054|   2.148|41.4261|    23h|       Bona|43 Âµg/mÂ³|     23h|        Bona|18 Âµg/mÂ³|      23h|         Bona|20 Âµg/mÂ³|01/12/2018 0:00|1543619102|2018_12|\n",
      "+--------------------+-------------+---------+---+--------+--------+-------+-------+-----------+----------+--------+------------+----------+---------+-------------+----------+---------------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airquality_df_old.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 16:56:12 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+----+---+---+---+---+---+---+---+----+---+----+---+----+---+----+---+----+---+----+---+----+---+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-------+\n",
      "|codi_provincia|provincia|codi_municipi| municipi|estacio|codi_contaminant| any|mes|dia| h01|v01|h02|v02|h03|v03|h04|v04| h05|v05| h06|v06| h07|v07| h08|v08| h09|v09| h10|v10| h11|v11| h12|v12|h13|v13|h14|v14|h15|v15|h16|v16|h17|v17|h18|v18|h19|v19|h20|v20|h21|v21|h22|v22|h23|v23|h24|v24|  month|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+----+---+---+---+---+---+---+---+----+---+----+---+----+---+----+---+----+---+----+---+----+---+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-------+\n",
      "|             8|Barcelona|           19|Barcelona|      4|               7|2022| 10|  1| 1.0|  V|1.0|  V|1.0|  V|1.0|  V| 1.0|  V| 1.0|  V| 1.0|  V| 1.0|  V| 2.0|  V| 5.0|  V| 4.0|  V| 3.0|  V|3.0|  V|5.0|  V|3.0|  V|9.0|  V|2.0|  V|2.0|  V|2.0|  V|3.0|  V|2.0|  V|1.0|  V|1.0|  V|5.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|      4|               7|2022| 10|  2| 6.0|  V|2.0|  V|2.0|  V|1.0|  V| 1.0|  V| 1.0|  V| 1.0|  V| 2.0|  V| 2.0|  V| 7.0|  V| 5.0|  V| 3.0|  V|2.0|  V|5.0|  V|1.0|  V|3.0|  V|2.0|  V|2.0|  V|1.0|  V|1.0|  V|1.0|  V|1.0|  V|1.0|  V|2.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|      4|               7|2022| 10|  3| 1.0|  V|1.0|  V|1.0|  V|1.0|  V| 1.0|  V| 2.0|  V| 3.0|  V|15.0|  V|53.0|  V|56.0|  V|51.0|  V|14.0|  V|2.0|  V|2.0|  V|2.0|  V|1.0|  V|1.0|  V|1.0|  V|1.0|  V|1.0|  V|1.0|  V|1.0|  V|1.0|  V|6.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|      4|               7|2022| 10|  4|14.0|  V|9.0|  V|2.0|  V|1.0|  V|10.0|  V|10.0|  V|21.0|  V|34.0|  V|67.0|  V|49.0|  V|22.0|  V| 5.0|  V|3.0|  V|2.0|  V|NaN|  N|2.0|  V|3.0|  V|2.0|  V|2.0|  V|1.0|  V|1.0|  V|1.0|  V|2.0|  V|3.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|      4|               7|2022| 10|  5| 2.0|  V|2.0|  V|2.0|  V|1.0|  V| 2.0|  V| 6.0|  V|17.0|  V|33.0|  V|61.0|  V|48.0|  V|27.0|  V| 8.0|  V|2.0|  V|2.0|  V|1.0|  V|1.0|  V|3.0|  V|2.0|  V|2.0|  V|1.0|  V|2.0|  V|1.0|  V|3.0|  V|2.0|  V|2022_10|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+----+---+---+---+---+---+---+---+----+---+----+---+----+---+----+---+----+---+----+---+----+---+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 16:56:14 WARN DAGScheduler: Broadcasting large task binary with size 1699.3 KiB\n"
     ]
    }
   ],
   "source": [
    "airquality_df_new.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before deduplication: 53191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 616:============================================>           (8 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after deduplication: 53191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# airquality_df_old\n",
    "print(f\"Size before deduplication: {airquality_df_old.count()}\")\n",
    "airquality_df_old = airquality_df_old.dropDuplicates()\n",
    "print(f\"Size after deduplication: {airquality_df_old.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before deduplication: 95226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 16:58:34 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 16:58:40 WARN DAGScheduler: Broadcasting large task binary with size 1554.8 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after deduplication: 95226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f\"Size before deduplication: {airquality_df_new.count()}\")\n",
    "airquality_df_new = airquality_df_new.dropDuplicates()\n",
    "print(f\"Size after deduplication: {airquality_df_new.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/uni-projects/bdm2/.venv/lib/python3.10/site-packages/pyspark/sql/functions.py:3796: FutureWarning: Deprecated in 2.1, use approx_count_distinct instead.\n",
      "  warnings.warn(\"Deprecated in 2.1, use approx_count_distinct instead.\", FutureWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['latitud', 'datetime']\n"
     ]
    }
   ],
   "source": [
    "continuous_variables_old = detect_continuous_variables(airquality_df_old, 10)\n",
    "print(continuous_variables_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: latitud\n",
      "Q1: 41.3864\n",
      "Q3: 41.4039\n",
      "IQR: 0.017499999999998295\n",
      "Lower Bound: 41.33390000000001\n",
      "Upper Bound: 41.456399999999995\n",
      "Number of outliers: 192\n",
      "Outliers:\n",
      "+--------------------+-------------+---------+---+--------+--------+--------+-------+-----------+----------+--------+------------+----------+---------+-------------+----------+----------------+----------+-------+\n",
      "|          nom_cabina|qualitat_aire|codi_dtes|zqa|codi_eoi|longitud| latitud|hora_o3|qualitat_o3|  valor_o3|hora_no2|qualitat_no2| valor_no2|hora_pm10|qualitat_pm10|valor_pm10|         generat|  datetime|  month|\n",
      "+--------------------+-------------+---------+---+--------+--------+--------+-------+-----------+----------+--------+------------+----------+---------+-------------+----------+----------------+----------+-------+\n",
      "|Barcelona - Vall ...|         Bona|       IN|  1| 8019054|   2.148|414261.0|     5h|       Bona|53 Âµg/mÂ³|      5h|        Bona|12 Âµg/mÂ³|       5h|         Bona|26 Âµg/mÂ³| 13/11/2018 6:00|1542085503|2018_11|\n",
      "|Barcelona - Vall ...|         Bona|       IN|  1| 8019054|   2.148|414261.0|    10h|       Bona|42 Âµg/mÂ³|     10h|        Bona|34 Âµg/mÂ³|      10h|         Bona|29 Âµg/mÂ³|13/11/2018 11:00|1542103502|2018_11|\n",
      "|Barcelona - Obser...|         Bona|       OF|  1| 8019058|  2.1239|414183.0|    21h|       Bona|71 Âµg/mÂ³|     21h|        Bona|11 Âµg/mÂ³|      21h|         Bona|22 Âµg/mÂ³|13/11/2018 22:00|1542143102|2018_11|\n",
      "|Barcelona - Palau...|         Bona|       IZ|  1| 8019057|  2.1151|413875.0|    15h|       Bona|49 Âµg/mÂ³|     15h|        Bona|23 Âµg/mÂ³|      15h|         Bona|29 Âµg/mÂ³|13/11/2018 16:00|1542121502|2018_11|\n",
      "|Barcelona - Obser...|         Bona|       OF|  1| 8019058|  2.1239|414183.0|     8h|       Bona|86 Âµg/mÂ³|      8h|        Bona| 4 Âµg/mÂ³|       8h|         Bona|18 Âµg/mÂ³| 13/11/2018 9:00|1542096302|2018_11|\n",
      "|Barcelona - Palau...|         Bona|       IZ|  1| 8019057|  2.1151|413875.0|    12h|       Bona|51 Âµg/mÂ³|     12h|        Bona|23 Âµg/mÂ³|      12h|         Bona|28 Âµg/mÂ³|13/11/2018 13:00|1542110701|2018_11|\n",
      "|Barcelona - Vall ...|         Bona|       IN|  1| 8019054|   2.148|414261.0|    18h|       Bona|46 Âµg/mÂ³|     18h|        Bona|30 Âµg/mÂ³|      18h|         Bona|32 Âµg/mÂ³|13/11/2018 19:00|1542132302|2018_11|\n",
      "|Barcelona - Obser...|         Bona|       OF|  1| 8019058|  2.1239|414183.0|    19h|       Bona|56 Âµg/mÂ³|     19h|        Bona|22 Âµg/mÂ³|      19h|         Bona|21 Âµg/mÂ³|13/11/2018 20:00|1542135901|2018_11|\n",
      "|Barcelona - Obser...|         Bona|       OF|  1| 8019058|  2.1239|414183.0|    18h|       Bona|64 Âµg/mÂ³|     18h|        Bona|16 Âµg/mÂ³|      18h|         Bona|20 Âµg/mÂ³|13/11/2018 19:00|1542132302|2018_11|\n",
      "|Barcelona - Vall ...|         Bona|       IN|  1| 8019054|   2.148|414261.0|     7h|       Bona|37 Âµg/mÂ³|      7h|        Bona|30 Âµg/mÂ³|       7h|         Bona|28 Âµg/mÂ³| 13/11/2018 8:00|1542092703|2018_11|\n",
      "|Barcelona - Vall ...|         Bona|       IN|  1| 8019054|   2.148|414261.0|     4h|       Bona|56 Âµg/mÂ³|      4h|        Bona| 9 Âµg/mÂ³|       4h|         Bona|25 Âµg/mÂ³| 13/11/2018 5:00|1542081903|2018_11|\n",
      "|Barcelona - Vall ...|         Bona|       IN|  1| 8019054|   2.148|414261.0|    14h|       Bona|64 Âµg/mÂ³|     14h|        Bona|18 Âµg/mÂ³|      14h|         Bona|31 Âµg/mÂ³|13/11/2018 15:00|1542117902|2018_11|\n",
      "|Barcelona - Vall ...|         Bona|       IN|  1| 8019054|   2.148|414261.0|    21h|       Bona|26 Âµg/mÂ³|     21h|        Bona|53 Âµg/mÂ³|      21h|         Bona|31 Âµg/mÂ³|13/11/2018 22:00|1542143102|2018_11|\n",
      "|Barcelona - Palau...|         Bona|       IZ|  1| 8019057|  2.1151|413875.0|    21h|       Bona|12 Âµg/mÂ³|     21h|        Bona|61 Âµg/mÂ³|      21h|         Bona|28 Âµg/mÂ³|13/11/2018 22:00|1542143102|2018_11|\n",
      "|Barcelona - Obser...|         Bona|       OF|  1| 8019058|  2.1239|414183.0|     0h|       Bona|78 Âµg/mÂ³|      0h|        Bona| 8 Âµg/mÂ³|       0h|         Bona|15 Âµg/mÂ³| 13/11/2018 1:00|1542067503|2018_11|\n",
      "|Barcelona - Vall ...|         Bona|       IN|  1| 8019054|   2.148|414261.0|     6h|       Bona|47 Âµg/mÂ³|      6h|        Bona|17 Âµg/mÂ³|       6h|         Bona|27 Âµg/mÂ³| 13/11/2018 7:00|1542089103|2018_11|\n",
      "|Barcelona - Eixample|      Regular|       IH|  1| 8019043|  2.1538|413853.0|     9h|       Bona| 3 Âµg/mÂ³|      9h|     Regular|95 Âµg/mÂ³|       9h|         Bona|36 Âµg/mÂ³|13/11/2018 10:00|1542099902|2018_11|\n",
      "|Barcelona - Eixample|      Regular|       IH|  1| 8019043|  2.1538|413853.0|    17h|       Bona|30 Âµg/mÂ³|     17h|        Bona|54 Âµg/mÂ³|      18h|      Regular|40 Âµg/mÂ³|13/11/2018 18:00|1542128702|2018_11|\n",
      "|Barcelona - Vall ...|         Bona|       IN|  1| 8019054|   2.148|414261.0|    17h|       Bona|57 Âµg/mÂ³|     17h|        Bona|21 Âµg/mÂ³|      17h|         Bona|32 Âµg/mÂ³|13/11/2018 18:00|1542128702|2018_11|\n",
      "|Barcelona - Vall ...|         Bona|       IN|  1| 8019054|   2.148|414261.0|     8h|       Bona|29 Âµg/mÂ³|      8h|        Bona|44 Âµg/mÂ³|       8h|         Bona|29 Âµg/mÂ³| 13/11/2018 9:00|1542096302|2018_11|\n",
      "+--------------------+-------------+---------+---+--------+--------+--------+-------+-----------+----------+--------+------------+----------+---------+-------------+----------+----------------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Column: datetime\n",
      "Q1: 1535839504.0\n",
      "Q3: 1547953502.0\n",
      "IQR: 12113998.0\n",
      "Lower Bound: 1499497510.0\n",
      "Upper Bound: 1584295496.0\n",
      "Number of outliers: 0\n"
     ]
    }
   ],
   "source": [
    "airquality_df_old = iqr_outlier_treatment(airquality_df_old, continuous_variables_old, factor=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/uni-projects/bdm2/.venv/lib/python3.10/site-packages/pyspark/sql/functions.py:3796: FutureWarning: Deprecated in 2.1, use approx_count_distinct instead.\n",
      "  warnings.warn(\"Deprecated in 2.1, use approx_count_distinct instead.\", FutureWarning)\n",
      "24/05/16 17:01:27 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:01:32 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "24/05/16 17:01:33 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "24/05/16 17:01:35 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:01:40 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "24/05/16 17:01:41 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "24/05/16 17:01:43 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:01:48 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "24/05/16 17:01:49 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "24/05/16 17:01:51 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:01:56 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "24/05/16 17:01:57 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "24/05/16 17:02:00 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:02:05 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "24/05/16 17:02:06 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "24/05/16 17:02:08 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:02:13 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "24/05/16 17:02:14 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "24/05/16 17:02:16 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:02:21 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "24/05/16 17:02:22 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "24/05/16 17:02:24 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:02:29 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "24/05/16 17:02:30 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "24/05/16 17:02:32 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:02:37 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "24/05/16 17:02:38 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "24/05/16 17:02:40 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:02:45 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "24/05/16 17:02:46 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "24/05/16 17:02:49 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:02:53 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "24/05/16 17:02:54 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "24/05/16 17:02:56 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:03:02 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "24/05/16 17:03:02 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "24/05/16 17:03:05 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:03:09 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "24/05/16 17:03:10 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "24/05/16 17:03:12 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:03:16 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "24/05/16 17:03:17 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "24/05/16 17:03:19 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:03:23 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "24/05/16 17:03:24 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "24/05/16 17:03:26 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:03:31 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "24/05/16 17:03:31 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "24/05/16 17:03:33 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:03:38 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "24/05/16 17:03:39 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "24/05/16 17:03:41 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:03:45 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "24/05/16 17:03:46 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "24/05/16 17:03:48 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:03:52 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "24/05/16 17:03:53 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "24/05/16 17:03:55 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:04:00 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "24/05/16 17:04:01 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "24/05/16 17:04:03 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:04:07 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "24/05/16 17:04:08 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "24/05/16 17:04:10 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:04:14 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "24/05/16 17:04:15 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "24/05/16 17:04:17 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:04:22 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "24/05/16 17:04:23 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "24/05/16 17:04:25 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:04:30 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "24/05/16 17:04:30 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "24/05/16 17:04:33 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:04:37 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "24/05/16 17:04:38 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "24/05/16 17:04:40 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:04:46 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "24/05/16 17:04:47 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "24/05/16 17:04:49 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:04:54 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "24/05/16 17:04:55 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "24/05/16 17:04:57 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:05:03 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "24/05/16 17:05:03 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "24/05/16 17:05:06 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:05:12 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "24/05/16 17:05:13 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "24/05/16 17:05:15 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:05:21 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "24/05/16 17:05:22 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "24/05/16 17:05:25 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:05:30 WARN DAGScheduler: Broadcasting large task binary with size 1563.6 KiB\n",
      "[Stage 867:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['codi_contaminant', 'mes', 'dia', 'h01', 'h02', 'h03', 'h04', 'h05', 'h06', 'h07', 'h08', 'h09', 'h10', 'h11', 'h12', 'h13', 'h14', 'h15', 'h16', 'h17', 'h18', 'h19', 'h20', 'h21', 'h22', 'h23', 'h24']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:05:30 WARN DAGScheduler: Broadcasting large task binary with size 1521.9 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "continuous_variables_new = detect_continuous_variables(airquality_df_new, 10)\n",
    "print(continuous_variables_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:05:47 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:05:51 WARN DAGScheduler: Broadcasting large task binary with size 1508.9 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: codi_contaminant\n",
      "Q1: 7.0\n",
      "Q3: 14.0\n",
      "IQR: 7.0\n",
      "Lower Bound: -14.0\n",
      "Upper Bound: 35.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:05:55 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:05:58 WARN DAGScheduler: Broadcasting large task binary with size 1600.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 17044\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:06:01 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:06:04 WARN DAGScheduler: Broadcasting large task binary with size 1625.1 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|codi_provincia|provincia|codi_municipi| municipi|estacio|codi_contaminant| any|mes|dia|   h01|v01|   h02|v02|   h03|v03|   h04|v04|   h05|v05|   h06|v06|   h07|v07|   h08|v08|   h09|v09|   h10|v10|   h11|v11|   h12|v12|   h13|v13|   h14|v14|   h15|v15|   h16|v16|   h17|v17|   h18|v18|   h19|v19|   h20|v20|   h21|v21|   h22|v22|   h23|v23|   h24|v24|  month|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|             8|Barcelona|           19|Barcelona|     57|             108|2022| 10| 18| 13.59|  V| 12.24|  V| 11.04|  V|  8.21|  V| 13.35|  V|  8.96|  V| 14.44|  V| 37.04|  V| 50.18|  V| 46.96|  V|  26.6|  V|  9.87|  V| 16.79|  V| 16.02|  V| 18.07|  V| 20.41|  V| 28.24|  V| 34.69|  V| 33.71|  V| 60.17|  V| 53.05|  V| 49.71|  V| 59.95|  V| 55.84|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     57|             112|2022| 10| 10| 35.38|  V|  9.77|  V|  4.19|  V|  9.68|  V|  7.23|  V|  7.22|  V| 11.59|  V| 34.24|  V| 83.17|  V| 67.63|  V| 69.93|  V| 53.76|  V| 34.98|  V|   NaN|  N|   NaN|  N|   NaN|  N| 23.42|  V| 28.96|  V| 27.81|  V| 27.49|  V| 18.18|  V| 13.44|  V| 13.53|  V| 29.74|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  3|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     57|             101|2022| 10| 29|  1.82|  V|   1.9|  V|   2.0|  V|  1.61|  V|  1.53|  V|  1.52|  V|  1.68|  V|  1.97|  V|   2.3|  V|  1.86|  V|  1.82|  V|   1.8|  V|  1.68|  V|  1.78|  V|  1.72|  V|   1.4|  V|  1.53|  V|  1.41|  V|  1.43|  V|  1.34|  V|  1.46|  V|  1.57|  V|  1.37|  V|  1.28|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     57|             114|2022| 10|  4|  25.0|  V|  20.2|  V|  39.1|  V|  44.6|  V|  36.8|  V|  20.6|  V|  16.6|  V|   9.8|  V|   6.6|  V|  29.7|  V|  35.4|  V|  50.5|  V|  56.3|  V|  66.0|  V|  70.4|  V|  70.7|  V|  74.1|  V|  68.8|  V|  59.6|  V|  62.7|  V|  54.5|  V|  33.6|  V|  32.7|  V|  32.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 20|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     57|             107|2022| 10| 18|  0.05|  V|  0.29|  V|  0.35|  V|  1.89|  V|  4.58|  V|  0.46|  V|  0.75|  V| 10.98|  V| 32.19|  V| 17.29|  V|  4.11|  V|  1.62|  V|  2.25|  V|  2.67|  V|  2.37|  V|  3.05|  V|  2.62|  V|  2.25|  V|  1.31|  V|   4.9|  V|  1.05|  V|  3.04|  V| 10.72|  V|  9.15|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             901|2022| 10| 15|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 18|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3864.8|  V|3886.1|  V|3893.0|  V|3893.0|  V|3892.5|  V|3892.4|  V|3888.9|  V|3881.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3885.5|  V|3893.0|  V|3892.8|  V|3883.7|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 26|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     57|             106|2022| 10| 23|  0.24|  V|  0.17|  V|  0.15|  V|   0.1|  V|  0.15|  V|   0.2|  V|  0.22|  V|  0.23|  V|  0.28|  V|  0.27|  V|  0.18|  V|  0.25|  V|  0.18|  V|  0.18|  V|  0.17|  V|  0.18|  V|   0.2|  V|   0.2|  V|  0.18|  V|  0.17|  V|  0.29|  V|  0.22|  V|  0.18|  V|  0.23|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             901|2022| 10| 11|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             901|2022| 10| 26|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|   0.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             996|2022| 10| 21|   NaN|  N|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1115.2|  V|1123.7|  V|1126.0|  V|1126.0|  V|1126.0|  V|1128.9|  V|1137.6|  V|1133.0|  V|1133.7|  V|1138.7|  V|1112.6|  V|1101.1|  V|1106.9|  V|1113.6|  V|1114.0|  V|1114.0|  V|1114.7|  V|1124.2|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 16|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             999|2022| 10| 17|  11.4|  V|  11.6|  V|  12.1|  V|  13.0|  V|  12.3|  V|   9.5|  V|   7.6|  V|   6.2|  V|   8.5|  V|   7.7|  V|   8.5|  V|   9.1|  V|   9.6|  V|  10.3|  V|   9.8|  V|   8.7|  V|   8.6|  V|   9.4|  V|   8.5|  V|   8.6|  V|   8.2|  V|   8.6|  V|   8.3|  V|   8.5|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     57|             106|2022| 10| 28|   0.2|  V|  0.18|  V|  0.15|  V|  0.16|  V|  0.16|  V|  0.19|  V|  0.23|  V|  0.34|  V|  0.58|  V|  0.67|  V|  0.32|  V|  0.23|  V|  0.22|  V|  0.21|  V|  0.24|  V|  0.27|  V|  0.14|  V|   0.1|  V|  0.14|  V|  0.21|  V|  0.44|  V|  0.46|  V|  0.49|  V|  0.47|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     57|             101|2022| 10| 10|  2.45|  V|  2.41|  V|  2.32|  V|  2.41|  V|  2.26|  V|  2.15|  V|  2.18|  V|  2.34|  V|  2.62|  V|  2.43|  V|  2.72|  V|  2.65|  V|  2.95|  V|   NaN|  N|   NaN|  N|   NaN|  N|  2.08|  V|  2.26|  V|  1.92|  V|  1.88|  V|  1.98|  V|  1.92|  V|  2.04|  V|  2.07|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             999|2022| 10| 22|  23.9|  V|  23.3|  V|  24.8|  V|  27.7|  V|  29.7|  V|  28.7|  V|  25.4|  V|  22.3|  V|  19.3|  V|  17.9|  V|  16.8|  V|  14.4|  V|  17.8|  V|  20.0|  V|  19.8|  V|  16.3|  V|  15.6|  V|  15.8|  V|  15.5|  V|  17.0|  V|  17.2|  V|  21.6|  V|  20.1|  V|  21.1|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     57|             107|2022| 10| 27|  4.83|  V|  1.08|  V|  1.41|  V|  0.79|  V|  0.32|  V|  0.68|  V|   2.9|  V| 38.72|  V| 86.51|  V|107.11|  V| 70.77|  V| 25.57|  V| 16.88|  V|  6.23|  V|  3.38|  V|  1.61|  V|  1.41|  V|  0.92|  V|  0.44|  V|   0.2|  V|  0.26|  V|  0.09|  V|  0.26|  V|  1.02|  V|2022_10|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:06:07 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:06:11 WARN DAGScheduler: Broadcasting large task binary with size 1508.9 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: mes\n",
      "Q1: 4.0\n",
      "Q3: 10.0\n",
      "IQR: 6.0\n",
      "Lower Bound: -14.0\n",
      "Upper Bound: 28.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:06:14 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:06:19 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:06:23 WARN DAGScheduler: Broadcasting large task binary with size 1508.9 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: dia\n",
      "Q1: 8.0\n",
      "Q3: 23.0\n",
      "IQR: 15.0\n",
      "Lower Bound: -37.0\n",
      "Upper Bound: 68.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:06:26 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:06:32 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:06:36 WARN DAGScheduler: Broadcasting large task binary with size 1507.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: h01\n",
      "Q1: 2.0\n",
      "Q3: 40.0\n",
      "IQR: 38.0\n",
      "Lower Bound: -112.0\n",
      "Upper Bound: 154.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:06:39 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:06:42 WARN DAGScheduler: Broadcasting large task binary with size 1596.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 12400\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:06:46 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:06:49 WARN DAGScheduler: Broadcasting large task binary with size 1621.0 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|codi_provincia|provincia|codi_municipi| municipi|estacio|codi_contaminant| any|mes|dia|   h01|v01|   h02|v02|   h03|v03|   h04|v04|   h05|v05|   h06|v06|   h07|v07|   h08|v08|   h09|v09|   h10|v10|   h11|v11|   h12|v12|   h13|v13|   h14|v14|   h15|v15|   h16|v16|   h17|v17|   h18|v18|   h19|v19|   h20|v20|   h21|v21|   h22|v22|   h23|v23|   h24|v24|  month|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 11| 771.0|  V|1024.0|  V| 998.0|  V| 659.0|  V| 641.0|  V| 734.0|  V| 846.0|  V|1278.0|  V|1077.0|  V|1086.0|  V|1096.0|  V|1746.0|  V|1653.0|  V| 825.0|  V| 707.0|  V| 730.0|  V| 833.0|  V| 791.0|  V| 944.0|  V|1002.0|  V| 949.0|  V|1116.0|  V|1621.0|  V|5794.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 22|1146.0|  V| 915.0|  V| 471.0|  V| 288.0|  V| 215.0|  V| 256.0|  V| 521.0|  V| 458.0|  V| 419.0|  V| 374.0|  V| 479.0|  V| 449.0|  V| 392.0|  V| 338.0|  V| 502.0|  V| 575.0|  V| 353.0|  V| 365.0|  V| 512.0|  V|1238.0|  V|2378.0|  V|2081.0|  V|3206.0|  V|1822.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  3|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 20|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 24|1921.0|  V|1593.0|  V|2850.0|  V|1949.0|  V|1559.0|  V|1102.0|  V|1619.0|  V|3558.0|  V|4674.0|  V|2410.0|  V|1657.0|  V|1159.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|1549.0|  V|1891.0|  V| 750.0|  V| 996.0|  V|1629.0|  V|2417.0|  V|2799.0|  V|2117.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 18|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3864.8|  V|3886.1|  V|3893.0|  V|3893.0|  V|3892.5|  V|3892.4|  V|3888.9|  V|3881.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3885.5|  V|3893.0|  V|3892.8|  V|3883.7|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 26|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             996|2022| 10| 21|   NaN|  N|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1115.2|  V|1123.7|  V|1126.0|  V|1126.0|  V|1126.0|  V|1128.9|  V|1137.6|  V|1133.0|  V|1133.7|  V|1138.7|  V|1112.6|  V|1101.1|  V|1106.9|  V|1113.6|  V|1114.0|  V|1114.0|  V|1114.7|  V|1124.2|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 16|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10|  4|1109.0|  V| 961.0|  V| 676.0|  V|1247.0|  V| 603.0|  V| 947.0|  V|1866.0|  V|3970.0|  V|5062.0|  V|2724.0|  V|2478.0|  V|2210.0|  V|2092.0|  V|1366.0|  V|1361.0|  V|1221.0|  V|1294.0|  V|1971.0|  V|2911.0|  V|2825.0|  V|1434.0|  V|1555.0|  V|1135.0|  V|1603.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 30|3873.0|  V|3888.3|  V|3885.0|  V|3885.0|  V|3883.7|  V|3876.5|  V|3873.0|  V|   NaN|  N|3883.1|  V|3876.0|  V|3873.0|  V|3873.0|  V|3873.0|  V|3871.5|  V|3862.5|  V|3861.0|  V|3861.0|  V|   NaN|  N|3875.0|  V|3873.0|  V|3873.0|  V|3870.9|  V|3863.4|  V|3861.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  5|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10|  7|   NaN|  N| 781.0|  V| 580.0|  V| 565.0|  V| 598.0|  V| 642.0|  V|1057.0|  V|1484.0|  V|1734.0|  V|1576.0|  V|1729.0|  V|1549.0|  V|1391.0|  V|2047.0|  V|1710.0|  V|1686.0|  V|2023.0|  V|1676.0|  V|2308.0|  V|1983.0|  V|1309.0|  V|1513.0|  V| 931.0|  V| 832.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10|  1|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.8|  V|3853.0|  V|3847.9|  V|3847.2|  V|3847.9|  V|3846.3|  V|3844.2|  V|3843.6|  V|3843.3|  V|3843.1|  V|3853.0|  V|3893.0|  V|3893.0|  V|3893.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 13|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.9|  V|3852.3|  V|3846.3|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3888.8|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3868.1|  V|3868.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 16|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 24|1816.0|  V|1310.0|  V|1025.0|  V| 588.0|  V| 590.0|  V| 443.0|  V| 430.0|  V| 734.0|  V|1792.0|  V|1898.0|  V| 674.0|  V| 616.0|  V| 636.0|  V| 857.0|  V|1151.0|  V| 986.0|  V|1129.0|  V| 609.0|  V| 486.0|  V| 652.0|  V| 538.0|  V| 599.0|  V| 534.0|  V| 419.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 30|5860.0|  V|3499.0|  V|2035.0|  V|1272.0|  V|1355.0|  V|2306.0|  V|2439.0|  V|   NaN|  N|1736.0|  V|1910.0|  V|1579.0|  V|1364.0|  V|1696.0|  V|1862.0|  V|2094.0|  V|1949.0|  V|1686.0|  V|   NaN|  N|2125.0|  V|2665.0|  V|3017.0|  V|2511.0|  V|2709.0|  V|2312.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 28|   NaN|  N|3885.0|  V|3884.3|  V|3880.1|  V|3874.4|  V|3873.0|  V|3873.0|  V|   NaN|  N|3849.1|  V|3846.3|  V|3837.2|  V|   NaN|  N|3873.0|  V|3870.1|  V|3861.8|  V|3861.0|  V|3861.0|  V|3859.4|  V|   NaN|  N|3863.4|  V|3861.0|  V|3859.9|  V|3851.2|  V|3849.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 19|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:06:52 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:06:56 WARN DAGScheduler: Broadcasting large task binary with size 1507.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: h02\n",
      "Q1: 1.99\n",
      "Q3: 39.0\n",
      "IQR: 37.01\n",
      "Lower Bound: -109.04\n",
      "Upper Bound: 150.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:06:59 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:07:02 WARN DAGScheduler: Broadcasting large task binary with size 1596.8 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 12517\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:07:06 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:07:09 WARN DAGScheduler: Broadcasting large task binary with size 1621.3 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|codi_provincia|provincia|codi_municipi| municipi|estacio|codi_contaminant| any|mes|dia|   h01|v01|   h02|v02|   h03|v03|   h04|v04|   h05|v05|   h06|v06|   h07|v07|   h08|v08|   h09|v09|   h10|v10|   h11|v11|   h12|v12|   h13|v13|   h14|v14|   h15|v15|   h16|v16|   h17|v17|   h18|v18|   h19|v19|   h20|v20|   h21|v21|   h22|v22|   h23|v23|   h24|v24|  month|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 11| 771.0|  V|1024.0|  V| 998.0|  V| 659.0|  V| 641.0|  V| 734.0|  V| 846.0|  V|1278.0|  V|1077.0|  V|1086.0|  V|1096.0|  V|1746.0|  V|1653.0|  V| 825.0|  V| 707.0|  V| 730.0|  V| 833.0|  V| 791.0|  V| 944.0|  V|1002.0|  V| 949.0|  V|1116.0|  V|1621.0|  V|5794.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 22|1146.0|  V| 915.0|  V| 471.0|  V| 288.0|  V| 215.0|  V| 256.0|  V| 521.0|  V| 458.0|  V| 419.0|  V| 374.0|  V| 479.0|  V| 449.0|  V| 392.0|  V| 338.0|  V| 502.0|  V| 575.0|  V| 353.0|  V| 365.0|  V| 512.0|  V|1238.0|  V|2378.0|  V|2081.0|  V|3206.0|  V|1822.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  3|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 20|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 24|1921.0|  V|1593.0|  V|2850.0|  V|1949.0|  V|1559.0|  V|1102.0|  V|1619.0|  V|3558.0|  V|4674.0|  V|2410.0|  V|1657.0|  V|1159.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|1549.0|  V|1891.0|  V| 750.0|  V| 996.0|  V|1629.0|  V|2417.0|  V|2799.0|  V|2117.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 18|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3864.8|  V|3886.1|  V|3893.0|  V|3893.0|  V|3892.5|  V|3892.4|  V|3888.9|  V|3881.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3885.5|  V|3893.0|  V|3892.8|  V|3883.7|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 26|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             996|2022| 10| 21|   NaN|  N|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1115.2|  V|1123.7|  V|1126.0|  V|1126.0|  V|1126.0|  V|1128.9|  V|1137.6|  V|1133.0|  V|1133.7|  V|1138.7|  V|1112.6|  V|1101.1|  V|1106.9|  V|1113.6|  V|1114.0|  V|1114.0|  V|1114.7|  V|1124.2|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 16|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10|  4|1109.0|  V| 961.0|  V| 676.0|  V|1247.0|  V| 603.0|  V| 947.0|  V|1866.0|  V|3970.0|  V|5062.0|  V|2724.0|  V|2478.0|  V|2210.0|  V|2092.0|  V|1366.0|  V|1361.0|  V|1221.0|  V|1294.0|  V|1971.0|  V|2911.0|  V|2825.0|  V|1434.0|  V|1555.0|  V|1135.0|  V|1603.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 30|3873.0|  V|3888.3|  V|3885.0|  V|3885.0|  V|3883.7|  V|3876.5|  V|3873.0|  V|   NaN|  N|3883.1|  V|3876.0|  V|3873.0|  V|3873.0|  V|3873.0|  V|3871.5|  V|3862.5|  V|3861.0|  V|3861.0|  V|   NaN|  N|3875.0|  V|3873.0|  V|3873.0|  V|3870.9|  V|3863.4|  V|3861.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  5|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10|  7|   NaN|  N| 781.0|  V| 580.0|  V| 565.0|  V| 598.0|  V| 642.0|  V|1057.0|  V|1484.0|  V|1734.0|  V|1576.0|  V|1729.0|  V|1549.0|  V|1391.0|  V|2047.0|  V|1710.0|  V|1686.0|  V|2023.0|  V|1676.0|  V|2308.0|  V|1983.0|  V|1309.0|  V|1513.0|  V| 931.0|  V| 832.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10|  1|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.8|  V|3853.0|  V|3847.9|  V|3847.2|  V|3847.9|  V|3846.3|  V|3844.2|  V|3843.6|  V|3843.3|  V|3843.1|  V|3853.0|  V|3893.0|  V|3893.0|  V|3893.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 13|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.9|  V|3852.3|  V|3846.3|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3888.8|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3868.1|  V|3868.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 16|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 24|1816.0|  V|1310.0|  V|1025.0|  V| 588.0|  V| 590.0|  V| 443.0|  V| 430.0|  V| 734.0|  V|1792.0|  V|1898.0|  V| 674.0|  V| 616.0|  V| 636.0|  V| 857.0|  V|1151.0|  V| 986.0|  V|1129.0|  V| 609.0|  V| 486.0|  V| 652.0|  V| 538.0|  V| 599.0|  V| 534.0|  V| 419.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 30|5860.0|  V|3499.0|  V|2035.0|  V|1272.0|  V|1355.0|  V|2306.0|  V|2439.0|  V|   NaN|  N|1736.0|  V|1910.0|  V|1579.0|  V|1364.0|  V|1696.0|  V|1862.0|  V|2094.0|  V|1949.0|  V|1686.0|  V|   NaN|  N|2125.0|  V|2665.0|  V|3017.0|  V|2511.0|  V|2709.0|  V|2312.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 28|   NaN|  N|3885.0|  V|3884.3|  V|3880.1|  V|3874.4|  V|3873.0|  V|3873.0|  V|   NaN|  N|3849.1|  V|3846.3|  V|3837.2|  V|   NaN|  N|3873.0|  V|3870.1|  V|3861.8|  V|3861.0|  V|3861.0|  V|3859.4|  V|   NaN|  N|3863.4|  V|3861.0|  V|3859.9|  V|3851.2|  V|3849.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 19|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:07:12 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:07:16 WARN DAGScheduler: Broadcasting large task binary with size 1507.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: h03\n",
      "Q1: 2.0\n",
      "Q3: 37.0\n",
      "IQR: 35.0\n",
      "Lower Bound: -103.0\n",
      "Upper Bound: 142.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:07:19 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:07:22 WARN DAGScheduler: Broadcasting large task binary with size 1596.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 12273\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:07:25 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:07:28 WARN DAGScheduler: Broadcasting large task binary with size 1621.0 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|codi_provincia|provincia|codi_municipi| municipi|estacio|codi_contaminant| any|mes|dia|   h01|v01|   h02|v02|   h03|v03|   h04|v04|   h05|v05|   h06|v06|   h07|v07|   h08|v08|   h09|v09|   h10|v10|   h11|v11|   h12|v12|   h13|v13|   h14|v14|   h15|v15|   h16|v16|   h17|v17|   h18|v18|   h19|v19|   h20|v20|   h21|v21|   h22|v22|   h23|v23|   h24|v24|  month|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|             8|Barcelona|           19|Barcelona|     43|              14|2022| 10| 30|   2.0|  V|  18.0|  V|   NaN|  N|  43.0|  V|  43.0|  V|  18.0|  V|  20.0|  V|  21.0|  V|  34.0|  V|  37.0|  V|  59.0|  V|  64.0|  V|  64.0|  V|  56.0|  V|  40.0|  V|  49.0|  V|  35.0|  V|  13.0|  V|  44.0|  V|  40.0|  V|  39.0|  V|  40.0|  V|  31.0|  V|  23.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 11| 771.0|  V|1024.0|  V| 998.0|  V| 659.0|  V| 641.0|  V| 734.0|  V| 846.0|  V|1278.0|  V|1077.0|  V|1086.0|  V|1096.0|  V|1746.0|  V|1653.0|  V| 825.0|  V| 707.0|  V| 730.0|  V| 833.0|  V| 791.0|  V| 944.0|  V|1002.0|  V| 949.0|  V|1116.0|  V|1621.0|  V|5794.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 22|1146.0|  V| 915.0|  V| 471.0|  V| 288.0|  V| 215.0|  V| 256.0|  V| 521.0|  V| 458.0|  V| 419.0|  V| 374.0|  V| 479.0|  V| 449.0|  V| 392.0|  V| 338.0|  V| 502.0|  V| 575.0|  V| 353.0|  V| 365.0|  V| 512.0|  V|1238.0|  V|2378.0|  V|2081.0|  V|3206.0|  V|1822.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  3|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 20|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 24|1921.0|  V|1593.0|  V|2850.0|  V|1949.0|  V|1559.0|  V|1102.0|  V|1619.0|  V|3558.0|  V|4674.0|  V|2410.0|  V|1657.0|  V|1159.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|1549.0|  V|1891.0|  V| 750.0|  V| 996.0|  V|1629.0|  V|2417.0|  V|2799.0|  V|2117.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 18|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3864.8|  V|3886.1|  V|3893.0|  V|3893.0|  V|3892.5|  V|3892.4|  V|3888.9|  V|3881.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3885.5|  V|3893.0|  V|3892.8|  V|3883.7|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 26|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             996|2022| 10| 21|   NaN|  N|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1115.2|  V|1123.7|  V|1126.0|  V|1126.0|  V|1126.0|  V|1128.9|  V|1137.6|  V|1133.0|  V|1133.7|  V|1138.7|  V|1112.6|  V|1101.1|  V|1106.9|  V|1113.6|  V|1114.0|  V|1114.0|  V|1114.7|  V|1124.2|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 16|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10|  4|1109.0|  V| 961.0|  V| 676.0|  V|1247.0|  V| 603.0|  V| 947.0|  V|1866.0|  V|3970.0|  V|5062.0|  V|2724.0|  V|2478.0|  V|2210.0|  V|2092.0|  V|1366.0|  V|1361.0|  V|1221.0|  V|1294.0|  V|1971.0|  V|2911.0|  V|2825.0|  V|1434.0|  V|1555.0|  V|1135.0|  V|1603.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 30|3873.0|  V|3888.3|  V|3885.0|  V|3885.0|  V|3883.7|  V|3876.5|  V|3873.0|  V|   NaN|  N|3883.1|  V|3876.0|  V|3873.0|  V|3873.0|  V|3873.0|  V|3871.5|  V|3862.5|  V|3861.0|  V|3861.0|  V|   NaN|  N|3875.0|  V|3873.0|  V|3873.0|  V|3870.9|  V|3863.4|  V|3861.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  5|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10|  7|   NaN|  N| 781.0|  V| 580.0|  V| 565.0|  V| 598.0|  V| 642.0|  V|1057.0|  V|1484.0|  V|1734.0|  V|1576.0|  V|1729.0|  V|1549.0|  V|1391.0|  V|2047.0|  V|1710.0|  V|1686.0|  V|2023.0|  V|1676.0|  V|2308.0|  V|1983.0|  V|1309.0|  V|1513.0|  V| 931.0|  V| 832.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10|  1|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.8|  V|3853.0|  V|3847.9|  V|3847.2|  V|3847.9|  V|3846.3|  V|3844.2|  V|3843.6|  V|3843.3|  V|3843.1|  V|3853.0|  V|3893.0|  V|3893.0|  V|3893.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 13|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.9|  V|3852.3|  V|3846.3|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3888.8|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3868.1|  V|3868.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 16|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 24|1816.0|  V|1310.0|  V|1025.0|  V| 588.0|  V| 590.0|  V| 443.0|  V| 430.0|  V| 734.0|  V|1792.0|  V|1898.0|  V| 674.0|  V| 616.0|  V| 636.0|  V| 857.0|  V|1151.0|  V| 986.0|  V|1129.0|  V| 609.0|  V| 486.0|  V| 652.0|  V| 538.0|  V| 599.0|  V| 534.0|  V| 419.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 30|5860.0|  V|3499.0|  V|2035.0|  V|1272.0|  V|1355.0|  V|2306.0|  V|2439.0|  V|   NaN|  N|1736.0|  V|1910.0|  V|1579.0|  V|1364.0|  V|1696.0|  V|1862.0|  V|2094.0|  V|1949.0|  V|1686.0|  V|   NaN|  N|2125.0|  V|2665.0|  V|3017.0|  V|2511.0|  V|2709.0|  V|2312.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 28|   NaN|  N|3885.0|  V|3884.3|  V|3880.1|  V|3874.4|  V|3873.0|  V|3873.0|  V|   NaN|  N|3849.1|  V|3846.3|  V|3837.2|  V|   NaN|  N|3873.0|  V|3870.1|  V|3861.8|  V|3861.0|  V|3861.0|  V|3859.4|  V|   NaN|  N|3863.4|  V|3861.0|  V|3859.9|  V|3851.2|  V|3849.0|  V|2022_10|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:07:31 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:07:35 WARN DAGScheduler: Broadcasting large task binary with size 1507.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: h04\n",
      "Q1: 2.0\n",
      "Q3: 35.0\n",
      "IQR: 33.0\n",
      "Lower Bound: -97.0\n",
      "Upper Bound: 134.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:07:38 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:07:42 WARN DAGScheduler: Broadcasting large task binary with size 1596.5 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 12023\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:07:45 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:07:48 WARN DAGScheduler: Broadcasting large task binary with size 1620.9 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|codi_provincia|provincia|codi_municipi| municipi|estacio|codi_contaminant| any|mes|dia|   h01|v01|   h02|v02|   h03|v03|   h04|v04|   h05|v05|   h06|v06|   h07|v07|   h08|v08|   h09|v09|   h10|v10|   h11|v11|   h12|v12|   h13|v13|   h14|v14|   h15|v15|   h16|v16|   h17|v17|   h18|v18|   h19|v19|   h20|v20|   h21|v21|   h22|v22|   h23|v23|   h24|v24|  month|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 11| 771.0|  V|1024.0|  V| 998.0|  V| 659.0|  V| 641.0|  V| 734.0|  V| 846.0|  V|1278.0|  V|1077.0|  V|1086.0|  V|1096.0|  V|1746.0|  V|1653.0|  V| 825.0|  V| 707.0|  V| 730.0|  V| 833.0|  V| 791.0|  V| 944.0|  V|1002.0|  V| 949.0|  V|1116.0|  V|1621.0|  V|5794.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 22|1146.0|  V| 915.0|  V| 471.0|  V| 288.0|  V| 215.0|  V| 256.0|  V| 521.0|  V| 458.0|  V| 419.0|  V| 374.0|  V| 479.0|  V| 449.0|  V| 392.0|  V| 338.0|  V| 502.0|  V| 575.0|  V| 353.0|  V| 365.0|  V| 512.0|  V|1238.0|  V|2378.0|  V|2081.0|  V|3206.0|  V|1822.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  3|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 20|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 24|1921.0|  V|1593.0|  V|2850.0|  V|1949.0|  V|1559.0|  V|1102.0|  V|1619.0|  V|3558.0|  V|4674.0|  V|2410.0|  V|1657.0|  V|1159.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|1549.0|  V|1891.0|  V| 750.0|  V| 996.0|  V|1629.0|  V|2417.0|  V|2799.0|  V|2117.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 18|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3864.8|  V|3886.1|  V|3893.0|  V|3893.0|  V|3892.5|  V|3892.4|  V|3888.9|  V|3881.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3885.5|  V|3893.0|  V|3892.8|  V|3883.7|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 26|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             996|2022| 10| 21|   NaN|  N|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1115.2|  V|1123.7|  V|1126.0|  V|1126.0|  V|1126.0|  V|1128.9|  V|1137.6|  V|1133.0|  V|1133.7|  V|1138.7|  V|1112.6|  V|1101.1|  V|1106.9|  V|1113.6|  V|1114.0|  V|1114.0|  V|1114.7|  V|1124.2|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 16|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10|  4|1109.0|  V| 961.0|  V| 676.0|  V|1247.0|  V| 603.0|  V| 947.0|  V|1866.0|  V|3970.0|  V|5062.0|  V|2724.0|  V|2478.0|  V|2210.0|  V|2092.0|  V|1366.0|  V|1361.0|  V|1221.0|  V|1294.0|  V|1971.0|  V|2911.0|  V|2825.0|  V|1434.0|  V|1555.0|  V|1135.0|  V|1603.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 30|3873.0|  V|3888.3|  V|3885.0|  V|3885.0|  V|3883.7|  V|3876.5|  V|3873.0|  V|   NaN|  N|3883.1|  V|3876.0|  V|3873.0|  V|3873.0|  V|3873.0|  V|3871.5|  V|3862.5|  V|3861.0|  V|3861.0|  V|   NaN|  N|3875.0|  V|3873.0|  V|3873.0|  V|3870.9|  V|3863.4|  V|3861.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  5|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10|  7|   NaN|  N| 781.0|  V| 580.0|  V| 565.0|  V| 598.0|  V| 642.0|  V|1057.0|  V|1484.0|  V|1734.0|  V|1576.0|  V|1729.0|  V|1549.0|  V|1391.0|  V|2047.0|  V|1710.0|  V|1686.0|  V|2023.0|  V|1676.0|  V|2308.0|  V|1983.0|  V|1309.0|  V|1513.0|  V| 931.0|  V| 832.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10|  1|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.8|  V|3853.0|  V|3847.9|  V|3847.2|  V|3847.9|  V|3846.3|  V|3844.2|  V|3843.6|  V|3843.3|  V|3843.1|  V|3853.0|  V|3893.0|  V|3893.0|  V|3893.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 13|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.9|  V|3852.3|  V|3846.3|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3888.8|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3868.1|  V|3868.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 16|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 24|1816.0|  V|1310.0|  V|1025.0|  V| 588.0|  V| 590.0|  V| 443.0|  V| 430.0|  V| 734.0|  V|1792.0|  V|1898.0|  V| 674.0|  V| 616.0|  V| 636.0|  V| 857.0|  V|1151.0|  V| 986.0|  V|1129.0|  V| 609.0|  V| 486.0|  V| 652.0|  V| 538.0|  V| 599.0|  V| 534.0|  V| 419.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 30|5860.0|  V|3499.0|  V|2035.0|  V|1272.0|  V|1355.0|  V|2306.0|  V|2439.0|  V|   NaN|  N|1736.0|  V|1910.0|  V|1579.0|  V|1364.0|  V|1696.0|  V|1862.0|  V|2094.0|  V|1949.0|  V|1686.0|  V|   NaN|  N|2125.0|  V|2665.0|  V|3017.0|  V|2511.0|  V|2709.0|  V|2312.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 28|   NaN|  N|3885.0|  V|3884.3|  V|3880.1|  V|3874.4|  V|3873.0|  V|3873.0|  V|   NaN|  N|3849.1|  V|3846.3|  V|3837.2|  V|   NaN|  N|3873.0|  V|3870.1|  V|3861.8|  V|3861.0|  V|3861.0|  V|3859.4|  V|   NaN|  N|3863.4|  V|3861.0|  V|3859.9|  V|3851.2|  V|3849.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 19|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:07:51 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:07:56 WARN DAGScheduler: Broadcasting large task binary with size 1507.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: h05\n",
      "Q1: 1.0\n",
      "Q3: 33.0\n",
      "IQR: 32.0\n",
      "Lower Bound: -95.0\n",
      "Upper Bound: 129.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:07:59 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:08:02 WARN DAGScheduler: Broadcasting large task binary with size 1596.5 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 12009\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:08:05 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:08:08 WARN DAGScheduler: Broadcasting large task binary with size 1620.9 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|codi_provincia|provincia|codi_municipi| municipi|estacio|codi_contaminant| any|mes|dia|   h01|v01|   h02|v02|   h03|v03|   h04|v04|   h05|v05|   h06|v06|   h07|v07|   h08|v08|   h09|v09|   h10|v10|   h11|v11|   h12|v12|   h13|v13|   h14|v14|   h15|v15|   h16|v16|   h17|v17|   h18|v18|   h19|v19|   h20|v20|   h21|v21|   h22|v22|   h23|v23|   h24|v24|  month|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 11| 771.0|  V|1024.0|  V| 998.0|  V| 659.0|  V| 641.0|  V| 734.0|  V| 846.0|  V|1278.0|  V|1077.0|  V|1086.0|  V|1096.0|  V|1746.0|  V|1653.0|  V| 825.0|  V| 707.0|  V| 730.0|  V| 833.0|  V| 791.0|  V| 944.0|  V|1002.0|  V| 949.0|  V|1116.0|  V|1621.0|  V|5794.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 22|1146.0|  V| 915.0|  V| 471.0|  V| 288.0|  V| 215.0|  V| 256.0|  V| 521.0|  V| 458.0|  V| 419.0|  V| 374.0|  V| 479.0|  V| 449.0|  V| 392.0|  V| 338.0|  V| 502.0|  V| 575.0|  V| 353.0|  V| 365.0|  V| 512.0|  V|1238.0|  V|2378.0|  V|2081.0|  V|3206.0|  V|1822.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  3|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 20|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 24|1921.0|  V|1593.0|  V|2850.0|  V|1949.0|  V|1559.0|  V|1102.0|  V|1619.0|  V|3558.0|  V|4674.0|  V|2410.0|  V|1657.0|  V|1159.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|1549.0|  V|1891.0|  V| 750.0|  V| 996.0|  V|1629.0|  V|2417.0|  V|2799.0|  V|2117.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 18|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3864.8|  V|3886.1|  V|3893.0|  V|3893.0|  V|3892.5|  V|3892.4|  V|3888.9|  V|3881.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3885.5|  V|3893.0|  V|3892.8|  V|3883.7|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 26|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             996|2022| 10| 21|   NaN|  N|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1115.2|  V|1123.7|  V|1126.0|  V|1126.0|  V|1126.0|  V|1128.9|  V|1137.6|  V|1133.0|  V|1133.7|  V|1138.7|  V|1112.6|  V|1101.1|  V|1106.9|  V|1113.6|  V|1114.0|  V|1114.0|  V|1114.7|  V|1124.2|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 16|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10|  4|1109.0|  V| 961.0|  V| 676.0|  V|1247.0|  V| 603.0|  V| 947.0|  V|1866.0|  V|3970.0|  V|5062.0|  V|2724.0|  V|2478.0|  V|2210.0|  V|2092.0|  V|1366.0|  V|1361.0|  V|1221.0|  V|1294.0|  V|1971.0|  V|2911.0|  V|2825.0|  V|1434.0|  V|1555.0|  V|1135.0|  V|1603.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 30|3873.0|  V|3888.3|  V|3885.0|  V|3885.0|  V|3883.7|  V|3876.5|  V|3873.0|  V|   NaN|  N|3883.1|  V|3876.0|  V|3873.0|  V|3873.0|  V|3873.0|  V|3871.5|  V|3862.5|  V|3861.0|  V|3861.0|  V|   NaN|  N|3875.0|  V|3873.0|  V|3873.0|  V|3870.9|  V|3863.4|  V|3861.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  5|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10|  7|   NaN|  N| 781.0|  V| 580.0|  V| 565.0|  V| 598.0|  V| 642.0|  V|1057.0|  V|1484.0|  V|1734.0|  V|1576.0|  V|1729.0|  V|1549.0|  V|1391.0|  V|2047.0|  V|1710.0|  V|1686.0|  V|2023.0|  V|1676.0|  V|2308.0|  V|1983.0|  V|1309.0|  V|1513.0|  V| 931.0|  V| 832.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10|  1|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.8|  V|3853.0|  V|3847.9|  V|3847.2|  V|3847.9|  V|3846.3|  V|3844.2|  V|3843.6|  V|3843.3|  V|3843.1|  V|3853.0|  V|3893.0|  V|3893.0|  V|3893.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 13|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.9|  V|3852.3|  V|3846.3|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3888.8|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3868.1|  V|3868.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 16|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 24|1816.0|  V|1310.0|  V|1025.0|  V| 588.0|  V| 590.0|  V| 443.0|  V| 430.0|  V| 734.0|  V|1792.0|  V|1898.0|  V| 674.0|  V| 616.0|  V| 636.0|  V| 857.0|  V|1151.0|  V| 986.0|  V|1129.0|  V| 609.0|  V| 486.0|  V| 652.0|  V| 538.0|  V| 599.0|  V| 534.0|  V| 419.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 30|5860.0|  V|3499.0|  V|2035.0|  V|1272.0|  V|1355.0|  V|2306.0|  V|2439.0|  V|   NaN|  N|1736.0|  V|1910.0|  V|1579.0|  V|1364.0|  V|1696.0|  V|1862.0|  V|2094.0|  V|1949.0|  V|1686.0|  V|   NaN|  N|2125.0|  V|2665.0|  V|3017.0|  V|2511.0|  V|2709.0|  V|2312.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 28|   NaN|  N|3885.0|  V|3884.3|  V|3880.1|  V|3874.4|  V|3873.0|  V|3873.0|  V|   NaN|  N|3849.1|  V|3846.3|  V|3837.2|  V|   NaN|  N|3873.0|  V|3870.1|  V|3861.8|  V|3861.0|  V|3861.0|  V|3859.4|  V|   NaN|  N|3863.4|  V|3861.0|  V|3859.9|  V|3851.2|  V|3849.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 19|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:08:11 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:08:15 WARN DAGScheduler: Broadcasting large task binary with size 1507.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: h06\n",
      "Q1: 2.0\n",
      "Q3: 33.0\n",
      "IQR: 31.0\n",
      "Lower Bound: -91.0\n",
      "Upper Bound: 126.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:08:18 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:08:21 WARN DAGScheduler: Broadcasting large task binary with size 1596.5 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 12057\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:08:24 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:08:27 WARN DAGScheduler: Broadcasting large task binary with size 1620.9 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|codi_provincia|provincia|codi_municipi| municipi|estacio|codi_contaminant| any|mes|dia|   h01|v01|   h02|v02|   h03|v03|   h04|v04|   h05|v05|   h06|v06|   h07|v07|   h08|v08|   h09|v09|   h10|v10|   h11|v11|   h12|v12|   h13|v13|   h14|v14|   h15|v15|   h16|v16|   h17|v17|   h18|v18|   h19|v19|   h20|v20|   h21|v21|   h22|v22|   h23|v23|   h24|v24|  month|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 11| 771.0|  V|1024.0|  V| 998.0|  V| 659.0|  V| 641.0|  V| 734.0|  V| 846.0|  V|1278.0|  V|1077.0|  V|1086.0|  V|1096.0|  V|1746.0|  V|1653.0|  V| 825.0|  V| 707.0|  V| 730.0|  V| 833.0|  V| 791.0|  V| 944.0|  V|1002.0|  V| 949.0|  V|1116.0|  V|1621.0|  V|5794.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 22|1146.0|  V| 915.0|  V| 471.0|  V| 288.0|  V| 215.0|  V| 256.0|  V| 521.0|  V| 458.0|  V| 419.0|  V| 374.0|  V| 479.0|  V| 449.0|  V| 392.0|  V| 338.0|  V| 502.0|  V| 575.0|  V| 353.0|  V| 365.0|  V| 512.0|  V|1238.0|  V|2378.0|  V|2081.0|  V|3206.0|  V|1822.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  3|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 20|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 24|1921.0|  V|1593.0|  V|2850.0|  V|1949.0|  V|1559.0|  V|1102.0|  V|1619.0|  V|3558.0|  V|4674.0|  V|2410.0|  V|1657.0|  V|1159.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|1549.0|  V|1891.0|  V| 750.0|  V| 996.0|  V|1629.0|  V|2417.0|  V|2799.0|  V|2117.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 18|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3864.8|  V|3886.1|  V|3893.0|  V|3893.0|  V|3892.5|  V|3892.4|  V|3888.9|  V|3881.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3885.5|  V|3893.0|  V|3892.8|  V|3883.7|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 26|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             996|2022| 10| 21|   NaN|  N|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1115.2|  V|1123.7|  V|1126.0|  V|1126.0|  V|1126.0|  V|1128.9|  V|1137.6|  V|1133.0|  V|1133.7|  V|1138.7|  V|1112.6|  V|1101.1|  V|1106.9|  V|1113.6|  V|1114.0|  V|1114.0|  V|1114.7|  V|1124.2|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 16|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10|  4|1109.0|  V| 961.0|  V| 676.0|  V|1247.0|  V| 603.0|  V| 947.0|  V|1866.0|  V|3970.0|  V|5062.0|  V|2724.0|  V|2478.0|  V|2210.0|  V|2092.0|  V|1366.0|  V|1361.0|  V|1221.0|  V|1294.0|  V|1971.0|  V|2911.0|  V|2825.0|  V|1434.0|  V|1555.0|  V|1135.0|  V|1603.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 30|3873.0|  V|3888.3|  V|3885.0|  V|3885.0|  V|3883.7|  V|3876.5|  V|3873.0|  V|   NaN|  N|3883.1|  V|3876.0|  V|3873.0|  V|3873.0|  V|3873.0|  V|3871.5|  V|3862.5|  V|3861.0|  V|3861.0|  V|   NaN|  N|3875.0|  V|3873.0|  V|3873.0|  V|3870.9|  V|3863.4|  V|3861.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  5|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10|  7|   NaN|  N| 781.0|  V| 580.0|  V| 565.0|  V| 598.0|  V| 642.0|  V|1057.0|  V|1484.0|  V|1734.0|  V|1576.0|  V|1729.0|  V|1549.0|  V|1391.0|  V|2047.0|  V|1710.0|  V|1686.0|  V|2023.0|  V|1676.0|  V|2308.0|  V|1983.0|  V|1309.0|  V|1513.0|  V| 931.0|  V| 832.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10|  1|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.8|  V|3853.0|  V|3847.9|  V|3847.2|  V|3847.9|  V|3846.3|  V|3844.2|  V|3843.6|  V|3843.3|  V|3843.1|  V|3853.0|  V|3893.0|  V|3893.0|  V|3893.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 13|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.9|  V|3852.3|  V|3846.3|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3888.8|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3868.1|  V|3868.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 16|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 24|1816.0|  V|1310.0|  V|1025.0|  V| 588.0|  V| 590.0|  V| 443.0|  V| 430.0|  V| 734.0|  V|1792.0|  V|1898.0|  V| 674.0|  V| 616.0|  V| 636.0|  V| 857.0|  V|1151.0|  V| 986.0|  V|1129.0|  V| 609.0|  V| 486.0|  V| 652.0|  V| 538.0|  V| 599.0|  V| 534.0|  V| 419.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 30|5860.0|  V|3499.0|  V|2035.0|  V|1272.0|  V|1355.0|  V|2306.0|  V|2439.0|  V|   NaN|  N|1736.0|  V|1910.0|  V|1579.0|  V|1364.0|  V|1696.0|  V|1862.0|  V|2094.0|  V|1949.0|  V|1686.0|  V|   NaN|  N|2125.0|  V|2665.0|  V|3017.0|  V|2511.0|  V|2709.0|  V|2312.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 28|   NaN|  N|3885.0|  V|3884.3|  V|3880.1|  V|3874.4|  V|3873.0|  V|3873.0|  V|   NaN|  N|3849.1|  V|3846.3|  V|3837.2|  V|   NaN|  N|3873.0|  V|3870.1|  V|3861.8|  V|3861.0|  V|3861.0|  V|3859.4|  V|   NaN|  N|3863.4|  V|3861.0|  V|3859.9|  V|3851.2|  V|3849.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 19|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:08:30 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:08:35 WARN DAGScheduler: Broadcasting large task binary with size 1507.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: h07\n",
      "Q1: 2.0\n",
      "Q3: 38.0\n",
      "IQR: 36.0\n",
      "Lower Bound: -106.0\n",
      "Upper Bound: 146.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:08:37 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:08:41 WARN DAGScheduler: Broadcasting large task binary with size 1596.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 12076\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:08:44 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:08:47 WARN DAGScheduler: Broadcasting large task binary with size 1621.0 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|codi_provincia|provincia|codi_municipi| municipi|estacio|codi_contaminant| any|mes|dia|   h01|v01|   h02|v02|   h03|v03|   h04|v04|   h05|v05|   h06|v06|   h07|v07|   h08|v08|   h09|v09|   h10|v10|   h11|v11|   h12|v12|   h13|v13|   h14|v14|   h15|v15|   h16|v16|   h17|v17|   h18|v18|   h19|v19|   h20|v20|   h21|v21|   h22|v22|   h23|v23|   h24|v24|  month|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 11| 771.0|  V|1024.0|  V| 998.0|  V| 659.0|  V| 641.0|  V| 734.0|  V| 846.0|  V|1278.0|  V|1077.0|  V|1086.0|  V|1096.0|  V|1746.0|  V|1653.0|  V| 825.0|  V| 707.0|  V| 730.0|  V| 833.0|  V| 791.0|  V| 944.0|  V|1002.0|  V| 949.0|  V|1116.0|  V|1621.0|  V|5794.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 22|1146.0|  V| 915.0|  V| 471.0|  V| 288.0|  V| 215.0|  V| 256.0|  V| 521.0|  V| 458.0|  V| 419.0|  V| 374.0|  V| 479.0|  V| 449.0|  V| 392.0|  V| 338.0|  V| 502.0|  V| 575.0|  V| 353.0|  V| 365.0|  V| 512.0|  V|1238.0|  V|2378.0|  V|2081.0|  V|3206.0|  V|1822.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  3|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 20|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 24|1921.0|  V|1593.0|  V|2850.0|  V|1949.0|  V|1559.0|  V|1102.0|  V|1619.0|  V|3558.0|  V|4674.0|  V|2410.0|  V|1657.0|  V|1159.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|1549.0|  V|1891.0|  V| 750.0|  V| 996.0|  V|1629.0|  V|2417.0|  V|2799.0|  V|2117.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 18|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3864.8|  V|3886.1|  V|3893.0|  V|3893.0|  V|3892.5|  V|3892.4|  V|3888.9|  V|3881.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3885.5|  V|3893.0|  V|3892.8|  V|3883.7|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 26|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             996|2022| 10| 21|   NaN|  N|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1115.2|  V|1123.7|  V|1126.0|  V|1126.0|  V|1126.0|  V|1128.9|  V|1137.6|  V|1133.0|  V|1133.7|  V|1138.7|  V|1112.6|  V|1101.1|  V|1106.9|  V|1113.6|  V|1114.0|  V|1114.0|  V|1114.7|  V|1124.2|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 16|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10|  4|1109.0|  V| 961.0|  V| 676.0|  V|1247.0|  V| 603.0|  V| 947.0|  V|1866.0|  V|3970.0|  V|5062.0|  V|2724.0|  V|2478.0|  V|2210.0|  V|2092.0|  V|1366.0|  V|1361.0|  V|1221.0|  V|1294.0|  V|1971.0|  V|2911.0|  V|2825.0|  V|1434.0|  V|1555.0|  V|1135.0|  V|1603.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 30|3873.0|  V|3888.3|  V|3885.0|  V|3885.0|  V|3883.7|  V|3876.5|  V|3873.0|  V|   NaN|  N|3883.1|  V|3876.0|  V|3873.0|  V|3873.0|  V|3873.0|  V|3871.5|  V|3862.5|  V|3861.0|  V|3861.0|  V|   NaN|  N|3875.0|  V|3873.0|  V|3873.0|  V|3870.9|  V|3863.4|  V|3861.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  5|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10|  7|   NaN|  N| 781.0|  V| 580.0|  V| 565.0|  V| 598.0|  V| 642.0|  V|1057.0|  V|1484.0|  V|1734.0|  V|1576.0|  V|1729.0|  V|1549.0|  V|1391.0|  V|2047.0|  V|1710.0|  V|1686.0|  V|2023.0|  V|1676.0|  V|2308.0|  V|1983.0|  V|1309.0|  V|1513.0|  V| 931.0|  V| 832.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10|  1|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.8|  V|3853.0|  V|3847.9|  V|3847.2|  V|3847.9|  V|3846.3|  V|3844.2|  V|3843.6|  V|3843.3|  V|3843.1|  V|3853.0|  V|3893.0|  V|3893.0|  V|3893.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 13|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.9|  V|3852.3|  V|3846.3|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3888.8|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3868.1|  V|3868.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 16|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 24|1816.0|  V|1310.0|  V|1025.0|  V| 588.0|  V| 590.0|  V| 443.0|  V| 430.0|  V| 734.0|  V|1792.0|  V|1898.0|  V| 674.0|  V| 616.0|  V| 636.0|  V| 857.0|  V|1151.0|  V| 986.0|  V|1129.0|  V| 609.0|  V| 486.0|  V| 652.0|  V| 538.0|  V| 599.0|  V| 534.0|  V| 419.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 30|5860.0|  V|3499.0|  V|2035.0|  V|1272.0|  V|1355.0|  V|2306.0|  V|2439.0|  V|   NaN|  N|1736.0|  V|1910.0|  V|1579.0|  V|1364.0|  V|1696.0|  V|1862.0|  V|2094.0|  V|1949.0|  V|1686.0|  V|   NaN|  N|2125.0|  V|2665.0|  V|3017.0|  V|2511.0|  V|2709.0|  V|2312.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 28|   NaN|  N|3885.0|  V|3884.3|  V|3880.1|  V|3874.4|  V|3873.0|  V|3873.0|  V|   NaN|  N|3849.1|  V|3846.3|  V|3837.2|  V|   NaN|  N|3873.0|  V|3870.1|  V|3861.8|  V|3861.0|  V|3861.0|  V|3859.4|  V|   NaN|  N|3863.4|  V|3861.0|  V|3859.9|  V|3851.2|  V|3849.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 19|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:08:50 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:08:54 WARN DAGScheduler: Broadcasting large task binary with size 1507.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: h08\n",
      "Q1: 2.29\n",
      "Q3: 48.0\n",
      "IQR: 45.71\n",
      "Lower Bound: -134.84\n",
      "Upper Bound: 185.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:08:57 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:09:00 WARN DAGScheduler: Broadcasting large task binary with size 1596.8 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 12296\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:09:03 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:09:06 WARN DAGScheduler: Broadcasting large task binary with size 1621.3 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|codi_provincia|provincia|codi_municipi| municipi|estacio|codi_contaminant| any|mes|dia|   h01|v01|   h02|v02|   h03|v03|   h04|v04|   h05|v05|   h06|v06|   h07|v07|   h08|v08|   h09|v09|   h10|v10|   h11|v11|   h12|v12|   h13|v13|   h14|v14|   h15|v15|   h16|v16|   h17|v17|   h18|v18|   h19|v19|   h20|v20|   h21|v21|   h22|v22|   h23|v23|   h24|v24|  month|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 11| 771.0|  V|1024.0|  V| 998.0|  V| 659.0|  V| 641.0|  V| 734.0|  V| 846.0|  V|1278.0|  V|1077.0|  V|1086.0|  V|1096.0|  V|1746.0|  V|1653.0|  V| 825.0|  V| 707.0|  V| 730.0|  V| 833.0|  V| 791.0|  V| 944.0|  V|1002.0|  V| 949.0|  V|1116.0|  V|1621.0|  V|5794.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 22|1146.0|  V| 915.0|  V| 471.0|  V| 288.0|  V| 215.0|  V| 256.0|  V| 521.0|  V| 458.0|  V| 419.0|  V| 374.0|  V| 479.0|  V| 449.0|  V| 392.0|  V| 338.0|  V| 502.0|  V| 575.0|  V| 353.0|  V| 365.0|  V| 512.0|  V|1238.0|  V|2378.0|  V|2081.0|  V|3206.0|  V|1822.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  3|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 20|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 24|1921.0|  V|1593.0|  V|2850.0|  V|1949.0|  V|1559.0|  V|1102.0|  V|1619.0|  V|3558.0|  V|4674.0|  V|2410.0|  V|1657.0|  V|1159.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|1549.0|  V|1891.0|  V| 750.0|  V| 996.0|  V|1629.0|  V|2417.0|  V|2799.0|  V|2117.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 18|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3864.8|  V|3886.1|  V|3893.0|  V|3893.0|  V|3892.5|  V|3892.4|  V|3888.9|  V|3881.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3885.5|  V|3893.0|  V|3892.8|  V|3883.7|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 26|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             996|2022| 10| 21|   NaN|  N|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1115.2|  V|1123.7|  V|1126.0|  V|1126.0|  V|1126.0|  V|1128.9|  V|1137.6|  V|1133.0|  V|1133.7|  V|1138.7|  V|1112.6|  V|1101.1|  V|1106.9|  V|1113.6|  V|1114.0|  V|1114.0|  V|1114.7|  V|1124.2|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 16|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10|  4|1109.0|  V| 961.0|  V| 676.0|  V|1247.0|  V| 603.0|  V| 947.0|  V|1866.0|  V|3970.0|  V|5062.0|  V|2724.0|  V|2478.0|  V|2210.0|  V|2092.0|  V|1366.0|  V|1361.0|  V|1221.0|  V|1294.0|  V|1971.0|  V|2911.0|  V|2825.0|  V|1434.0|  V|1555.0|  V|1135.0|  V|1603.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 30|3873.0|  V|3888.3|  V|3885.0|  V|3885.0|  V|3883.7|  V|3876.5|  V|3873.0|  V|   NaN|  N|3883.1|  V|3876.0|  V|3873.0|  V|3873.0|  V|3873.0|  V|3871.5|  V|3862.5|  V|3861.0|  V|3861.0|  V|   NaN|  N|3875.0|  V|3873.0|  V|3873.0|  V|3870.9|  V|3863.4|  V|3861.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  5|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10|  7|   NaN|  N| 781.0|  V| 580.0|  V| 565.0|  V| 598.0|  V| 642.0|  V|1057.0|  V|1484.0|  V|1734.0|  V|1576.0|  V|1729.0|  V|1549.0|  V|1391.0|  V|2047.0|  V|1710.0|  V|1686.0|  V|2023.0|  V|1676.0|  V|2308.0|  V|1983.0|  V|1309.0|  V|1513.0|  V| 931.0|  V| 832.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10|  1|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.8|  V|3853.0|  V|3847.9|  V|3847.2|  V|3847.9|  V|3846.3|  V|3844.2|  V|3843.6|  V|3843.3|  V|3843.1|  V|3853.0|  V|3893.0|  V|3893.0|  V|3893.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 13|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.9|  V|3852.3|  V|3846.3|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3888.8|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3868.1|  V|3868.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 16|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 24|1816.0|  V|1310.0|  V|1025.0|  V| 588.0|  V| 590.0|  V| 443.0|  V| 430.0|  V| 734.0|  V|1792.0|  V|1898.0|  V| 674.0|  V| 616.0|  V| 636.0|  V| 857.0|  V|1151.0|  V| 986.0|  V|1129.0|  V| 609.0|  V| 486.0|  V| 652.0|  V| 538.0|  V| 599.0|  V| 534.0|  V| 419.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 30|5860.0|  V|3499.0|  V|2035.0|  V|1272.0|  V|1355.0|  V|2306.0|  V|2439.0|  V|   NaN|  N|1736.0|  V|1910.0|  V|1579.0|  V|1364.0|  V|1696.0|  V|1862.0|  V|2094.0|  V|1949.0|  V|1686.0|  V|   NaN|  N|2125.0|  V|2665.0|  V|3017.0|  V|2511.0|  V|2709.0|  V|2312.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 28|   NaN|  N|3885.0|  V|3884.3|  V|3880.1|  V|3874.4|  V|3873.0|  V|3873.0|  V|   NaN|  N|3849.1|  V|3846.3|  V|3837.2|  V|   NaN|  N|3873.0|  V|3870.1|  V|3861.8|  V|3861.0|  V|3861.0|  V|3859.4|  V|   NaN|  N|3863.4|  V|3861.0|  V|3859.9|  V|3851.2|  V|3849.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 19|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:09:09 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:09:14 WARN DAGScheduler: Broadcasting large task binary with size 1507.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: h09\n",
      "Q1: 3.0\n",
      "Q3: 54.0\n",
      "IQR: 51.0\n",
      "Lower Bound: -150.0\n",
      "Upper Bound: 207.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:09:17 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:09:20 WARN DAGScheduler: Broadcasting large task binary with size 1596.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 12801\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:09:23 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:09:26 WARN DAGScheduler: Broadcasting large task binary with size 1621.0 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|codi_provincia|provincia|codi_municipi| municipi|estacio|codi_contaminant| any|mes|dia|   h01|v01|   h02|v02|   h03|v03|   h04|v04|   h05|v05|   h06|v06|   h07|v07|   h08|v08|   h09|v09|   h10|v10|   h11|v11|   h12|v12|   h13|v13|   h14|v14|   h15|v15|   h16|v16|   h17|v17|   h18|v18|   h19|v19|   h20|v20|   h21|v21|   h22|v22|   h23|v23|   h24|v24|  month|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 11| 771.0|  V|1024.0|  V| 998.0|  V| 659.0|  V| 641.0|  V| 734.0|  V| 846.0|  V|1278.0|  V|1077.0|  V|1086.0|  V|1096.0|  V|1746.0|  V|1653.0|  V| 825.0|  V| 707.0|  V| 730.0|  V| 833.0|  V| 791.0|  V| 944.0|  V|1002.0|  V| 949.0|  V|1116.0|  V|1621.0|  V|5794.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 22|1146.0|  V| 915.0|  V| 471.0|  V| 288.0|  V| 215.0|  V| 256.0|  V| 521.0|  V| 458.0|  V| 419.0|  V| 374.0|  V| 479.0|  V| 449.0|  V| 392.0|  V| 338.0|  V| 502.0|  V| 575.0|  V| 353.0|  V| 365.0|  V| 512.0|  V|1238.0|  V|2378.0|  V|2081.0|  V|3206.0|  V|1822.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  3|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 20|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 24|1921.0|  V|1593.0|  V|2850.0|  V|1949.0|  V|1559.0|  V|1102.0|  V|1619.0|  V|3558.0|  V|4674.0|  V|2410.0|  V|1657.0|  V|1159.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|1549.0|  V|1891.0|  V| 750.0|  V| 996.0|  V|1629.0|  V|2417.0|  V|2799.0|  V|2117.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 18|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3864.8|  V|3886.1|  V|3893.0|  V|3893.0|  V|3892.5|  V|3892.4|  V|3888.9|  V|3881.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3885.5|  V|3893.0|  V|3892.8|  V|3883.7|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 26|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             996|2022| 10| 21|   NaN|  N|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1115.2|  V|1123.7|  V|1126.0|  V|1126.0|  V|1126.0|  V|1128.9|  V|1137.6|  V|1133.0|  V|1133.7|  V|1138.7|  V|1112.6|  V|1101.1|  V|1106.9|  V|1113.6|  V|1114.0|  V|1114.0|  V|1114.7|  V|1124.2|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 16|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10|  4|1109.0|  V| 961.0|  V| 676.0|  V|1247.0|  V| 603.0|  V| 947.0|  V|1866.0|  V|3970.0|  V|5062.0|  V|2724.0|  V|2478.0|  V|2210.0|  V|2092.0|  V|1366.0|  V|1361.0|  V|1221.0|  V|1294.0|  V|1971.0|  V|2911.0|  V|2825.0|  V|1434.0|  V|1555.0|  V|1135.0|  V|1603.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 30|3873.0|  V|3888.3|  V|3885.0|  V|3885.0|  V|3883.7|  V|3876.5|  V|3873.0|  V|   NaN|  N|3883.1|  V|3876.0|  V|3873.0|  V|3873.0|  V|3873.0|  V|3871.5|  V|3862.5|  V|3861.0|  V|3861.0|  V|   NaN|  N|3875.0|  V|3873.0|  V|3873.0|  V|3870.9|  V|3863.4|  V|3861.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  5|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10|  7|   NaN|  N| 781.0|  V| 580.0|  V| 565.0|  V| 598.0|  V| 642.0|  V|1057.0|  V|1484.0|  V|1734.0|  V|1576.0|  V|1729.0|  V|1549.0|  V|1391.0|  V|2047.0|  V|1710.0|  V|1686.0|  V|2023.0|  V|1676.0|  V|2308.0|  V|1983.0|  V|1309.0|  V|1513.0|  V| 931.0|  V| 832.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10|  1|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.8|  V|3853.0|  V|3847.9|  V|3847.2|  V|3847.9|  V|3846.3|  V|3844.2|  V|3843.6|  V|3843.3|  V|3843.1|  V|3853.0|  V|3893.0|  V|3893.0|  V|3893.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 13|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.9|  V|3852.3|  V|3846.3|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3888.8|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3868.1|  V|3868.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 16|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 24|1816.0|  V|1310.0|  V|1025.0|  V| 588.0|  V| 590.0|  V| 443.0|  V| 430.0|  V| 734.0|  V|1792.0|  V|1898.0|  V| 674.0|  V| 616.0|  V| 636.0|  V| 857.0|  V|1151.0|  V| 986.0|  V|1129.0|  V| 609.0|  V| 486.0|  V| 652.0|  V| 538.0|  V| 599.0|  V| 534.0|  V| 419.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 30|5860.0|  V|3499.0|  V|2035.0|  V|1272.0|  V|1355.0|  V|2306.0|  V|2439.0|  V|   NaN|  N|1736.0|  V|1910.0|  V|1579.0|  V|1364.0|  V|1696.0|  V|1862.0|  V|2094.0|  V|1949.0|  V|1686.0|  V|   NaN|  N|2125.0|  V|2665.0|  V|3017.0|  V|2511.0|  V|2709.0|  V|2312.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 28|   NaN|  N|3885.0|  V|3884.3|  V|3880.1|  V|3874.4|  V|3873.0|  V|3873.0|  V|   NaN|  N|3849.1|  V|3846.3|  V|3837.2|  V|   NaN|  N|3873.0|  V|3870.1|  V|3861.8|  V|3861.0|  V|3861.0|  V|3859.4|  V|   NaN|  N|3863.4|  V|3861.0|  V|3859.9|  V|3851.2|  V|3849.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 19|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:09:29 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:09:33 WARN DAGScheduler: Broadcasting large task binary with size 1507.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: h10\n",
      "Q1: 4.0\n",
      "Q3: 54.0\n",
      "IQR: 50.0\n",
      "Lower Bound: -146.0\n",
      "Upper Bound: 204.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:09:36 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:09:39 WARN DAGScheduler: Broadcasting large task binary with size 1596.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 13683\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:09:42 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:09:46 WARN DAGScheduler: Broadcasting large task binary with size 1621.0 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|codi_provincia|provincia|codi_municipi| municipi|estacio|codi_contaminant| any|mes|dia|   h01|v01|   h02|v02|   h03|v03|   h04|v04|   h05|v05|   h06|v06|   h07|v07|   h08|v08|   h09|v09|   h10|v10|   h11|v11|   h12|v12|   h13|v13|   h14|v14|   h15|v15|   h16|v16|   h17|v17|   h18|v18|   h19|v19|   h20|v20|   h21|v21|   h22|v22|   h23|v23|   h24|v24|  month|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 11| 771.0|  V|1024.0|  V| 998.0|  V| 659.0|  V| 641.0|  V| 734.0|  V| 846.0|  V|1278.0|  V|1077.0|  V|1086.0|  V|1096.0|  V|1746.0|  V|1653.0|  V| 825.0|  V| 707.0|  V| 730.0|  V| 833.0|  V| 791.0|  V| 944.0|  V|1002.0|  V| 949.0|  V|1116.0|  V|1621.0|  V|5794.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 22|1146.0|  V| 915.0|  V| 471.0|  V| 288.0|  V| 215.0|  V| 256.0|  V| 521.0|  V| 458.0|  V| 419.0|  V| 374.0|  V| 479.0|  V| 449.0|  V| 392.0|  V| 338.0|  V| 502.0|  V| 575.0|  V| 353.0|  V| 365.0|  V| 512.0|  V|1238.0|  V|2378.0|  V|2081.0|  V|3206.0|  V|1822.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  3|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 20|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 24|1921.0|  V|1593.0|  V|2850.0|  V|1949.0|  V|1559.0|  V|1102.0|  V|1619.0|  V|3558.0|  V|4674.0|  V|2410.0|  V|1657.0|  V|1159.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|1549.0|  V|1891.0|  V| 750.0|  V| 996.0|  V|1629.0|  V|2417.0|  V|2799.0|  V|2117.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 18|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3864.8|  V|3886.1|  V|3893.0|  V|3893.0|  V|3892.5|  V|3892.4|  V|3888.9|  V|3881.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3885.5|  V|3893.0|  V|3892.8|  V|3883.7|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 26|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             996|2022| 10| 21|   NaN|  N|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1115.2|  V|1123.7|  V|1126.0|  V|1126.0|  V|1126.0|  V|1128.9|  V|1137.6|  V|1133.0|  V|1133.7|  V|1138.7|  V|1112.6|  V|1101.1|  V|1106.9|  V|1113.6|  V|1114.0|  V|1114.0|  V|1114.7|  V|1124.2|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 16|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10|  4|1109.0|  V| 961.0|  V| 676.0|  V|1247.0|  V| 603.0|  V| 947.0|  V|1866.0|  V|3970.0|  V|5062.0|  V|2724.0|  V|2478.0|  V|2210.0|  V|2092.0|  V|1366.0|  V|1361.0|  V|1221.0|  V|1294.0|  V|1971.0|  V|2911.0|  V|2825.0|  V|1434.0|  V|1555.0|  V|1135.0|  V|1603.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 30|3873.0|  V|3888.3|  V|3885.0|  V|3885.0|  V|3883.7|  V|3876.5|  V|3873.0|  V|   NaN|  N|3883.1|  V|3876.0|  V|3873.0|  V|3873.0|  V|3873.0|  V|3871.5|  V|3862.5|  V|3861.0|  V|3861.0|  V|   NaN|  N|3875.0|  V|3873.0|  V|3873.0|  V|3870.9|  V|3863.4|  V|3861.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  5|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10|  7|   NaN|  N| 781.0|  V| 580.0|  V| 565.0|  V| 598.0|  V| 642.0|  V|1057.0|  V|1484.0|  V|1734.0|  V|1576.0|  V|1729.0|  V|1549.0|  V|1391.0|  V|2047.0|  V|1710.0|  V|1686.0|  V|2023.0|  V|1676.0|  V|2308.0|  V|1983.0|  V|1309.0|  V|1513.0|  V| 931.0|  V| 832.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10|  1|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.8|  V|3853.0|  V|3847.9|  V|3847.2|  V|3847.9|  V|3846.3|  V|3844.2|  V|3843.6|  V|3843.3|  V|3843.1|  V|3853.0|  V|3893.0|  V|3893.0|  V|3893.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 13|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.9|  V|3852.3|  V|3846.3|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3888.8|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3868.1|  V|3868.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 16|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 24|1816.0|  V|1310.0|  V|1025.0|  V| 588.0|  V| 590.0|  V| 443.0|  V| 430.0|  V| 734.0|  V|1792.0|  V|1898.0|  V| 674.0|  V| 616.0|  V| 636.0|  V| 857.0|  V|1151.0|  V| 986.0|  V|1129.0|  V| 609.0|  V| 486.0|  V| 652.0|  V| 538.0|  V| 599.0|  V| 534.0|  V| 419.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|               7|2022| 10| 26|   5.0|  V|   5.0|  V|  12.0|  V|   7.0|  V|  18.0|  V|   9.0|  V|  34.0|  V|  79.0|  V| 120.0|  V| 249.0|  V| 194.0|  V| 161.0|  V| 103.0|  V|  80.0|  V|  48.0|  V|  34.0|  V|  41.0|  V|  43.0|  V|  40.0|  V|  32.0|  V|  19.0|  V|  10.0|  V|   7.0|  V|  38.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 30|5860.0|  V|3499.0|  V|2035.0|  V|1272.0|  V|1355.0|  V|2306.0|  V|2439.0|  V|   NaN|  N|1736.0|  V|1910.0|  V|1579.0|  V|1364.0|  V|1696.0|  V|1862.0|  V|2094.0|  V|1949.0|  V|1686.0|  V|   NaN|  N|2125.0|  V|2665.0|  V|3017.0|  V|2511.0|  V|2709.0|  V|2312.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 28|   NaN|  N|3885.0|  V|3884.3|  V|3880.1|  V|3874.4|  V|3873.0|  V|3873.0|  V|   NaN|  N|3849.1|  V|3846.3|  V|3837.2|  V|   NaN|  N|3873.0|  V|3870.1|  V|3861.8|  V|3861.0|  V|3861.0|  V|3859.4|  V|   NaN|  N|3863.4|  V|3861.0|  V|3859.9|  V|3851.2|  V|3849.0|  V|2022_10|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:09:49 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:09:53 WARN DAGScheduler: Broadcasting large task binary with size 1507.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: h11\n",
      "Q1: 4.0\n",
      "Q3: 50.0\n",
      "IQR: 46.0\n",
      "Lower Bound: -134.0\n",
      "Upper Bound: 188.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:09:56 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:09:59 WARN DAGScheduler: Broadcasting large task binary with size 1596.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 15094\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:10:02 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:10:05 WARN DAGScheduler: Broadcasting large task binary with size 1621.0 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|codi_provincia|provincia|codi_municipi| municipi|estacio|codi_contaminant| any|mes|dia|   h01|v01|   h02|v02|   h03|v03|   h04|v04|   h05|v05|   h06|v06|   h07|v07|   h08|v08|   h09|v09|   h10|v10|   h11|v11|   h12|v12|   h13|v13|   h14|v14|   h15|v15|   h16|v16|   h17|v17|   h18|v18|   h19|v19|   h20|v20|   h21|v21|   h22|v22|   h23|v23|   h24|v24|  month|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 11| 771.0|  V|1024.0|  V| 998.0|  V| 659.0|  V| 641.0|  V| 734.0|  V| 846.0|  V|1278.0|  V|1077.0|  V|1086.0|  V|1096.0|  V|1746.0|  V|1653.0|  V| 825.0|  V| 707.0|  V| 730.0|  V| 833.0|  V| 791.0|  V| 944.0|  V|1002.0|  V| 949.0|  V|1116.0|  V|1621.0|  V|5794.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 22|1146.0|  V| 915.0|  V| 471.0|  V| 288.0|  V| 215.0|  V| 256.0|  V| 521.0|  V| 458.0|  V| 419.0|  V| 374.0|  V| 479.0|  V| 449.0|  V| 392.0|  V| 338.0|  V| 502.0|  V| 575.0|  V| 353.0|  V| 365.0|  V| 512.0|  V|1238.0|  V|2378.0|  V|2081.0|  V|3206.0|  V|1822.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  3|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 20|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 24|1921.0|  V|1593.0|  V|2850.0|  V|1949.0|  V|1559.0|  V|1102.0|  V|1619.0|  V|3558.0|  V|4674.0|  V|2410.0|  V|1657.0|  V|1159.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|1549.0|  V|1891.0|  V| 750.0|  V| 996.0|  V|1629.0|  V|2417.0|  V|2799.0|  V|2117.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 18|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3864.8|  V|3886.1|  V|3893.0|  V|3893.0|  V|3892.5|  V|3892.4|  V|3888.9|  V|3881.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3885.5|  V|3893.0|  V|3892.8|  V|3883.7|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 26|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             996|2022| 10| 21|   NaN|  N|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1115.2|  V|1123.7|  V|1126.0|  V|1126.0|  V|1126.0|  V|1128.9|  V|1137.6|  V|1133.0|  V|1133.7|  V|1138.7|  V|1112.6|  V|1101.1|  V|1106.9|  V|1113.6|  V|1114.0|  V|1114.0|  V|1114.7|  V|1124.2|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 16|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10|  4|1109.0|  V| 961.0|  V| 676.0|  V|1247.0|  V| 603.0|  V| 947.0|  V|1866.0|  V|3970.0|  V|5062.0|  V|2724.0|  V|2478.0|  V|2210.0|  V|2092.0|  V|1366.0|  V|1361.0|  V|1221.0|  V|1294.0|  V|1971.0|  V|2911.0|  V|2825.0|  V|1434.0|  V|1555.0|  V|1135.0|  V|1603.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     44|               7|2022| 10|  5|   3.0|  V|   8.0|  V|   5.0|  V|   4.0|  V|   4.0|  V|   4.0|  V|   3.0|  V|   9.0|  V|  33.0|  V|  19.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|  17.0|  V|  14.0|  V|  17.0|  V|  16.0|  V|   9.0|  V|   8.0|  V|   9.0|  V|   5.0|  V|  13.0|  V|   5.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     44|              10|2022| 10|  5|  19.0|  V|  18.0|  V|  17.0|  V|  14.0|  V|  15.0|  V|  17.0|  V|  18.0|  V|  15.0|  V|  26.0|  V|  27.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|  57.0|  V|  40.0|  V|  31.0|  V|  29.0|  V|  26.0|  V|  24.0|  V|  25.0|  V|  25.0|  V|  21.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 30|3873.0|  V|3888.3|  V|3885.0|  V|3885.0|  V|3883.7|  V|3876.5|  V|3873.0|  V|   NaN|  N|3883.1|  V|3876.0|  V|3873.0|  V|3873.0|  V|3873.0|  V|3871.5|  V|3862.5|  V|3861.0|  V|3861.0|  V|   NaN|  N|3875.0|  V|3873.0|  V|3873.0|  V|3870.9|  V|3863.4|  V|3861.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  5|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10|  7|   NaN|  N| 781.0|  V| 580.0|  V| 565.0|  V| 598.0|  V| 642.0|  V|1057.0|  V|1484.0|  V|1734.0|  V|1576.0|  V|1729.0|  V|1549.0|  V|1391.0|  V|2047.0|  V|1710.0|  V|1686.0|  V|2023.0|  V|1676.0|  V|2308.0|  V|1983.0|  V|1309.0|  V|1513.0|  V| 931.0|  V| 832.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10|  1|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.8|  V|3853.0|  V|3847.9|  V|3847.2|  V|3847.9|  V|3846.3|  V|3844.2|  V|3843.6|  V|3843.3|  V|3843.1|  V|3853.0|  V|3893.0|  V|3893.0|  V|3893.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 13|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.9|  V|3852.3|  V|3846.3|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3888.8|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3868.1|  V|3868.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 16|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|               8|2022| 10| 21|  20.0|  V|  11.0|  V|  10.0|  V|  10.0|  V|   9.0|  V|  22.0|  V|  28.0|  V|  18.0|  V|  32.0|  V|  28.0|  V|   NaN|  N|   NaN|  N|  15.0|  V|   9.0|  V|   7.0|  V|   8.0|  V|  11.0|  V|  12.0|  V|  15.0|  V|  21.0|  V|  24.0|  V|  33.0|  V|  53.0|  V|  33.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 24|1816.0|  V|1310.0|  V|1025.0|  V| 588.0|  V| 590.0|  V| 443.0|  V| 430.0|  V| 734.0|  V|1792.0|  V|1898.0|  V| 674.0|  V| 616.0|  V| 636.0|  V| 857.0|  V|1151.0|  V| 986.0|  V|1129.0|  V| 609.0|  V| 486.0|  V| 652.0|  V| 538.0|  V| 599.0|  V| 534.0|  V| 419.0|  V|2022_10|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:10:08 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:10:12 WARN DAGScheduler: Broadcasting large task binary with size 1507.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: h12\n",
      "Q1: 4.0\n",
      "Q3: 48.0\n",
      "IQR: 44.0\n",
      "Lower Bound: -128.0\n",
      "Upper Bound: 180.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:10:15 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:10:18 WARN DAGScheduler: Broadcasting large task binary with size 1596.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 15122\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:10:21 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:10:24 WARN DAGScheduler: Broadcasting large task binary with size 1621.0 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|codi_provincia|provincia|codi_municipi| municipi|estacio|codi_contaminant| any|mes|dia|   h01|v01|   h02|v02|   h03|v03|   h04|v04|   h05|v05|   h06|v06|   h07|v07|   h08|v08|   h09|v09|   h10|v10|   h11|v11|   h12|v12|   h13|v13|   h14|v14|   h15|v15|   h16|v16|   h17|v17|   h18|v18|   h19|v19|   h20|v20|   h21|v21|   h22|v22|   h23|v23|   h24|v24|  month|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 11| 771.0|  V|1024.0|  V| 998.0|  V| 659.0|  V| 641.0|  V| 734.0|  V| 846.0|  V|1278.0|  V|1077.0|  V|1086.0|  V|1096.0|  V|1746.0|  V|1653.0|  V| 825.0|  V| 707.0|  V| 730.0|  V| 833.0|  V| 791.0|  V| 944.0|  V|1002.0|  V| 949.0|  V|1116.0|  V|1621.0|  V|5794.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 22|1146.0|  V| 915.0|  V| 471.0|  V| 288.0|  V| 215.0|  V| 256.0|  V| 521.0|  V| 458.0|  V| 419.0|  V| 374.0|  V| 479.0|  V| 449.0|  V| 392.0|  V| 338.0|  V| 502.0|  V| 575.0|  V| 353.0|  V| 365.0|  V| 512.0|  V|1238.0|  V|2378.0|  V|2081.0|  V|3206.0|  V|1822.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  3|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 20|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 24|1921.0|  V|1593.0|  V|2850.0|  V|1949.0|  V|1559.0|  V|1102.0|  V|1619.0|  V|3558.0|  V|4674.0|  V|2410.0|  V|1657.0|  V|1159.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|1549.0|  V|1891.0|  V| 750.0|  V| 996.0|  V|1629.0|  V|2417.0|  V|2799.0|  V|2117.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 18|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3864.8|  V|3886.1|  V|3893.0|  V|3893.0|  V|3892.5|  V|3892.4|  V|3888.9|  V|3881.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3885.5|  V|3893.0|  V|3892.8|  V|3883.7|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 26|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             996|2022| 10| 21|   NaN|  N|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1115.2|  V|1123.7|  V|1126.0|  V|1126.0|  V|1126.0|  V|1128.9|  V|1137.6|  V|1133.0|  V|1133.7|  V|1138.7|  V|1112.6|  V|1101.1|  V|1106.9|  V|1113.6|  V|1114.0|  V|1114.0|  V|1114.7|  V|1124.2|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 16|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10|  4|1109.0|  V| 961.0|  V| 676.0|  V|1247.0|  V| 603.0|  V| 947.0|  V|1866.0|  V|3970.0|  V|5062.0|  V|2724.0|  V|2478.0|  V|2210.0|  V|2092.0|  V|1366.0|  V|1361.0|  V|1221.0|  V|1294.0|  V|1971.0|  V|2911.0|  V|2825.0|  V|1434.0|  V|1555.0|  V|1135.0|  V|1603.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     44|               7|2022| 10|  5|   3.0|  V|   8.0|  V|   5.0|  V|   4.0|  V|   4.0|  V|   4.0|  V|   3.0|  V|   9.0|  V|  33.0|  V|  19.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|  17.0|  V|  14.0|  V|  17.0|  V|  16.0|  V|   9.0|  V|   8.0|  V|   9.0|  V|   5.0|  V|  13.0|  V|   5.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     44|              10|2022| 10|  5|  19.0|  V|  18.0|  V|  17.0|  V|  14.0|  V|  15.0|  V|  17.0|  V|  18.0|  V|  15.0|  V|  26.0|  V|  27.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|  57.0|  V|  40.0|  V|  31.0|  V|  29.0|  V|  26.0|  V|  24.0|  V|  25.0|  V|  25.0|  V|  21.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 30|3873.0|  V|3888.3|  V|3885.0|  V|3885.0|  V|3883.7|  V|3876.5|  V|3873.0|  V|   NaN|  N|3883.1|  V|3876.0|  V|3873.0|  V|3873.0|  V|3873.0|  V|3871.5|  V|3862.5|  V|3861.0|  V|3861.0|  V|   NaN|  N|3875.0|  V|3873.0|  V|3873.0|  V|3870.9|  V|3863.4|  V|3861.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  5|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10|  7|   NaN|  N| 781.0|  V| 580.0|  V| 565.0|  V| 598.0|  V| 642.0|  V|1057.0|  V|1484.0|  V|1734.0|  V|1576.0|  V|1729.0|  V|1549.0|  V|1391.0|  V|2047.0|  V|1710.0|  V|1686.0|  V|2023.0|  V|1676.0|  V|2308.0|  V|1983.0|  V|1309.0|  V|1513.0|  V| 931.0|  V| 832.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     44|               8|2022| 10| 20|  45.0|  V|  38.0|  V|  28.0|  V|  38.0|  V|  44.0|  V|  26.0|  V|  38.0|  V|  53.0|  V|  58.0|  V|  64.0|  V|  70.0|  V|   NaN|  N|  51.0|  V|  53.0|  V|  45.0|  V|  59.0|  V|  64.0|  V|  58.0|  V|  78.0|  V|  54.0|  V|  25.0|  V|  18.0|  V|  16.0|  V|  20.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10|  1|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.8|  V|3853.0|  V|3847.9|  V|3847.2|  V|3847.9|  V|3846.3|  V|3844.2|  V|3843.6|  V|3843.3|  V|3843.1|  V|3853.0|  V|3893.0|  V|3893.0|  V|3893.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 13|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.9|  V|3852.3|  V|3846.3|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3888.8|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3868.1|  V|3868.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 16|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|               8|2022| 10| 21|  20.0|  V|  11.0|  V|  10.0|  V|  10.0|  V|   9.0|  V|  22.0|  V|  28.0|  V|  18.0|  V|  32.0|  V|  28.0|  V|   NaN|  N|   NaN|  N|  15.0|  V|   9.0|  V|   7.0|  V|   8.0|  V|  11.0|  V|  12.0|  V|  15.0|  V|  21.0|  V|  24.0|  V|  33.0|  V|  53.0|  V|  33.0|  V|2022_10|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:10:27 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:10:31 WARN DAGScheduler: Broadcasting large task binary with size 1507.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: h13\n",
      "Q1: 3.17\n",
      "Q3: 48.0\n",
      "IQR: 44.83\n",
      "Lower Bound: -131.32000000000002\n",
      "Upper Bound: 182.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:10:34 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:10:37 WARN DAGScheduler: Broadcasting large task binary with size 1598.1 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 14831\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:10:40 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:10:43 WARN DAGScheduler: Broadcasting large task binary with size 1622.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|codi_provincia|provincia|codi_municipi| municipi|estacio|codi_contaminant| any|mes|dia|   h01|v01|   h02|v02|   h03|v03|   h04|v04|   h05|v05|   h06|v06|   h07|v07|   h08|v08|   h09|v09|   h10|v10|   h11|v11|   h12|v12|   h13|v13|   h14|v14|   h15|v15|   h16|v16|   h17|v17|   h18|v18|   h19|v19|   h20|v20|   h21|v21|   h22|v22|   h23|v23|   h24|v24|  month|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 11| 771.0|  V|1024.0|  V| 998.0|  V| 659.0|  V| 641.0|  V| 734.0|  V| 846.0|  V|1278.0|  V|1077.0|  V|1086.0|  V|1096.0|  V|1746.0|  V|1653.0|  V| 825.0|  V| 707.0|  V| 730.0|  V| 833.0|  V| 791.0|  V| 944.0|  V|1002.0|  V| 949.0|  V|1116.0|  V|1621.0|  V|5794.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 22|1146.0|  V| 915.0|  V| 471.0|  V| 288.0|  V| 215.0|  V| 256.0|  V| 521.0|  V| 458.0|  V| 419.0|  V| 374.0|  V| 479.0|  V| 449.0|  V| 392.0|  V| 338.0|  V| 502.0|  V| 575.0|  V| 353.0|  V| 365.0|  V| 512.0|  V|1238.0|  V|2378.0|  V|2081.0|  V|3206.0|  V|1822.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  3|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     50|               7|2022| 10| 19|   2.0|  V|   4.0|  V|   6.0|  V|  11.0|  V|  16.0|  V|  19.0|  V|  21.0|  V|  24.0|  V|  53.0|  V|  93.0|  V|  91.0|  V|  67.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|  16.0|  V|   6.0|  V|  12.0|  V|   3.0|  V|   1.0|  V|   1.0|  V|   1.0|  V|   1.0|  V|   5.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 20|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 24|1921.0|  V|1593.0|  V|2850.0|  V|1949.0|  V|1559.0|  V|1102.0|  V|1619.0|  V|3558.0|  V|4674.0|  V|2410.0|  V|1657.0|  V|1159.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|1549.0|  V|1891.0|  V| 750.0|  V| 996.0|  V|1629.0|  V|2417.0|  V|2799.0|  V|2117.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 18|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3864.8|  V|3886.1|  V|3893.0|  V|3893.0|  V|3892.5|  V|3892.4|  V|3888.9|  V|3881.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3885.5|  V|3893.0|  V|3892.8|  V|3883.7|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 26|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             996|2022| 10| 21|   NaN|  N|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1115.2|  V|1123.7|  V|1126.0|  V|1126.0|  V|1126.0|  V|1128.9|  V|1137.6|  V|1133.0|  V|1133.7|  V|1138.7|  V|1112.6|  V|1101.1|  V|1106.9|  V|1113.6|  V|1114.0|  V|1114.0|  V|1114.7|  V|1124.2|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 16|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10|  4|1109.0|  V| 961.0|  V| 676.0|  V|1247.0|  V| 603.0|  V| 947.0|  V|1866.0|  V|3970.0|  V|5062.0|  V|2724.0|  V|2478.0|  V|2210.0|  V|2092.0|  V|1366.0|  V|1361.0|  V|1221.0|  V|1294.0|  V|1971.0|  V|2911.0|  V|2825.0|  V|1434.0|  V|1555.0|  V|1135.0|  V|1603.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     50|               8|2022| 10| 19|  19.0|  V|  26.0|  V|  27.0|  V|  24.0|  V|  22.0|  V|  21.0|  V|  19.0|  V|  19.0|  V|  23.0|  V|  31.0|  V|  38.0|  V|  49.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|  62.0|  V|  36.0|  V|  55.0|  V|  39.0|  V|  34.0|  V|  35.0|  V|  35.0|  V|  31.0|  V|  45.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     44|               7|2022| 10|  5|   3.0|  V|   8.0|  V|   5.0|  V|   4.0|  V|   4.0|  V|   4.0|  V|   3.0|  V|   9.0|  V|  33.0|  V|  19.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|  17.0|  V|  14.0|  V|  17.0|  V|  16.0|  V|   9.0|  V|   8.0|  V|   9.0|  V|   5.0|  V|  13.0|  V|   5.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     44|              10|2022| 10|  5|  19.0|  V|  18.0|  V|  17.0|  V|  14.0|  V|  15.0|  V|  17.0|  V|  18.0|  V|  15.0|  V|  26.0|  V|  27.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|  57.0|  V|  40.0|  V|  31.0|  V|  29.0|  V|  26.0|  V|  24.0|  V|  25.0|  V|  25.0|  V|  21.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 30|3873.0|  V|3888.3|  V|3885.0|  V|3885.0|  V|3883.7|  V|3876.5|  V|3873.0|  V|   NaN|  N|3883.1|  V|3876.0|  V|3873.0|  V|3873.0|  V|3873.0|  V|3871.5|  V|3862.5|  V|3861.0|  V|3861.0|  V|   NaN|  N|3875.0|  V|3873.0|  V|3873.0|  V|3870.9|  V|3863.4|  V|3861.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  5|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10|  7|   NaN|  N| 781.0|  V| 580.0|  V| 565.0|  V| 598.0|  V| 642.0|  V|1057.0|  V|1484.0|  V|1734.0|  V|1576.0|  V|1729.0|  V|1549.0|  V|1391.0|  V|2047.0|  V|1710.0|  V|1686.0|  V|2023.0|  V|1676.0|  V|2308.0|  V|1983.0|  V|1309.0|  V|1513.0|  V| 931.0|  V| 832.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     50|              12|2022| 10| 19|  22.0|  V|  32.0|  V|  36.0|  V|  41.0|  V|  47.0|  V|  50.0|  V|  51.0|  V|  55.0|  V| 104.0|  V| 172.0|  V| 177.0|  V| 151.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|  86.0|  V|  45.0|  V|  73.0|  V|  44.0|  V|  36.0|  V|  37.0|  V|  37.0|  V|  33.0|  V|  53.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10|  1|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.8|  V|3853.0|  V|3847.9|  V|3847.2|  V|3847.9|  V|3846.3|  V|3844.2|  V|3843.6|  V|3843.3|  V|3843.1|  V|3853.0|  V|3893.0|  V|3893.0|  V|3893.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 13|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.9|  V|3852.3|  V|3846.3|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3888.8|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3868.1|  V|3868.0|  V|2022_10|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:10:46 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:10:50 WARN DAGScheduler: Broadcasting large task binary with size 1507.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: h14\n",
      "Q1: 3.0\n",
      "Q3: 44.0\n",
      "IQR: 41.0\n",
      "Lower Bound: -120.0\n",
      "Upper Bound: 167.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:10:53 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:10:57 WARN DAGScheduler: Broadcasting large task binary with size 1596.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 14204\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:11:00 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:11:03 WARN DAGScheduler: Broadcasting large task binary with size 1621.0 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|codi_provincia|provincia|codi_municipi| municipi|estacio|codi_contaminant| any|mes|dia|   h01|v01|   h02|v02|   h03|v03|   h04|v04|   h05|v05|   h06|v06|   h07|v07|   h08|v08|   h09|v09|   h10|v10|   h11|v11|   h12|v12|   h13|v13|   h14|v14|   h15|v15|   h16|v16|   h17|v17|   h18|v18|   h19|v19|   h20|v20|   h21|v21|   h22|v22|   h23|v23|   h24|v24|  month|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 11| 771.0|  V|1024.0|  V| 998.0|  V| 659.0|  V| 641.0|  V| 734.0|  V| 846.0|  V|1278.0|  V|1077.0|  V|1086.0|  V|1096.0|  V|1746.0|  V|1653.0|  V| 825.0|  V| 707.0|  V| 730.0|  V| 833.0|  V| 791.0|  V| 944.0|  V|1002.0|  V| 949.0|  V|1116.0|  V|1621.0|  V|5794.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 22|1146.0|  V| 915.0|  V| 471.0|  V| 288.0|  V| 215.0|  V| 256.0|  V| 521.0|  V| 458.0|  V| 419.0|  V| 374.0|  V| 479.0|  V| 449.0|  V| 392.0|  V| 338.0|  V| 502.0|  V| 575.0|  V| 353.0|  V| 365.0|  V| 512.0|  V|1238.0|  V|2378.0|  V|2081.0|  V|3206.0|  V|1822.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     57|             112|2022| 10| 10| 35.38|  V|  9.77|  V|  4.19|  V|  9.68|  V|  7.23|  V|  7.22|  V| 11.59|  V| 34.24|  V| 83.17|  V| 67.63|  V| 69.93|  V| 53.76|  V| 34.98|  V|   NaN|  N|   NaN|  N|   NaN|  N| 23.42|  V| 28.96|  V| 27.81|  V| 27.49|  V| 18.18|  V| 13.44|  V| 13.53|  V| 29.74|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  3|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     50|               7|2022| 10| 19|   2.0|  V|   4.0|  V|   6.0|  V|  11.0|  V|  16.0|  V|  19.0|  V|  21.0|  V|  24.0|  V|  53.0|  V|  93.0|  V|  91.0|  V|  67.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|  16.0|  V|   6.0|  V|  12.0|  V|   3.0|  V|   1.0|  V|   1.0|  V|   1.0|  V|   1.0|  V|   5.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 20|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 24|1921.0|  V|1593.0|  V|2850.0|  V|1949.0|  V|1559.0|  V|1102.0|  V|1619.0|  V|3558.0|  V|4674.0|  V|2410.0|  V|1657.0|  V|1159.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|1549.0|  V|1891.0|  V| 750.0|  V| 996.0|  V|1629.0|  V|2417.0|  V|2799.0|  V|2117.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 18|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3864.8|  V|3886.1|  V|3893.0|  V|3893.0|  V|3892.5|  V|3892.4|  V|3888.9|  V|3881.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3885.5|  V|3893.0|  V|3892.8|  V|3883.7|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 26|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             996|2022| 10| 21|   NaN|  N|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1115.2|  V|1123.7|  V|1126.0|  V|1126.0|  V|1126.0|  V|1128.9|  V|1137.6|  V|1133.0|  V|1133.7|  V|1138.7|  V|1112.6|  V|1101.1|  V|1106.9|  V|1113.6|  V|1114.0|  V|1114.0|  V|1114.7|  V|1124.2|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 16|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10|  4|1109.0|  V| 961.0|  V| 676.0|  V|1247.0|  V| 603.0|  V| 947.0|  V|1866.0|  V|3970.0|  V|5062.0|  V|2724.0|  V|2478.0|  V|2210.0|  V|2092.0|  V|1366.0|  V|1361.0|  V|1221.0|  V|1294.0|  V|1971.0|  V|2911.0|  V|2825.0|  V|1434.0|  V|1555.0|  V|1135.0|  V|1603.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     50|               8|2022| 10| 19|  19.0|  V|  26.0|  V|  27.0|  V|  24.0|  V|  22.0|  V|  21.0|  V|  19.0|  V|  19.0|  V|  23.0|  V|  31.0|  V|  38.0|  V|  49.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|  62.0|  V|  36.0|  V|  55.0|  V|  39.0|  V|  34.0|  V|  35.0|  V|  35.0|  V|  31.0|  V|  45.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     57|             101|2022| 10| 10|  2.45|  V|  2.41|  V|  2.32|  V|  2.41|  V|  2.26|  V|  2.15|  V|  2.18|  V|  2.34|  V|  2.62|  V|  2.43|  V|  2.72|  V|  2.65|  V|  2.95|  V|   NaN|  N|   NaN|  N|   NaN|  N|  2.08|  V|  2.26|  V|  1.92|  V|  1.88|  V|  1.98|  V|  1.92|  V|  2.04|  V|  2.07|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     44|               7|2022| 10|  5|   3.0|  V|   8.0|  V|   5.0|  V|   4.0|  V|   4.0|  V|   4.0|  V|   3.0|  V|   9.0|  V|  33.0|  V|  19.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|  17.0|  V|  14.0|  V|  17.0|  V|  16.0|  V|   9.0|  V|   8.0|  V|   9.0|  V|   5.0|  V|  13.0|  V|   5.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     44|              10|2022| 10|  5|  19.0|  V|  18.0|  V|  17.0|  V|  14.0|  V|  15.0|  V|  17.0|  V|  18.0|  V|  15.0|  V|  26.0|  V|  27.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|  57.0|  V|  40.0|  V|  31.0|  V|  29.0|  V|  26.0|  V|  24.0|  V|  25.0|  V|  25.0|  V|  21.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 30|3873.0|  V|3888.3|  V|3885.0|  V|3885.0|  V|3883.7|  V|3876.5|  V|3873.0|  V|   NaN|  N|3883.1|  V|3876.0|  V|3873.0|  V|3873.0|  V|3873.0|  V|3871.5|  V|3862.5|  V|3861.0|  V|3861.0|  V|   NaN|  N|3875.0|  V|3873.0|  V|3873.0|  V|3870.9|  V|3863.4|  V|3861.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  5|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10|  7|   NaN|  N| 781.0|  V| 580.0|  V| 565.0|  V| 598.0|  V| 642.0|  V|1057.0|  V|1484.0|  V|1734.0|  V|1576.0|  V|1729.0|  V|1549.0|  V|1391.0|  V|2047.0|  V|1710.0|  V|1686.0|  V|2023.0|  V|1676.0|  V|2308.0|  V|1983.0|  V|1309.0|  V|1513.0|  V| 931.0|  V| 832.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     50|              12|2022| 10| 19|  22.0|  V|  32.0|  V|  36.0|  V|  41.0|  V|  47.0|  V|  50.0|  V|  51.0|  V|  55.0|  V| 104.0|  V| 172.0|  V| 177.0|  V| 151.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|  86.0|  V|  45.0|  V|  73.0|  V|  44.0|  V|  36.0|  V|  37.0|  V|  37.0|  V|  33.0|  V|  53.0|  V|2022_10|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:11:05 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:11:09 WARN DAGScheduler: Broadcasting large task binary with size 1507.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: h15\n",
      "Q1: 3.0\n",
      "Q3: 42.0\n",
      "IQR: 39.0\n",
      "Lower Bound: -114.0\n",
      "Upper Bound: 159.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:11:13 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:11:16 WARN DAGScheduler: Broadcasting large task binary with size 1596.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 13575\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:11:19 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:11:22 WARN DAGScheduler: Broadcasting large task binary with size 1621.0 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|codi_provincia|provincia|codi_municipi| municipi|estacio|codi_contaminant| any|mes|dia|   h01|v01|   h02|v02|   h03|v03|   h04|v04|   h05|v05|   h06|v06|   h07|v07|   h08|v08|   h09|v09|   h10|v10|   h11|v11|   h12|v12|   h13|v13|   h14|v14|   h15|v15|   h16|v16|   h17|v17|   h18|v18|   h19|v19|   h20|v20|   h21|v21|   h22|v22|   h23|v23|   h24|v24|  month|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 11| 771.0|  V|1024.0|  V| 998.0|  V| 659.0|  V| 641.0|  V| 734.0|  V| 846.0|  V|1278.0|  V|1077.0|  V|1086.0|  V|1096.0|  V|1746.0|  V|1653.0|  V| 825.0|  V| 707.0|  V| 730.0|  V| 833.0|  V| 791.0|  V| 944.0|  V|1002.0|  V| 949.0|  V|1116.0|  V|1621.0|  V|5794.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 22|1146.0|  V| 915.0|  V| 471.0|  V| 288.0|  V| 215.0|  V| 256.0|  V| 521.0|  V| 458.0|  V| 419.0|  V| 374.0|  V| 479.0|  V| 449.0|  V| 392.0|  V| 338.0|  V| 502.0|  V| 575.0|  V| 353.0|  V| 365.0|  V| 512.0|  V|1238.0|  V|2378.0|  V|2081.0|  V|3206.0|  V|1822.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     57|             112|2022| 10| 10| 35.38|  V|  9.77|  V|  4.19|  V|  9.68|  V|  7.23|  V|  7.22|  V| 11.59|  V| 34.24|  V| 83.17|  V| 67.63|  V| 69.93|  V| 53.76|  V| 34.98|  V|   NaN|  N|   NaN|  N|   NaN|  N| 23.42|  V| 28.96|  V| 27.81|  V| 27.49|  V| 18.18|  V| 13.44|  V| 13.53|  V| 29.74|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  3|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     50|               7|2022| 10| 19|   2.0|  V|   4.0|  V|   6.0|  V|  11.0|  V|  16.0|  V|  19.0|  V|  21.0|  V|  24.0|  V|  53.0|  V|  93.0|  V|  91.0|  V|  67.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|  16.0|  V|   6.0|  V|  12.0|  V|   3.0|  V|   1.0|  V|   1.0|  V|   1.0|  V|   1.0|  V|   5.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 20|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 24|1921.0|  V|1593.0|  V|2850.0|  V|1949.0|  V|1559.0|  V|1102.0|  V|1619.0|  V|3558.0|  V|4674.0|  V|2410.0|  V|1657.0|  V|1159.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|1549.0|  V|1891.0|  V| 750.0|  V| 996.0|  V|1629.0|  V|2417.0|  V|2799.0|  V|2117.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 18|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3864.8|  V|3886.1|  V|3893.0|  V|3893.0|  V|3892.5|  V|3892.4|  V|3888.9|  V|3881.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3885.5|  V|3893.0|  V|3892.8|  V|3883.7|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 26|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             996|2022| 10| 21|   NaN|  N|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1115.2|  V|1123.7|  V|1126.0|  V|1126.0|  V|1126.0|  V|1128.9|  V|1137.6|  V|1133.0|  V|1133.7|  V|1138.7|  V|1112.6|  V|1101.1|  V|1106.9|  V|1113.6|  V|1114.0|  V|1114.0|  V|1114.7|  V|1124.2|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 16|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10|  4|1109.0|  V| 961.0|  V| 676.0|  V|1247.0|  V| 603.0|  V| 947.0|  V|1866.0|  V|3970.0|  V|5062.0|  V|2724.0|  V|2478.0|  V|2210.0|  V|2092.0|  V|1366.0|  V|1361.0|  V|1221.0|  V|1294.0|  V|1971.0|  V|2911.0|  V|2825.0|  V|1434.0|  V|1555.0|  V|1135.0|  V|1603.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     50|               8|2022| 10| 19|  19.0|  V|  26.0|  V|  27.0|  V|  24.0|  V|  22.0|  V|  21.0|  V|  19.0|  V|  19.0|  V|  23.0|  V|  31.0|  V|  38.0|  V|  49.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|  62.0|  V|  36.0|  V|  55.0|  V|  39.0|  V|  34.0|  V|  35.0|  V|  35.0|  V|  31.0|  V|  45.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     57|             101|2022| 10| 10|  2.45|  V|  2.41|  V|  2.32|  V|  2.41|  V|  2.26|  V|  2.15|  V|  2.18|  V|  2.34|  V|  2.62|  V|  2.43|  V|  2.72|  V|  2.65|  V|  2.95|  V|   NaN|  N|   NaN|  N|   NaN|  N|  2.08|  V|  2.26|  V|  1.92|  V|  1.88|  V|  1.98|  V|  1.92|  V|  2.04|  V|  2.07|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     44|              10|2022| 10|  5|  19.0|  V|  18.0|  V|  17.0|  V|  14.0|  V|  15.0|  V|  17.0|  V|  18.0|  V|  15.0|  V|  26.0|  V|  27.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|  57.0|  V|  40.0|  V|  31.0|  V|  29.0|  V|  26.0|  V|  24.0|  V|  25.0|  V|  25.0|  V|  21.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 30|3873.0|  V|3888.3|  V|3885.0|  V|3885.0|  V|3883.7|  V|3876.5|  V|3873.0|  V|   NaN|  N|3883.1|  V|3876.0|  V|3873.0|  V|3873.0|  V|3873.0|  V|3871.5|  V|3862.5|  V|3861.0|  V|3861.0|  V|   NaN|  N|3875.0|  V|3873.0|  V|3873.0|  V|3870.9|  V|3863.4|  V|3861.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  5|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10|  7|   NaN|  N| 781.0|  V| 580.0|  V| 565.0|  V| 598.0|  V| 642.0|  V|1057.0|  V|1484.0|  V|1734.0|  V|1576.0|  V|1729.0|  V|1549.0|  V|1391.0|  V|2047.0|  V|1710.0|  V|1686.0|  V|2023.0|  V|1676.0|  V|2308.0|  V|1983.0|  V|1309.0|  V|1513.0|  V| 931.0|  V| 832.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     50|              12|2022| 10| 19|  22.0|  V|  32.0|  V|  36.0|  V|  41.0|  V|  47.0|  V|  50.0|  V|  51.0|  V|  55.0|  V| 104.0|  V| 172.0|  V| 177.0|  V| 151.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|  86.0|  V|  45.0|  V|  73.0|  V|  44.0|  V|  36.0|  V|  37.0|  V|  37.0|  V|  33.0|  V|  53.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10|  1|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.8|  V|3853.0|  V|3847.9|  V|3847.2|  V|3847.9|  V|3846.3|  V|3844.2|  V|3843.6|  V|3843.3|  V|3843.1|  V|3853.0|  V|3893.0|  V|3893.0|  V|3893.0|  V|2022_10|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:11:25 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:11:29 WARN DAGScheduler: Broadcasting large task binary with size 1507.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: h16\n",
      "Q1: 3.0\n",
      "Q3: 40.0\n",
      "IQR: 37.0\n",
      "Lower Bound: -108.0\n",
      "Upper Bound: 151.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:11:32 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:11:35 WARN DAGScheduler: Broadcasting large task binary with size 1596.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 13005\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:11:39 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:11:42 WARN DAGScheduler: Broadcasting large task binary with size 1621.0 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|codi_provincia|provincia|codi_municipi| municipi|estacio|codi_contaminant| any|mes|dia|   h01|v01|   h02|v02|   h03|v03|   h04|v04|   h05|v05|   h06|v06|   h07|v07|   h08|v08|   h09|v09|   h10|v10|   h11|v11|   h12|v12|   h13|v13|   h14|v14|   h15|v15|   h16|v16|   h17|v17|   h18|v18|   h19|v19|   h20|v20|   h21|v21|   h22|v22|   h23|v23|   h24|v24|  month|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 11| 771.0|  V|1024.0|  V| 998.0|  V| 659.0|  V| 641.0|  V| 734.0|  V| 846.0|  V|1278.0|  V|1077.0|  V|1086.0|  V|1096.0|  V|1746.0|  V|1653.0|  V| 825.0|  V| 707.0|  V| 730.0|  V| 833.0|  V| 791.0|  V| 944.0|  V|1002.0|  V| 949.0|  V|1116.0|  V|1621.0|  V|5794.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 22|1146.0|  V| 915.0|  V| 471.0|  V| 288.0|  V| 215.0|  V| 256.0|  V| 521.0|  V| 458.0|  V| 419.0|  V| 374.0|  V| 479.0|  V| 449.0|  V| 392.0|  V| 338.0|  V| 502.0|  V| 575.0|  V| 353.0|  V| 365.0|  V| 512.0|  V|1238.0|  V|2378.0|  V|2081.0|  V|3206.0|  V|1822.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     57|             112|2022| 10| 10| 35.38|  V|  9.77|  V|  4.19|  V|  9.68|  V|  7.23|  V|  7.22|  V| 11.59|  V| 34.24|  V| 83.17|  V| 67.63|  V| 69.93|  V| 53.76|  V| 34.98|  V|   NaN|  N|   NaN|  N|   NaN|  N| 23.42|  V| 28.96|  V| 27.81|  V| 27.49|  V| 18.18|  V| 13.44|  V| 13.53|  V| 29.74|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  3|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 20|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 24|1921.0|  V|1593.0|  V|2850.0|  V|1949.0|  V|1559.0|  V|1102.0|  V|1619.0|  V|3558.0|  V|4674.0|  V|2410.0|  V|1657.0|  V|1159.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|1549.0|  V|1891.0|  V| 750.0|  V| 996.0|  V|1629.0|  V|2417.0|  V|2799.0|  V|2117.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 18|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3864.8|  V|3886.1|  V|3893.0|  V|3893.0|  V|3892.5|  V|3892.4|  V|3888.9|  V|3881.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3885.5|  V|3893.0|  V|3892.8|  V|3883.7|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 26|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             996|2022| 10| 21|   NaN|  N|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1115.2|  V|1123.7|  V|1126.0|  V|1126.0|  V|1126.0|  V|1128.9|  V|1137.6|  V|1133.0|  V|1133.7|  V|1138.7|  V|1112.6|  V|1101.1|  V|1106.9|  V|1113.6|  V|1114.0|  V|1114.0|  V|1114.7|  V|1124.2|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 16|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10|  4|1109.0|  V| 961.0|  V| 676.0|  V|1247.0|  V| 603.0|  V| 947.0|  V|1866.0|  V|3970.0|  V|5062.0|  V|2724.0|  V|2478.0|  V|2210.0|  V|2092.0|  V|1366.0|  V|1361.0|  V|1221.0|  V|1294.0|  V|1971.0|  V|2911.0|  V|2825.0|  V|1434.0|  V|1555.0|  V|1135.0|  V|1603.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     57|             101|2022| 10| 10|  2.45|  V|  2.41|  V|  2.32|  V|  2.41|  V|  2.26|  V|  2.15|  V|  2.18|  V|  2.34|  V|  2.62|  V|  2.43|  V|  2.72|  V|  2.65|  V|  2.95|  V|   NaN|  N|   NaN|  N|   NaN|  N|  2.08|  V|  2.26|  V|  1.92|  V|  1.88|  V|  1.98|  V|  1.92|  V|  2.04|  V|  2.07|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 30|3873.0|  V|3888.3|  V|3885.0|  V|3885.0|  V|3883.7|  V|3876.5|  V|3873.0|  V|   NaN|  N|3883.1|  V|3876.0|  V|3873.0|  V|3873.0|  V|3873.0|  V|3871.5|  V|3862.5|  V|3861.0|  V|3861.0|  V|   NaN|  N|3875.0|  V|3873.0|  V|3873.0|  V|3870.9|  V|3863.4|  V|3861.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  5|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10|  7|   NaN|  N| 781.0|  V| 580.0|  V| 565.0|  V| 598.0|  V| 642.0|  V|1057.0|  V|1484.0|  V|1734.0|  V|1576.0|  V|1729.0|  V|1549.0|  V|1391.0|  V|2047.0|  V|1710.0|  V|1686.0|  V|2023.0|  V|1676.0|  V|2308.0|  V|1983.0|  V|1309.0|  V|1513.0|  V| 931.0|  V| 832.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10|  1|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.8|  V|3853.0|  V|3847.9|  V|3847.2|  V|3847.9|  V|3846.3|  V|3844.2|  V|3843.6|  V|3843.3|  V|3843.1|  V|3853.0|  V|3893.0|  V|3893.0|  V|3893.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 13|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.9|  V|3852.3|  V|3846.3|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3888.8|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3868.1|  V|3868.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 16|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 24|1816.0|  V|1310.0|  V|1025.0|  V| 588.0|  V| 590.0|  V| 443.0|  V| 430.0|  V| 734.0|  V|1792.0|  V|1898.0|  V| 674.0|  V| 616.0|  V| 636.0|  V| 857.0|  V|1151.0|  V| 986.0|  V|1129.0|  V| 609.0|  V| 486.0|  V| 652.0|  V| 538.0|  V| 599.0|  V| 534.0|  V| 419.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 30|5860.0|  V|3499.0|  V|2035.0|  V|1272.0|  V|1355.0|  V|2306.0|  V|2439.0|  V|   NaN|  N|1736.0|  V|1910.0|  V|1579.0|  V|1364.0|  V|1696.0|  V|1862.0|  V|2094.0|  V|1949.0|  V|1686.0|  V|   NaN|  N|2125.0|  V|2665.0|  V|3017.0|  V|2511.0|  V|2709.0|  V|2312.0|  V|2022_10|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:11:44 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:11:49 WARN DAGScheduler: Broadcasting large task binary with size 1507.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: h17\n",
      "Q1: 2.0\n",
      "Q3: 44.0\n",
      "IQR: 42.0\n",
      "Lower Bound: -124.0\n",
      "Upper Bound: 170.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:11:52 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:11:55 WARN DAGScheduler: Broadcasting large task binary with size 1596.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 11936\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:11:58 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:12:01 WARN DAGScheduler: Broadcasting large task binary with size 1621.0 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|codi_provincia|provincia|codi_municipi| municipi|estacio|codi_contaminant| any|mes|dia|   h01|v01|   h02|v02|   h03|v03|   h04|v04|   h05|v05|   h06|v06|   h07|v07|   h08|v08|   h09|v09|   h10|v10|   h11|v11|   h12|v12|   h13|v13|   h14|v14|   h15|v15|   h16|v16|   h17|v17|   h18|v18|   h19|v19|   h20|v20|   h21|v21|   h22|v22|   h23|v23|   h24|v24|  month|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 11| 771.0|  V|1024.0|  V| 998.0|  V| 659.0|  V| 641.0|  V| 734.0|  V| 846.0|  V|1278.0|  V|1077.0|  V|1086.0|  V|1096.0|  V|1746.0|  V|1653.0|  V| 825.0|  V| 707.0|  V| 730.0|  V| 833.0|  V| 791.0|  V| 944.0|  V|1002.0|  V| 949.0|  V|1116.0|  V|1621.0|  V|5794.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 22|1146.0|  V| 915.0|  V| 471.0|  V| 288.0|  V| 215.0|  V| 256.0|  V| 521.0|  V| 458.0|  V| 419.0|  V| 374.0|  V| 479.0|  V| 449.0|  V| 392.0|  V| 338.0|  V| 502.0|  V| 575.0|  V| 353.0|  V| 365.0|  V| 512.0|  V|1238.0|  V|2378.0|  V|2081.0|  V|3206.0|  V|1822.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  3|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 20|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 24|1921.0|  V|1593.0|  V|2850.0|  V|1949.0|  V|1559.0|  V|1102.0|  V|1619.0|  V|3558.0|  V|4674.0|  V|2410.0|  V|1657.0|  V|1159.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|1549.0|  V|1891.0|  V| 750.0|  V| 996.0|  V|1629.0|  V|2417.0|  V|2799.0|  V|2117.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 18|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3864.8|  V|3886.1|  V|3893.0|  V|3893.0|  V|3892.5|  V|3892.4|  V|3888.9|  V|3881.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3885.5|  V|3893.0|  V|3892.8|  V|3883.7|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 26|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             996|2022| 10| 21|   NaN|  N|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1115.2|  V|1123.7|  V|1126.0|  V|1126.0|  V|1126.0|  V|1128.9|  V|1137.6|  V|1133.0|  V|1133.7|  V|1138.7|  V|1112.6|  V|1101.1|  V|1106.9|  V|1113.6|  V|1114.0|  V|1114.0|  V|1114.7|  V|1124.2|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 16|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10|  4|1109.0|  V| 961.0|  V| 676.0|  V|1247.0|  V| 603.0|  V| 947.0|  V|1866.0|  V|3970.0|  V|5062.0|  V|2724.0|  V|2478.0|  V|2210.0|  V|2092.0|  V|1366.0|  V|1361.0|  V|1221.0|  V|1294.0|  V|1971.0|  V|2911.0|  V|2825.0|  V|1434.0|  V|1555.0|  V|1135.0|  V|1603.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 30|3873.0|  V|3888.3|  V|3885.0|  V|3885.0|  V|3883.7|  V|3876.5|  V|3873.0|  V|   NaN|  N|3883.1|  V|3876.0|  V|3873.0|  V|3873.0|  V|3873.0|  V|3871.5|  V|3862.5|  V|3861.0|  V|3861.0|  V|   NaN|  N|3875.0|  V|3873.0|  V|3873.0|  V|3870.9|  V|3863.4|  V|3861.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  5|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10|  7|   NaN|  N| 781.0|  V| 580.0|  V| 565.0|  V| 598.0|  V| 642.0|  V|1057.0|  V|1484.0|  V|1734.0|  V|1576.0|  V|1729.0|  V|1549.0|  V|1391.0|  V|2047.0|  V|1710.0|  V|1686.0|  V|2023.0|  V|1676.0|  V|2308.0|  V|1983.0|  V|1309.0|  V|1513.0|  V| 931.0|  V| 832.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10|  1|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.8|  V|3853.0|  V|3847.9|  V|3847.2|  V|3847.9|  V|3846.3|  V|3844.2|  V|3843.6|  V|3843.3|  V|3843.1|  V|3853.0|  V|3893.0|  V|3893.0|  V|3893.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 13|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.9|  V|3852.3|  V|3846.3|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3888.8|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3868.1|  V|3868.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 16|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 24|1816.0|  V|1310.0|  V|1025.0|  V| 588.0|  V| 590.0|  V| 443.0|  V| 430.0|  V| 734.0|  V|1792.0|  V|1898.0|  V| 674.0|  V| 616.0|  V| 636.0|  V| 857.0|  V|1151.0|  V| 986.0|  V|1129.0|  V| 609.0|  V| 486.0|  V| 652.0|  V| 538.0|  V| 599.0|  V| 534.0|  V| 419.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 30|5860.0|  V|3499.0|  V|2035.0|  V|1272.0|  V|1355.0|  V|2306.0|  V|2439.0|  V|   NaN|  N|1736.0|  V|1910.0|  V|1579.0|  V|1364.0|  V|1696.0|  V|1862.0|  V|2094.0|  V|1949.0|  V|1686.0|  V|   NaN|  N|2125.0|  V|2665.0|  V|3017.0|  V|2511.0|  V|2709.0|  V|2312.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 28|   NaN|  N|3885.0|  V|3884.3|  V|3880.1|  V|3874.4|  V|3873.0|  V|3873.0|  V|   NaN|  N|3849.1|  V|3846.3|  V|3837.2|  V|   NaN|  N|3873.0|  V|3870.1|  V|3861.8|  V|3861.0|  V|3861.0|  V|3859.4|  V|   NaN|  N|3863.4|  V|3861.0|  V|3859.9|  V|3851.2|  V|3849.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 19|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:12:04 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:12:09 WARN DAGScheduler: Broadcasting large task binary with size 1507.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: h18\n",
      "Q1: 2.0\n",
      "Q3: 47.0\n",
      "IQR: 45.0\n",
      "Lower Bound: -133.0\n",
      "Upper Bound: 182.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:12:12 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:12:15 WARN DAGScheduler: Broadcasting large task binary with size 1596.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 11869\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:12:18 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:12:21 WARN DAGScheduler: Broadcasting large task binary with size 1621.0 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|codi_provincia|provincia|codi_municipi| municipi|estacio|codi_contaminant| any|mes|dia|   h01|v01|   h02|v02|   h03|v03|   h04|v04|   h05|v05|   h06|v06|   h07|v07|   h08|v08|   h09|v09|   h10|v10|   h11|v11|   h12|v12|   h13|v13|   h14|v14|   h15|v15|   h16|v16|   h17|v17|   h18|v18|   h19|v19|   h20|v20|   h21|v21|   h22|v22|   h23|v23|   h24|v24|  month|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 11| 771.0|  V|1024.0|  V| 998.0|  V| 659.0|  V| 641.0|  V| 734.0|  V| 846.0|  V|1278.0|  V|1077.0|  V|1086.0|  V|1096.0|  V|1746.0|  V|1653.0|  V| 825.0|  V| 707.0|  V| 730.0|  V| 833.0|  V| 791.0|  V| 944.0|  V|1002.0|  V| 949.0|  V|1116.0|  V|1621.0|  V|5794.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 22|1146.0|  V| 915.0|  V| 471.0|  V| 288.0|  V| 215.0|  V| 256.0|  V| 521.0|  V| 458.0|  V| 419.0|  V| 374.0|  V| 479.0|  V| 449.0|  V| 392.0|  V| 338.0|  V| 502.0|  V| 575.0|  V| 353.0|  V| 365.0|  V| 512.0|  V|1238.0|  V|2378.0|  V|2081.0|  V|3206.0|  V|1822.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  3|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 20|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 24|1921.0|  V|1593.0|  V|2850.0|  V|1949.0|  V|1559.0|  V|1102.0|  V|1619.0|  V|3558.0|  V|4674.0|  V|2410.0|  V|1657.0|  V|1159.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|1549.0|  V|1891.0|  V| 750.0|  V| 996.0|  V|1629.0|  V|2417.0|  V|2799.0|  V|2117.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 18|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3864.8|  V|3886.1|  V|3893.0|  V|3893.0|  V|3892.5|  V|3892.4|  V|3888.9|  V|3881.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3885.5|  V|3893.0|  V|3892.8|  V|3883.7|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 26|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             996|2022| 10| 21|   NaN|  N|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1115.2|  V|1123.7|  V|1126.0|  V|1126.0|  V|1126.0|  V|1128.9|  V|1137.6|  V|1133.0|  V|1133.7|  V|1138.7|  V|1112.6|  V|1101.1|  V|1106.9|  V|1113.6|  V|1114.0|  V|1114.0|  V|1114.7|  V|1124.2|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 16|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10|  4|1109.0|  V| 961.0|  V| 676.0|  V|1247.0|  V| 603.0|  V| 947.0|  V|1866.0|  V|3970.0|  V|5062.0|  V|2724.0|  V|2478.0|  V|2210.0|  V|2092.0|  V|1366.0|  V|1361.0|  V|1221.0|  V|1294.0|  V|1971.0|  V|2911.0|  V|2825.0|  V|1434.0|  V|1555.0|  V|1135.0|  V|1603.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 30|3873.0|  V|3888.3|  V|3885.0|  V|3885.0|  V|3883.7|  V|3876.5|  V|3873.0|  V|   NaN|  N|3883.1|  V|3876.0|  V|3873.0|  V|3873.0|  V|3873.0|  V|3871.5|  V|3862.5|  V|3861.0|  V|3861.0|  V|   NaN|  N|3875.0|  V|3873.0|  V|3873.0|  V|3870.9|  V|3863.4|  V|3861.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  5|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10|  7|   NaN|  N| 781.0|  V| 580.0|  V| 565.0|  V| 598.0|  V| 642.0|  V|1057.0|  V|1484.0|  V|1734.0|  V|1576.0|  V|1729.0|  V|1549.0|  V|1391.0|  V|2047.0|  V|1710.0|  V|1686.0|  V|2023.0|  V|1676.0|  V|2308.0|  V|1983.0|  V|1309.0|  V|1513.0|  V| 931.0|  V| 832.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10|  1|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.8|  V|3853.0|  V|3847.9|  V|3847.2|  V|3847.9|  V|3846.3|  V|3844.2|  V|3843.6|  V|3843.3|  V|3843.1|  V|3853.0|  V|3893.0|  V|3893.0|  V|3893.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 13|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.9|  V|3852.3|  V|3846.3|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3888.8|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3868.1|  V|3868.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 16|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 24|1816.0|  V|1310.0|  V|1025.0|  V| 588.0|  V| 590.0|  V| 443.0|  V| 430.0|  V| 734.0|  V|1792.0|  V|1898.0|  V| 674.0|  V| 616.0|  V| 636.0|  V| 857.0|  V|1151.0|  V| 986.0|  V|1129.0|  V| 609.0|  V| 486.0|  V| 652.0|  V| 538.0|  V| 599.0|  V| 534.0|  V| 419.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 30|5860.0|  V|3499.0|  V|2035.0|  V|1272.0|  V|1355.0|  V|2306.0|  V|2439.0|  V|   NaN|  N|1736.0|  V|1910.0|  V|1579.0|  V|1364.0|  V|1696.0|  V|1862.0|  V|2094.0|  V|1949.0|  V|1686.0|  V|   NaN|  N|2125.0|  V|2665.0|  V|3017.0|  V|2511.0|  V|2709.0|  V|2312.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 28|   NaN|  N|3885.0|  V|3884.3|  V|3880.1|  V|3874.4|  V|3873.0|  V|3873.0|  V|   NaN|  N|3849.1|  V|3846.3|  V|3837.2|  V|   NaN|  N|3873.0|  V|3870.1|  V|3861.8|  V|3861.0|  V|3861.0|  V|3859.4|  V|   NaN|  N|3863.4|  V|3861.0|  V|3859.9|  V|3851.2|  V|3849.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 19|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:12:24 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:12:28 WARN DAGScheduler: Broadcasting large task binary with size 1507.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: h19\n",
      "Q1: 2.0\n",
      "Q3: 48.0\n",
      "IQR: 46.0\n",
      "Lower Bound: -136.0\n",
      "Upper Bound: 186.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:12:31 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:12:34 WARN DAGScheduler: Broadcasting large task binary with size 1596.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 11919\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:12:37 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:12:41 WARN DAGScheduler: Broadcasting large task binary with size 1621.0 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|codi_provincia|provincia|codi_municipi| municipi|estacio|codi_contaminant| any|mes|dia|   h01|v01|   h02|v02|   h03|v03|   h04|v04|   h05|v05|   h06|v06|   h07|v07|   h08|v08|   h09|v09|   h10|v10|   h11|v11|   h12|v12|   h13|v13|   h14|v14|   h15|v15|   h16|v16|   h17|v17|   h18|v18|   h19|v19|   h20|v20|   h21|v21|   h22|v22|   h23|v23|   h24|v24|  month|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 11| 771.0|  V|1024.0|  V| 998.0|  V| 659.0|  V| 641.0|  V| 734.0|  V| 846.0|  V|1278.0|  V|1077.0|  V|1086.0|  V|1096.0|  V|1746.0|  V|1653.0|  V| 825.0|  V| 707.0|  V| 730.0|  V| 833.0|  V| 791.0|  V| 944.0|  V|1002.0|  V| 949.0|  V|1116.0|  V|1621.0|  V|5794.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 22|1146.0|  V| 915.0|  V| 471.0|  V| 288.0|  V| 215.0|  V| 256.0|  V| 521.0|  V| 458.0|  V| 419.0|  V| 374.0|  V| 479.0|  V| 449.0|  V| 392.0|  V| 338.0|  V| 502.0|  V| 575.0|  V| 353.0|  V| 365.0|  V| 512.0|  V|1238.0|  V|2378.0|  V|2081.0|  V|3206.0|  V|1822.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  3|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 20|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 24|1921.0|  V|1593.0|  V|2850.0|  V|1949.0|  V|1559.0|  V|1102.0|  V|1619.0|  V|3558.0|  V|4674.0|  V|2410.0|  V|1657.0|  V|1159.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|1549.0|  V|1891.0|  V| 750.0|  V| 996.0|  V|1629.0|  V|2417.0|  V|2799.0|  V|2117.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 18|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3864.8|  V|3886.1|  V|3893.0|  V|3893.0|  V|3892.5|  V|3892.4|  V|3888.9|  V|3881.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3885.5|  V|3893.0|  V|3892.8|  V|3883.7|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 26|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             996|2022| 10| 21|   NaN|  N|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1115.2|  V|1123.7|  V|1126.0|  V|1126.0|  V|1126.0|  V|1128.9|  V|1137.6|  V|1133.0|  V|1133.7|  V|1138.7|  V|1112.6|  V|1101.1|  V|1106.9|  V|1113.6|  V|1114.0|  V|1114.0|  V|1114.7|  V|1124.2|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 16|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10|  4|1109.0|  V| 961.0|  V| 676.0|  V|1247.0|  V| 603.0|  V| 947.0|  V|1866.0|  V|3970.0|  V|5062.0|  V|2724.0|  V|2478.0|  V|2210.0|  V|2092.0|  V|1366.0|  V|1361.0|  V|1221.0|  V|1294.0|  V|1971.0|  V|2911.0|  V|2825.0|  V|1434.0|  V|1555.0|  V|1135.0|  V|1603.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 30|3873.0|  V|3888.3|  V|3885.0|  V|3885.0|  V|3883.7|  V|3876.5|  V|3873.0|  V|   NaN|  N|3883.1|  V|3876.0|  V|3873.0|  V|3873.0|  V|3873.0|  V|3871.5|  V|3862.5|  V|3861.0|  V|3861.0|  V|   NaN|  N|3875.0|  V|3873.0|  V|3873.0|  V|3870.9|  V|3863.4|  V|3861.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  5|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10|  7|   NaN|  N| 781.0|  V| 580.0|  V| 565.0|  V| 598.0|  V| 642.0|  V|1057.0|  V|1484.0|  V|1734.0|  V|1576.0|  V|1729.0|  V|1549.0|  V|1391.0|  V|2047.0|  V|1710.0|  V|1686.0|  V|2023.0|  V|1676.0|  V|2308.0|  V|1983.0|  V|1309.0|  V|1513.0|  V| 931.0|  V| 832.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10|  1|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.8|  V|3853.0|  V|3847.9|  V|3847.2|  V|3847.9|  V|3846.3|  V|3844.2|  V|3843.6|  V|3843.3|  V|3843.1|  V|3853.0|  V|3893.0|  V|3893.0|  V|3893.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 13|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.9|  V|3852.3|  V|3846.3|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3888.8|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3868.1|  V|3868.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 16|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 24|1816.0|  V|1310.0|  V|1025.0|  V| 588.0|  V| 590.0|  V| 443.0|  V| 430.0|  V| 734.0|  V|1792.0|  V|1898.0|  V| 674.0|  V| 616.0|  V| 636.0|  V| 857.0|  V|1151.0|  V| 986.0|  V|1129.0|  V| 609.0|  V| 486.0|  V| 652.0|  V| 538.0|  V| 599.0|  V| 534.0|  V| 419.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 30|5860.0|  V|3499.0|  V|2035.0|  V|1272.0|  V|1355.0|  V|2306.0|  V|2439.0|  V|   NaN|  N|1736.0|  V|1910.0|  V|1579.0|  V|1364.0|  V|1696.0|  V|1862.0|  V|2094.0|  V|1949.0|  V|1686.0|  V|   NaN|  N|2125.0|  V|2665.0|  V|3017.0|  V|2511.0|  V|2709.0|  V|2312.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 28|   NaN|  N|3885.0|  V|3884.3|  V|3880.1|  V|3874.4|  V|3873.0|  V|3873.0|  V|   NaN|  N|3849.1|  V|3846.3|  V|3837.2|  V|   NaN|  N|3873.0|  V|3870.1|  V|3861.8|  V|3861.0|  V|3861.0|  V|3859.4|  V|   NaN|  N|3863.4|  V|3861.0|  V|3859.9|  V|3851.2|  V|3849.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 19|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:12:43 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:12:48 WARN DAGScheduler: Broadcasting large task binary with size 1507.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: h20\n",
      "Q1: 2.0\n",
      "Q3: 49.0\n",
      "IQR: 47.0\n",
      "Lower Bound: -139.0\n",
      "Upper Bound: 190.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:12:50 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:12:54 WARN DAGScheduler: Broadcasting large task binary with size 1596.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 11958\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:12:57 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:13:00 WARN DAGScheduler: Broadcasting large task binary with size 1621.0 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|codi_provincia|provincia|codi_municipi| municipi|estacio|codi_contaminant| any|mes|dia|   h01|v01|   h02|v02|   h03|v03|   h04|v04|   h05|v05|   h06|v06|   h07|v07|   h08|v08|   h09|v09|   h10|v10|   h11|v11|   h12|v12|   h13|v13|   h14|v14|   h15|v15|   h16|v16|   h17|v17|   h18|v18|   h19|v19|   h20|v20|   h21|v21|   h22|v22|   h23|v23|   h24|v24|  month|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 11| 771.0|  V|1024.0|  V| 998.0|  V| 659.0|  V| 641.0|  V| 734.0|  V| 846.0|  V|1278.0|  V|1077.0|  V|1086.0|  V|1096.0|  V|1746.0|  V|1653.0|  V| 825.0|  V| 707.0|  V| 730.0|  V| 833.0|  V| 791.0|  V| 944.0|  V|1002.0|  V| 949.0|  V|1116.0|  V|1621.0|  V|5794.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 22|1146.0|  V| 915.0|  V| 471.0|  V| 288.0|  V| 215.0|  V| 256.0|  V| 521.0|  V| 458.0|  V| 419.0|  V| 374.0|  V| 479.0|  V| 449.0|  V| 392.0|  V| 338.0|  V| 502.0|  V| 575.0|  V| 353.0|  V| 365.0|  V| 512.0|  V|1238.0|  V|2378.0|  V|2081.0|  V|3206.0|  V|1822.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  3|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 20|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 24|1921.0|  V|1593.0|  V|2850.0|  V|1949.0|  V|1559.0|  V|1102.0|  V|1619.0|  V|3558.0|  V|4674.0|  V|2410.0|  V|1657.0|  V|1159.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|1549.0|  V|1891.0|  V| 750.0|  V| 996.0|  V|1629.0|  V|2417.0|  V|2799.0|  V|2117.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 18|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3864.8|  V|3886.1|  V|3893.0|  V|3893.0|  V|3892.5|  V|3892.4|  V|3888.9|  V|3881.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3885.5|  V|3893.0|  V|3892.8|  V|3883.7|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 26|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             996|2022| 10| 21|   NaN|  N|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1115.2|  V|1123.7|  V|1126.0|  V|1126.0|  V|1126.0|  V|1128.9|  V|1137.6|  V|1133.0|  V|1133.7|  V|1138.7|  V|1112.6|  V|1101.1|  V|1106.9|  V|1113.6|  V|1114.0|  V|1114.0|  V|1114.7|  V|1124.2|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 16|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10|  4|1109.0|  V| 961.0|  V| 676.0|  V|1247.0|  V| 603.0|  V| 947.0|  V|1866.0|  V|3970.0|  V|5062.0|  V|2724.0|  V|2478.0|  V|2210.0|  V|2092.0|  V|1366.0|  V|1361.0|  V|1221.0|  V|1294.0|  V|1971.0|  V|2911.0|  V|2825.0|  V|1434.0|  V|1555.0|  V|1135.0|  V|1603.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 30|3873.0|  V|3888.3|  V|3885.0|  V|3885.0|  V|3883.7|  V|3876.5|  V|3873.0|  V|   NaN|  N|3883.1|  V|3876.0|  V|3873.0|  V|3873.0|  V|3873.0|  V|3871.5|  V|3862.5|  V|3861.0|  V|3861.0|  V|   NaN|  N|3875.0|  V|3873.0|  V|3873.0|  V|3870.9|  V|3863.4|  V|3861.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  5|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10|  7|   NaN|  N| 781.0|  V| 580.0|  V| 565.0|  V| 598.0|  V| 642.0|  V|1057.0|  V|1484.0|  V|1734.0|  V|1576.0|  V|1729.0|  V|1549.0|  V|1391.0|  V|2047.0|  V|1710.0|  V|1686.0|  V|2023.0|  V|1676.0|  V|2308.0|  V|1983.0|  V|1309.0|  V|1513.0|  V| 931.0|  V| 832.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10|  1|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.8|  V|3853.0|  V|3847.9|  V|3847.2|  V|3847.9|  V|3846.3|  V|3844.2|  V|3843.6|  V|3843.3|  V|3843.1|  V|3853.0|  V|3893.0|  V|3893.0|  V|3893.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 13|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.9|  V|3852.3|  V|3846.3|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3888.8|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3868.1|  V|3868.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 16|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 24|1816.0|  V|1310.0|  V|1025.0|  V| 588.0|  V| 590.0|  V| 443.0|  V| 430.0|  V| 734.0|  V|1792.0|  V|1898.0|  V| 674.0|  V| 616.0|  V| 636.0|  V| 857.0|  V|1151.0|  V| 986.0|  V|1129.0|  V| 609.0|  V| 486.0|  V| 652.0|  V| 538.0|  V| 599.0|  V| 534.0|  V| 419.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 30|5860.0|  V|3499.0|  V|2035.0|  V|1272.0|  V|1355.0|  V|2306.0|  V|2439.0|  V|   NaN|  N|1736.0|  V|1910.0|  V|1579.0|  V|1364.0|  V|1696.0|  V|1862.0|  V|2094.0|  V|1949.0|  V|1686.0|  V|   NaN|  N|2125.0|  V|2665.0|  V|3017.0|  V|2511.0|  V|2709.0|  V|2312.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 28|   NaN|  N|3885.0|  V|3884.3|  V|3880.1|  V|3874.4|  V|3873.0|  V|3873.0|  V|   NaN|  N|3849.1|  V|3846.3|  V|3837.2|  V|   NaN|  N|3873.0|  V|3870.1|  V|3861.8|  V|3861.0|  V|3861.0|  V|3859.4|  V|   NaN|  N|3863.4|  V|3861.0|  V|3859.9|  V|3851.2|  V|3849.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 19|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:13:03 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:13:07 WARN DAGScheduler: Broadcasting large task binary with size 1507.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: h21\n",
      "Q1: 2.0\n",
      "Q3: 48.0\n",
      "IQR: 46.0\n",
      "Lower Bound: -136.0\n",
      "Upper Bound: 186.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:13:10 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:13:13 WARN DAGScheduler: Broadcasting large task binary with size 1596.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 12244\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:13:16 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:13:20 WARN DAGScheduler: Broadcasting large task binary with size 1621.0 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|codi_provincia|provincia|codi_municipi| municipi|estacio|codi_contaminant| any|mes|dia|   h01|v01|   h02|v02|   h03|v03|   h04|v04|   h05|v05|   h06|v06|   h07|v07|   h08|v08|   h09|v09|   h10|v10|   h11|v11|   h12|v12|   h13|v13|   h14|v14|   h15|v15|   h16|v16|   h17|v17|   h18|v18|   h19|v19|   h20|v20|   h21|v21|   h22|v22|   h23|v23|   h24|v24|  month|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 11| 771.0|  V|1024.0|  V| 998.0|  V| 659.0|  V| 641.0|  V| 734.0|  V| 846.0|  V|1278.0|  V|1077.0|  V|1086.0|  V|1096.0|  V|1746.0|  V|1653.0|  V| 825.0|  V| 707.0|  V| 730.0|  V| 833.0|  V| 791.0|  V| 944.0|  V|1002.0|  V| 949.0|  V|1116.0|  V|1621.0|  V|5794.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 22|1146.0|  V| 915.0|  V| 471.0|  V| 288.0|  V| 215.0|  V| 256.0|  V| 521.0|  V| 458.0|  V| 419.0|  V| 374.0|  V| 479.0|  V| 449.0|  V| 392.0|  V| 338.0|  V| 502.0|  V| 575.0|  V| 353.0|  V| 365.0|  V| 512.0|  V|1238.0|  V|2378.0|  V|2081.0|  V|3206.0|  V|1822.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  3|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 20|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 24|1921.0|  V|1593.0|  V|2850.0|  V|1949.0|  V|1559.0|  V|1102.0|  V|1619.0|  V|3558.0|  V|4674.0|  V|2410.0|  V|1657.0|  V|1159.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|1549.0|  V|1891.0|  V| 750.0|  V| 996.0|  V|1629.0|  V|2417.0|  V|2799.0|  V|2117.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 18|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3864.8|  V|3886.1|  V|3893.0|  V|3893.0|  V|3892.5|  V|3892.4|  V|3888.9|  V|3881.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3885.5|  V|3893.0|  V|3892.8|  V|3883.7|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 26|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             996|2022| 10| 21|   NaN|  N|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1115.2|  V|1123.7|  V|1126.0|  V|1126.0|  V|1126.0|  V|1128.9|  V|1137.6|  V|1133.0|  V|1133.7|  V|1138.7|  V|1112.6|  V|1101.1|  V|1106.9|  V|1113.6|  V|1114.0|  V|1114.0|  V|1114.7|  V|1124.2|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 16|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10|  4|1109.0|  V| 961.0|  V| 676.0|  V|1247.0|  V| 603.0|  V| 947.0|  V|1866.0|  V|3970.0|  V|5062.0|  V|2724.0|  V|2478.0|  V|2210.0|  V|2092.0|  V|1366.0|  V|1361.0|  V|1221.0|  V|1294.0|  V|1971.0|  V|2911.0|  V|2825.0|  V|1434.0|  V|1555.0|  V|1135.0|  V|1603.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 30|3873.0|  V|3888.3|  V|3885.0|  V|3885.0|  V|3883.7|  V|3876.5|  V|3873.0|  V|   NaN|  N|3883.1|  V|3876.0|  V|3873.0|  V|3873.0|  V|3873.0|  V|3871.5|  V|3862.5|  V|3861.0|  V|3861.0|  V|   NaN|  N|3875.0|  V|3873.0|  V|3873.0|  V|3870.9|  V|3863.4|  V|3861.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  5|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10|  7|   NaN|  N| 781.0|  V| 580.0|  V| 565.0|  V| 598.0|  V| 642.0|  V|1057.0|  V|1484.0|  V|1734.0|  V|1576.0|  V|1729.0|  V|1549.0|  V|1391.0|  V|2047.0|  V|1710.0|  V|1686.0|  V|2023.0|  V|1676.0|  V|2308.0|  V|1983.0|  V|1309.0|  V|1513.0|  V| 931.0|  V| 832.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10|  1|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.8|  V|3853.0|  V|3847.9|  V|3847.2|  V|3847.9|  V|3846.3|  V|3844.2|  V|3843.6|  V|3843.3|  V|3843.1|  V|3853.0|  V|3893.0|  V|3893.0|  V|3893.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 13|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.9|  V|3852.3|  V|3846.3|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3888.8|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3868.1|  V|3868.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 16|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 24|1816.0|  V|1310.0|  V|1025.0|  V| 588.0|  V| 590.0|  V| 443.0|  V| 430.0|  V| 734.0|  V|1792.0|  V|1898.0|  V| 674.0|  V| 616.0|  V| 636.0|  V| 857.0|  V|1151.0|  V| 986.0|  V|1129.0|  V| 609.0|  V| 486.0|  V| 652.0|  V| 538.0|  V| 599.0|  V| 534.0|  V| 419.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 30|5860.0|  V|3499.0|  V|2035.0|  V|1272.0|  V|1355.0|  V|2306.0|  V|2439.0|  V|   NaN|  N|1736.0|  V|1910.0|  V|1579.0|  V|1364.0|  V|1696.0|  V|1862.0|  V|2094.0|  V|1949.0|  V|1686.0|  V|   NaN|  N|2125.0|  V|2665.0|  V|3017.0|  V|2511.0|  V|2709.0|  V|2312.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 28|   NaN|  N|3885.0|  V|3884.3|  V|3880.1|  V|3874.4|  V|3873.0|  V|3873.0|  V|   NaN|  N|3849.1|  V|3846.3|  V|3837.2|  V|   NaN|  N|3873.0|  V|3870.1|  V|3861.8|  V|3861.0|  V|3861.0|  V|3859.4|  V|   NaN|  N|3863.4|  V|3861.0|  V|3859.9|  V|3851.2|  V|3849.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 19|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:13:22 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:13:26 WARN DAGScheduler: Broadcasting large task binary with size 1507.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: h22\n",
      "Q1: 2.0\n",
      "Q3: 49.0\n",
      "IQR: 47.0\n",
      "Lower Bound: -139.0\n",
      "Upper Bound: 190.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:13:29 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:13:33 WARN DAGScheduler: Broadcasting large task binary with size 1596.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 12405\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:13:36 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:13:39 WARN DAGScheduler: Broadcasting large task binary with size 1621.0 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|codi_provincia|provincia|codi_municipi| municipi|estacio|codi_contaminant| any|mes|dia|   h01|v01|   h02|v02|   h03|v03|   h04|v04|   h05|v05|   h06|v06|   h07|v07|   h08|v08|   h09|v09|   h10|v10|   h11|v11|   h12|v12|   h13|v13|   h14|v14|   h15|v15|   h16|v16|   h17|v17|   h18|v18|   h19|v19|   h20|v20|   h21|v21|   h22|v22|   h23|v23|   h24|v24|  month|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 11| 771.0|  V|1024.0|  V| 998.0|  V| 659.0|  V| 641.0|  V| 734.0|  V| 846.0|  V|1278.0|  V|1077.0|  V|1086.0|  V|1096.0|  V|1746.0|  V|1653.0|  V| 825.0|  V| 707.0|  V| 730.0|  V| 833.0|  V| 791.0|  V| 944.0|  V|1002.0|  V| 949.0|  V|1116.0|  V|1621.0|  V|5794.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 22|1146.0|  V| 915.0|  V| 471.0|  V| 288.0|  V| 215.0|  V| 256.0|  V| 521.0|  V| 458.0|  V| 419.0|  V| 374.0|  V| 479.0|  V| 449.0|  V| 392.0|  V| 338.0|  V| 502.0|  V| 575.0|  V| 353.0|  V| 365.0|  V| 512.0|  V|1238.0|  V|2378.0|  V|2081.0|  V|3206.0|  V|1822.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  3|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 20|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 24|1921.0|  V|1593.0|  V|2850.0|  V|1949.0|  V|1559.0|  V|1102.0|  V|1619.0|  V|3558.0|  V|4674.0|  V|2410.0|  V|1657.0|  V|1159.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|1549.0|  V|1891.0|  V| 750.0|  V| 996.0|  V|1629.0|  V|2417.0|  V|2799.0|  V|2117.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 18|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3864.8|  V|3886.1|  V|3893.0|  V|3893.0|  V|3892.5|  V|3892.4|  V|3888.9|  V|3881.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3885.5|  V|3893.0|  V|3892.8|  V|3883.7|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 26|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             996|2022| 10| 21|   NaN|  N|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1115.2|  V|1123.7|  V|1126.0|  V|1126.0|  V|1126.0|  V|1128.9|  V|1137.6|  V|1133.0|  V|1133.7|  V|1138.7|  V|1112.6|  V|1101.1|  V|1106.9|  V|1113.6|  V|1114.0|  V|1114.0|  V|1114.7|  V|1124.2|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 16|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10|  4|1109.0|  V| 961.0|  V| 676.0|  V|1247.0|  V| 603.0|  V| 947.0|  V|1866.0|  V|3970.0|  V|5062.0|  V|2724.0|  V|2478.0|  V|2210.0|  V|2092.0|  V|1366.0|  V|1361.0|  V|1221.0|  V|1294.0|  V|1971.0|  V|2911.0|  V|2825.0|  V|1434.0|  V|1555.0|  V|1135.0|  V|1603.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 30|3873.0|  V|3888.3|  V|3885.0|  V|3885.0|  V|3883.7|  V|3876.5|  V|3873.0|  V|   NaN|  N|3883.1|  V|3876.0|  V|3873.0|  V|3873.0|  V|3873.0|  V|3871.5|  V|3862.5|  V|3861.0|  V|3861.0|  V|   NaN|  N|3875.0|  V|3873.0|  V|3873.0|  V|3870.9|  V|3863.4|  V|3861.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  5|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10|  7|   NaN|  N| 781.0|  V| 580.0|  V| 565.0|  V| 598.0|  V| 642.0|  V|1057.0|  V|1484.0|  V|1734.0|  V|1576.0|  V|1729.0|  V|1549.0|  V|1391.0|  V|2047.0|  V|1710.0|  V|1686.0|  V|2023.0|  V|1676.0|  V|2308.0|  V|1983.0|  V|1309.0|  V|1513.0|  V| 931.0|  V| 832.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10|  1|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.8|  V|3853.0|  V|3847.9|  V|3847.2|  V|3847.9|  V|3846.3|  V|3844.2|  V|3843.6|  V|3843.3|  V|3843.1|  V|3853.0|  V|3893.0|  V|3893.0|  V|3893.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 13|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.9|  V|3852.3|  V|3846.3|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3888.8|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3868.1|  V|3868.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 16|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 24|1816.0|  V|1310.0|  V|1025.0|  V| 588.0|  V| 590.0|  V| 443.0|  V| 430.0|  V| 734.0|  V|1792.0|  V|1898.0|  V| 674.0|  V| 616.0|  V| 636.0|  V| 857.0|  V|1151.0|  V| 986.0|  V|1129.0|  V| 609.0|  V| 486.0|  V| 652.0|  V| 538.0|  V| 599.0|  V| 534.0|  V| 419.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 30|5860.0|  V|3499.0|  V|2035.0|  V|1272.0|  V|1355.0|  V|2306.0|  V|2439.0|  V|   NaN|  N|1736.0|  V|1910.0|  V|1579.0|  V|1364.0|  V|1696.0|  V|1862.0|  V|2094.0|  V|1949.0|  V|1686.0|  V|   NaN|  N|2125.0|  V|2665.0|  V|3017.0|  V|2511.0|  V|2709.0|  V|2312.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 28|   NaN|  N|3885.0|  V|3884.3|  V|3880.1|  V|3874.4|  V|3873.0|  V|3873.0|  V|   NaN|  N|3849.1|  V|3846.3|  V|3837.2|  V|   NaN|  N|3873.0|  V|3870.1|  V|3861.8|  V|3861.0|  V|3861.0|  V|3859.4|  V|   NaN|  N|3863.4|  V|3861.0|  V|3859.9|  V|3851.2|  V|3849.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 19|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:13:42 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:13:46 WARN DAGScheduler: Broadcasting large task binary with size 1507.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: h23\n",
      "Q1: 2.0\n",
      "Q3: 45.0\n",
      "IQR: 43.0\n",
      "Lower Bound: -127.0\n",
      "Upper Bound: 174.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:13:49 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:13:53 WARN DAGScheduler: Broadcasting large task binary with size 1596.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 13002\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:13:56 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:13:59 WARN DAGScheduler: Broadcasting large task binary with size 1621.0 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|codi_provincia|provincia|codi_municipi| municipi|estacio|codi_contaminant| any|mes|dia|   h01|v01|   h02|v02|   h03|v03|   h04|v04|   h05|v05|   h06|v06|   h07|v07|   h08|v08|   h09|v09|   h10|v10|   h11|v11|   h12|v12|   h13|v13|   h14|v14|   h15|v15|   h16|v16|   h17|v17|   h18|v18|   h19|v19|   h20|v20|   h21|v21|   h22|v22|   h23|v23|   h24|v24|  month|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 11| 771.0|  V|1024.0|  V| 998.0|  V| 659.0|  V| 641.0|  V| 734.0|  V| 846.0|  V|1278.0|  V|1077.0|  V|1086.0|  V|1096.0|  V|1746.0|  V|1653.0|  V| 825.0|  V| 707.0|  V| 730.0|  V| 833.0|  V| 791.0|  V| 944.0|  V|1002.0|  V| 949.0|  V|1116.0|  V|1621.0|  V|5794.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 22|1146.0|  V| 915.0|  V| 471.0|  V| 288.0|  V| 215.0|  V| 256.0|  V| 521.0|  V| 458.0|  V| 419.0|  V| 374.0|  V| 479.0|  V| 449.0|  V| 392.0|  V| 338.0|  V| 502.0|  V| 575.0|  V| 353.0|  V| 365.0|  V| 512.0|  V|1238.0|  V|2378.0|  V|2081.0|  V|3206.0|  V|1822.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  3|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 20|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 24|1921.0|  V|1593.0|  V|2850.0|  V|1949.0|  V|1559.0|  V|1102.0|  V|1619.0|  V|3558.0|  V|4674.0|  V|2410.0|  V|1657.0|  V|1159.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|1549.0|  V|1891.0|  V| 750.0|  V| 996.0|  V|1629.0|  V|2417.0|  V|2799.0|  V|2117.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 18|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3864.8|  V|3886.1|  V|3893.0|  V|3893.0|  V|3892.5|  V|3892.4|  V|3888.9|  V|3881.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3885.5|  V|3893.0|  V|3892.8|  V|3883.7|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 26|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             996|2022| 10| 21|   NaN|  N|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1115.2|  V|1123.7|  V|1126.0|  V|1126.0|  V|1126.0|  V|1128.9|  V|1137.6|  V|1133.0|  V|1133.7|  V|1138.7|  V|1112.6|  V|1101.1|  V|1106.9|  V|1113.6|  V|1114.0|  V|1114.0|  V|1114.7|  V|1124.2|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 16|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10|  4|1109.0|  V| 961.0|  V| 676.0|  V|1247.0|  V| 603.0|  V| 947.0|  V|1866.0|  V|3970.0|  V|5062.0|  V|2724.0|  V|2478.0|  V|2210.0|  V|2092.0|  V|1366.0|  V|1361.0|  V|1221.0|  V|1294.0|  V|1971.0|  V|2911.0|  V|2825.0|  V|1434.0|  V|1555.0|  V|1135.0|  V|1603.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 30|3873.0|  V|3888.3|  V|3885.0|  V|3885.0|  V|3883.7|  V|3876.5|  V|3873.0|  V|   NaN|  N|3883.1|  V|3876.0|  V|3873.0|  V|3873.0|  V|3873.0|  V|3871.5|  V|3862.5|  V|3861.0|  V|3861.0|  V|   NaN|  N|3875.0|  V|3873.0|  V|3873.0|  V|3870.9|  V|3863.4|  V|3861.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  5|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10|  7|   NaN|  N| 781.0|  V| 580.0|  V| 565.0|  V| 598.0|  V| 642.0|  V|1057.0|  V|1484.0|  V|1734.0|  V|1576.0|  V|1729.0|  V|1549.0|  V|1391.0|  V|2047.0|  V|1710.0|  V|1686.0|  V|2023.0|  V|1676.0|  V|2308.0|  V|1983.0|  V|1309.0|  V|1513.0|  V| 931.0|  V| 832.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10|  1|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.8|  V|3853.0|  V|3847.9|  V|3847.2|  V|3847.9|  V|3846.3|  V|3844.2|  V|3843.6|  V|3843.3|  V|3843.1|  V|3853.0|  V|3893.0|  V|3893.0|  V|3893.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 13|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.9|  V|3852.3|  V|3846.3|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3888.8|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3868.1|  V|3868.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 16|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 24|1816.0|  V|1310.0|  V|1025.0|  V| 588.0|  V| 590.0|  V| 443.0|  V| 430.0|  V| 734.0|  V|1792.0|  V|1898.0|  V| 674.0|  V| 616.0|  V| 636.0|  V| 857.0|  V|1151.0|  V| 986.0|  V|1129.0|  V| 609.0|  V| 486.0|  V| 652.0|  V| 538.0|  V| 599.0|  V| 534.0|  V| 419.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 30|5860.0|  V|3499.0|  V|2035.0|  V|1272.0|  V|1355.0|  V|2306.0|  V|2439.0|  V|   NaN|  N|1736.0|  V|1910.0|  V|1579.0|  V|1364.0|  V|1696.0|  V|1862.0|  V|2094.0|  V|1949.0|  V|1686.0|  V|   NaN|  N|2125.0|  V|2665.0|  V|3017.0|  V|2511.0|  V|2709.0|  V|2312.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 28|   NaN|  N|3885.0|  V|3884.3|  V|3880.1|  V|3874.4|  V|3873.0|  V|3873.0|  V|   NaN|  N|3849.1|  V|3846.3|  V|3837.2|  V|   NaN|  N|3873.0|  V|3870.1|  V|3861.8|  V|3861.0|  V|3861.0|  V|3859.4|  V|   NaN|  N|3863.4|  V|3861.0|  V|3859.9|  V|3851.2|  V|3849.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 19|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:14:02 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:14:06 WARN DAGScheduler: Broadcasting large task binary with size 1507.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: h24\n",
      "Q1: 2.0\n",
      "Q3: 44.53\n",
      "IQR: 42.53\n",
      "Lower Bound: -125.59\n",
      "Upper Bound: 172.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:14:09 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:14:13 WARN DAGScheduler: Broadcasting large task binary with size 1596.8 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 30754\n",
      "Outliers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:14:16 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/05/16 17:14:19 WARN DAGScheduler: Broadcasting large task binary with size 1621.3 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|codi_provincia|provincia|codi_municipi| municipi|estacio|codi_contaminant| any|mes|dia|   h01|v01|   h02|v02|   h03|v03|   h04|v04|   h05|v05|   h06|v06|   h07|v07|   h08|v08|   h09|v09|   h10|v10|   h11|v11|   h12|v12|   h13|v13|   h14|v14|   h15|v15|   h16|v16|   h17|v17|   h18|v18|   h19|v19|   h20|v20|   h21|v21|   h22|v22|   h23|v23|   h24|v24|  month|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 11| 771.0|  V|1024.0|  V| 998.0|  V| 659.0|  V| 641.0|  V| 734.0|  V| 846.0|  V|1278.0|  V|1077.0|  V|1086.0|  V|1096.0|  V|1746.0|  V|1653.0|  V| 825.0|  V| 707.0|  V| 730.0|  V| 833.0|  V| 791.0|  V| 944.0|  V|1002.0|  V| 949.0|  V|1116.0|  V|1621.0|  V|5794.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 22|1146.0|  V| 915.0|  V| 471.0|  V| 288.0|  V| 215.0|  V| 256.0|  V| 521.0|  V| 458.0|  V| 419.0|  V| 374.0|  V| 479.0|  V| 449.0|  V| 392.0|  V| 338.0|  V| 502.0|  V| 575.0|  V| 353.0|  V| 365.0|  V| 512.0|  V|1238.0|  V|2378.0|  V|2081.0|  V|3206.0|  V|1822.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  3|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 20|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 24|1921.0|  V|1593.0|  V|2850.0|  V|1949.0|  V|1559.0|  V|1102.0|  V|1619.0|  V|3558.0|  V|4674.0|  V|2410.0|  V|1657.0|  V|1159.0|  V|   NaN|  N|   NaN|  N|   NaN|  N|   NaN|  N|1549.0|  V|1891.0|  V| 750.0|  V| 996.0|  V|1629.0|  V|2417.0|  V|2799.0|  V|2117.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 18|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3868.0|  V|3864.8|  V|3886.1|  V|3893.0|  V|3893.0|  V|3892.5|  V|3892.4|  V|3888.9|  V|3881.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3885.5|  V|3893.0|  V|3892.8|  V|3883.7|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 26|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             996|2022| 10| 21|   NaN|  N|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1114.0|  V|1115.2|  V|1123.7|  V|1126.0|  V|1126.0|  V|1126.0|  V|1128.9|  V|1137.6|  V|1133.0|  V|1133.7|  V|1138.7|  V|1112.6|  V|1101.1|  V|1106.9|  V|1113.6|  V|1114.0|  V|1114.0|  V|1114.7|  V|1124.2|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             998|2022| 10| 16|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|4969.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10|  4|1109.0|  V| 961.0|  V| 676.0|  V|1247.0|  V| 603.0|  V| 947.0|  V|1866.0|  V|3970.0|  V|5062.0|  V|2724.0|  V|2478.0|  V|2210.0|  V|2092.0|  V|1366.0|  V|1361.0|  V|1221.0|  V|1294.0|  V|1971.0|  V|2911.0|  V|2825.0|  V|1434.0|  V|1555.0|  V|1135.0|  V|1603.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 30|3873.0|  V|3888.3|  V|3885.0|  V|3885.0|  V|3883.7|  V|3876.5|  V|3873.0|  V|   NaN|  N|3883.1|  V|3876.0|  V|3873.0|  V|3873.0|  V|3873.0|  V|3871.5|  V|3862.5|  V|3861.0|  V|3861.0|  V|   NaN|  N|3875.0|  V|3873.0|  V|3873.0|  V|3870.9|  V|3863.4|  V|3861.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10|  5|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10|  7|   NaN|  N| 781.0|  V| 580.0|  V| 565.0|  V| 598.0|  V| 642.0|  V|1057.0|  V|1484.0|  V|1734.0|  V|1576.0|  V|1729.0|  V|1549.0|  V|1391.0|  V|2047.0|  V|1710.0|  V|1686.0|  V|2023.0|  V|1676.0|  V|2308.0|  V|1983.0|  V|1309.0|  V|1513.0|  V| 931.0|  V| 832.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10|  1|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.8|  V|3853.0|  V|3847.9|  V|3847.2|  V|3847.9|  V|3846.3|  V|3844.2|  V|3843.6|  V|3843.3|  V|3843.1|  V|3853.0|  V|3893.0|  V|3893.0|  V|3893.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|             997|2022| 10| 13|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3855.0|  V|3854.9|  V|3852.3|  V|3846.3|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3843.0|  V|3888.8|  V|3880.0|  V|3880.0|  V|3880.0|  V|3874.8|  V|3868.1|  V|3868.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 16|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 24|1816.0|  V|1310.0|  V|1025.0|  V| 588.0|  V| 590.0|  V| 443.0|  V| 430.0|  V| 734.0|  V|1792.0|  V|1898.0|  V| 674.0|  V| 616.0|  V| 636.0|  V| 857.0|  V|1151.0|  V| 986.0|  V|1129.0|  V| 609.0|  V| 486.0|  V| 652.0|  V| 538.0|  V| 599.0|  V| 534.0|  V| 419.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|              22|2022| 10| 30|5860.0|  V|3499.0|  V|2035.0|  V|1272.0|  V|1355.0|  V|2306.0|  V|2439.0|  V|   NaN|  N|1736.0|  V|1910.0|  V|1579.0|  V|1364.0|  V|1696.0|  V|1862.0|  V|2094.0|  V|1949.0|  V|1686.0|  V|   NaN|  N|2125.0|  V|2665.0|  V|3017.0|  V|2511.0|  V|2709.0|  V|2312.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             997|2022| 10| 28|   NaN|  N|3885.0|  V|3884.3|  V|3880.1|  V|3874.4|  V|3873.0|  V|3873.0|  V|   NaN|  N|3849.1|  V|3846.3|  V|3837.2|  V|   NaN|  N|3873.0|  V|3870.1|  V|3861.8|  V|3861.0|  V|3861.0|  V|3859.4|  V|   NaN|  N|3863.4|  V|3861.0|  V|3859.9|  V|3851.2|  V|3849.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     43|             998|2022| 10| 19|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|4999.0|  V|2022_10|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "airquality_df_new = iqr_outlier_treatment(airquality_df_new, continuous_variables_new, factor=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+---------+---+--------+--------+-------+-------+-----------+----------+--------+------------+----------+---------+-------------+----------+----------------+----------+-------+\n",
      "|          nom_cabina|qualitat_aire|codi_dtes|zqa|codi_eoi|longitud|latitud|hora_o3|qualitat_o3|  valor_o3|hora_no2|qualitat_no2| valor_no2|hora_pm10|qualitat_pm10|valor_pm10|         generat|  datetime|  month|\n",
      "+--------------------+-------------+---------+---+--------+--------+-------+-------+-----------+----------+--------+------------+----------+---------+-------------+----------+----------------+----------+-------+\n",
      "|Barcelona - Eixample|         Bona|       IH|  1| 8019043|  2.1538|41.3853|    17h|       Bona|28 Âµg/mÂ³|     17h|        Bona|51 Âµg/mÂ³|      17h|         Bona|15 Âµg/mÂ³|02/12/2018 18:00|1543770302|2018_12|\n",
      "|Barcelona - Obser...|         Bona|       OF|  1| 8019058|  2.1239|41.4183|    22h|       Bona| 8 Âµg/mÂ³|     22h|        Bona|22 Âµg/mÂ³|      22h|         Bona|13 Âµg/mÂ³|02/12/2018 23:00|1543788301|2018_12|\n",
      "|Barcelona - Palau...|         Bona|       IZ|  1| 8019057|  2.1151|41.3875|     7h|       Bona|34 Âµg/mÂ³|      7h|        Bona|14 Âµg/mÂ³|       7h|         Bona|14 Âµg/mÂ³| 03/12/2018 8:00|1543820701|2018_12|\n",
      "|Barcelona - Palau...|         Bona|       IZ|  1| 8019057|  2.1151|41.3875|    22h|       Bona| 9 Âµg/mÂ³|     22h|        Bona|36 Âµg/mÂ³|      22h|         Bona|13 Âµg/mÂ³|03/12/2018 23:00|1543874701|2018_12|\n",
      "|Barcelona - Vall ...|         Bona|       IN|  1| 8019054|   2.148|41.4261|     3h|       Bona| 1 Âµg/mÂ³|      3h|        Bona|37 Âµg/mÂ³|       3h|         Bona|24 Âµg/mÂ³| 06/12/2018 4:00|1544065501|2018_12|\n",
      "|Barcelona - Eixample|         Bona|       IH|  1| 8019043|  2.1538|41.3853|    17h|       Bona|31 Âµg/mÂ³|     17h|        Bona|29 Âµg/mÂ³|      17h|         Bona|16 Âµg/mÂ³|08/12/2018 18:00|1544288701|2018_12|\n",
      "| Barcelona - GrÃ cia|         Bona|       IJ|  1| 8019044|  2.1534|41.3987|     3h|       Bona|54 Âµg/mÂ³|      3h|        Bona|11 Âµg/mÂ³|       3h|         Bona|21 Âµg/mÂ³| 10/12/2018 4:00|1544411101|2018_12|\n",
      "|Barcelona - Vall ...|         Bona|       IN|  1| 8019054|   2.148|41.4261|     0h|       Bona|30 Âµg/mÂ³|      0h|        Bona|33 Âµg/mÂ³|       0h|         Bona|19 Âµg/mÂ³| 11/12/2018 1:00|1544486701|2018_12|\n",
      "| Barcelona - GrÃ cia|         Bona|       IJ|  1| 8019044|  2.1534|41.3987|     7h|       Bona|13 Âµg/mÂ³|      7h|        Bona|44 Âµg/mÂ³|       7h|         Bona|30 Âµg/mÂ³| 11/12/2018 8:00|1544511901|2018_12|\n",
      "|Barcelona - Eixample|         Bona|       IH|  1| 8019043|  2.1538|41.3853|    14h|       Bona|31 Âµg/mÂ³|     14h|        Bona|54 Âµg/mÂ³|      14h|         Bona|36 Âµg/mÂ³|11/12/2018 15:00|1544537102|2018_12|\n",
      "+--------------------+-------------+---------+---+--------+--------+-------+-------+-----------+----------+--------+------------+----------+---------+-------------+----------+----------------+----------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airquality_df_old.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:50:56 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:51:01 WARN DAGScheduler: Broadcasting large task binary with size 1579.2 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+-----+---+-----+---+-----+---+-----+---+-----+---+------+---+------+---+------+---+------+---+------+---+------+---+-----+---+-----+---+-----+---+-----+---+-----+---+-----+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|codi_provincia|provincia|codi_municipi| municipi|estacio|codi_contaminant| any|mes|dia|   h01|v01|   h02|v02|  h03|v03|  h04|v04|  h05|v05|  h06|v06|  h07|v07|   h08|v08|   h09|v09|   h10|v10|   h11|v11|   h12|v12|   h13|v13|  h14|v14|  h15|v15|  h16|v16|  h17|v17|  h18|v18|  h19|v19|   h20|v20|   h21|v21|   h22|v22|   h23|v23|   h24|v24|  month|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+-----+---+-----+---+-----+---+-----+---+-----+---+------+---+------+---+------+---+------+---+------+---+------+---+-----+---+-----+---+-----+---+-----+---+-----+---+-----+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "|             8|Barcelona|           19|Barcelona|     43|              14|2022| 10| 30|   2.0|  V|  18.0|  V|  NaN|  N| 43.0|  V| 43.0|  V| 18.0|  V| 20.0|  V|  21.0|  V|  34.0|  V|  37.0|  V|  59.0|  V|  64.0|  V|  64.0|  V| 56.0|  V| 40.0|  V| 49.0|  V| 35.0|  V| 13.0|  V| 44.0|  V|  40.0|  V|  39.0|  V|  40.0|  V|  31.0|  V|  23.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     50|              14|2022| 10| 28|  56.0|  V|  49.0|  V| 14.0|  V|  1.0|  V|  1.0|  V|  4.0|  V| 30.0|  V|   6.0|  V|   1.0|  V|   6.0|  V|   8.0|  V|  30.0|  V|  82.0|  V| 86.0|  V| 90.0|  V| 86.0|  V| 86.0|  V| 84.0|  V| 73.0|  V|  71.0|  V|  54.0|  V|  54.0|  V|  58.0|  V|  45.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|               1|2022| 10|  6|   2.0|  V|   1.0|  V|  1.0|  V|  1.0|  V|  1.0|  V|  1.0|  V|  2.0|  V|   1.0|  V|   2.0|  V|   2.0|  V|   2.0|  V|   1.0|  V|   2.0|  V|  1.0|  V|  2.0|  V|  1.0|  V|  1.0|  V|  1.0|  V|  1.0|  V|   1.0|  V|   1.0|  V|   2.0|  V|   2.0|  V|   1.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              12|2022| 10|  8|  28.0|  V|  29.0|  V| 18.0|  V| 16.0|  V| 24.0|  V| 17.0|  V|  9.0|  V|  16.0|  V|  32.0|  V|  27.0|  V|  29.0|  V|  20.0|  V|  18.0|  V| 13.0|  V|  9.0|  V|  6.0|  V|  6.0|  V|  8.0|  V| 10.0|  V|  12.0|  V|  15.0|  V|  31.0|  V|  41.0|  V|  25.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              14|2022| 10| 20|  18.0|  V|  17.0|  V| 27.0|  V| 27.0|  V| 24.0|  V| 37.0|  V| 16.0|  V|   2.0|  V|   2.0|  V|   4.0|  V|  13.0|  V|  13.0|  V|  39.0|  V| 36.0|  V| 52.0|  V| 44.0|  V| 17.0|  V| 25.0|  V| 17.0|  V|  12.0|  V|  59.0|  V|  50.0|  V|  54.0|  V|  32.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 11| 771.0|  V|1024.0|  V|998.0|  V|659.0|  V|641.0|  V|734.0|  V|846.0|  V|1278.0|  V|1077.0|  V|1086.0|  V|1096.0|  V|1746.0|  V|1653.0|  V|825.0|  V|707.0|  V|730.0|  V|833.0|  V|791.0|  V|944.0|  V|1002.0|  V| 949.0|  V|1116.0|  V|1621.0|  V|5794.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     54|              22|2022| 10| 22|1146.0|  V| 915.0|  V|471.0|  V|288.0|  V|215.0|  V|256.0|  V|521.0|  V| 458.0|  V| 419.0|  V| 374.0|  V| 479.0|  V| 449.0|  V| 392.0|  V|338.0|  V|502.0|  V|575.0|  V|353.0|  V|365.0|  V|512.0|  V|1238.0|  V|2378.0|  V|2081.0|  V|3206.0|  V|1822.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     57|               7|2022| 10| 14|   1.0|  V|   1.0|  V|  1.0|  V|  4.0|  V|  6.0|  V|  3.0|  V|  1.0|  V|   7.0|  V|  11.0|  V|   4.0|  V|  10.0|  V|   1.0|  V|   1.0|  V|  4.0|  V|  3.0|  V|  6.0|  V|  3.0|  V|  5.0|  V|  4.0|  V|   1.0|  V|   1.0|  V|   1.0|  V|   1.0|  V|   1.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     57|               7|2022| 10| 17|   1.0|  V|   1.0|  V|  1.0|  V|  1.0|  V|  1.0|  V|  1.0|  V|  1.0|  V|   1.0|  V|   4.0|  V|   3.0|  V|   3.0|  V|   2.0|  V|   2.0|  V|  1.0|  V|  1.0|  V|  1.0|  V|  1.0|  V|  1.0|  V|  1.0|  V|   1.0|  V|   1.0|  V|   1.0|  V|   1.0|  V|   1.0|  V|2022_10|\n",
      "|             8|Barcelona|           19|Barcelona|     57|               9|2022| 10| 25|   6.0|  V|   5.0|  V|  5.0|  V|  5.0|  V|  5.0|  V|  3.0|  V|  4.0|  V|   4.0|  V|   8.0|  V|  17.0|  V|  15.0|  V|  10.0|  V|  11.0|  V| 12.0|  V| 12.0|  V| 16.0|  V| 17.0|  V| 20.0|  V| 20.0|  V|  21.0|  V|  22.0|  V|  23.0|  V|  21.0|  V|  20.0|  V|2022_10|\n",
      "+--------------+---------+-------------+---------+-------+----------------+----+---+---+------+---+------+---+-----+---+-----+---+-----+---+-----+---+-----+---+------+---+------+---+------+---+------+---+------+---+------+---+-----+---+-----+---+-----+---+-----+---+-----+---+-----+---+------+---+------+---+------+---+------+---+------+---+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "airquality_df_new.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:26:21 WARN CaseInsensitiveStringMap: Converting duplicated key idfieldlist into CaseInsensitiveStringMap.\n",
      "24/05/16 17:26:21 WARN CaseInsensitiveStringMap: Converting duplicated key upsertDocument into CaseInsensitiveStringMap.\n",
      "24/05/16 17:26:21 WARN CaseInsensitiveStringMap: Converting duplicated key replaceDocument into CaseInsensitiveStringMap.\n",
      "24/05/16 17:26:21 WARN CaseInsensitiveStringMap: Converting duplicated key operationType into CaseInsensitiveStringMap.\n",
      "24/05/16 17:26:21 WARN CaseInsensitiveStringMap: Converting duplicated key writeConcern.w into CaseInsensitiveStringMap.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "write_config = {\n",
    "    \"writeConcern.w\": \"majority\",\n",
    "    \"replaceDocument\": \"false\",\n",
    "    \"idFieldList\": \"codi_eoi,codi_dtes,datetime\",  # Specify the field to use for upsert\n",
    "    \"operationType\": \"update\",  # Specify the update operation\n",
    "    \"upsertDocument\": \"true\"  # Enable upsert logic\n",
    "}\n",
    "\n",
    "airquality_df_old.write.format(\"mongodb\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"database\", \"bdm\") \\\n",
    "    .option(\"collection\", \"airquality_old\") \\\n",
    "    .options(**write_config) \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 17:52:10 WARN CaseInsensitiveStringMap: Converting duplicated key idfieldlist into CaseInsensitiveStringMap.\n",
      "24/05/16 17:52:10 WARN CaseInsensitiveStringMap: Converting duplicated key upsertDocument into CaseInsensitiveStringMap.\n",
      "24/05/16 17:52:10 WARN CaseInsensitiveStringMap: Converting duplicated key replaceDocument into CaseInsensitiveStringMap.\n",
      "24/05/16 17:52:10 WARN CaseInsensitiveStringMap: Converting duplicated key operationType into CaseInsensitiveStringMap.\n",
      "24/05/16 17:52:10 WARN CaseInsensitiveStringMap: Converting duplicated key writeConcern.w into CaseInsensitiveStringMap.\n",
      "24/05/16 17:52:12 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/05/16 17:52:16 WARN DAGScheduler: Broadcasting large task binary with size 1574.3 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 19:43:32 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 3004339 ms exceeds timeout 120000 ms\n",
      "24/05/16 19:43:32 WARN SparkContext: Killing executors is not supported by current scheduler.\n",
      "24/05/16 19:43:35 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:43:35 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:43:45 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:43:45 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:43:55 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:43:55 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:44:05 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:44:05 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:44:15 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:44:15 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:44:25 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:44:25 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:44:35 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:44:35 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:44:45 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:44:45 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:44:55 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:44:55 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:45:05 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:45:05 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:45:15 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:45:15 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:45:25 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:45:25 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:45:35 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:45:35 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:45:45 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:45:45 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:45:55 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:45:55 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:46:05 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:46:05 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:46:15 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:46:15 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:46:25 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:46:25 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:46:35 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:46:35 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:46:45 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:46:45 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:46:55 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:46:55 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:47:05 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:47:05 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:47:15 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:47:15 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:47:25 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:47:25 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:47:35 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:47:35 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:47:45 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:47:45 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:47:55 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:47:55 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:48:05 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:48:05 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:48:15 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:48:15 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:48:25 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:48:25 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:48:35 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:48:35 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:48:45 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:48:45 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:48:55 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:48:55 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:49:05 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:49:05 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:49:15 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:49:15 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:49:25 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:49:25 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:49:35 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:49:35 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:49:45 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:49:45 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:49:55 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:49:55 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:50:05 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:50:05 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:50:15 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:50:15 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:50:25 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:50:25 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:50:35 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:50:35 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:50:45 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:50:45 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:50:55 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:50:55 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:51:05 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:51:05 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:51:15 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:51:15 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:51:25 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:51:25 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:51:35 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:51:35 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:51:45 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:51:45 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:51:55 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:51:55 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:52:05 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:52:05 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:52:15 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:52:15 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:52:25 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:52:25 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:52:35 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:52:35 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:52:45 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:52:45 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:52:55 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:52:55 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:53:05 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:53:05 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:53:15 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:53:15 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:53:25 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:53:25 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.175.226:46607\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/05/16 19:53:25 ERROR Executor: Exit as unable to send heartbeats to driver more than 60 times\n"
     ]
    }
   ],
   "source": [
    "write_config = {\n",
    "    \"writeConcern.w\": \"majority\",\n",
    "    \"replaceDocument\": \"false\",\n",
    "    \"idFieldList\": \"codi_provincia,codi_municipi,estacio,codi_contaminant,month,dia\",  # Specify the field to use for upsert\n",
    "    \"operationType\": \"update\",  # Specify the update operation\n",
    "    \"upsertDocument\": \"true\"  # Enable upsert logic\n",
    "}\n",
    "\n",
    "airquality_df_new.write.format(\"mongodb\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"database\", \"bdm\") \\\n",
    "    .option(\"collection\", \"airquality\") \\\n",
    "    .options(**write_config) \\\n",
    "    .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
